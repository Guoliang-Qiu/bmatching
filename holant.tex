\documentclass[11pt]{article}
\usepackage{fullpage}

\usepackage{amsthm,amsmath,amssymb}

\usepackage[bookmarks=true,hypertexnames=false,pagebackref]{hyperref}
\hypersetup{colorlinks=true, citecolor=blue, linkcolor=red,
  urlcolor=blue}
\usepackage{amsthm}
\usepackage[lining,semibold,type1]{libertine} % a bit lighter than Times--no osf in math
\usepackage[T1]{fontenc} % best for Western European languages
\usepackage{textcomp} % required to get special symbols
\usepackage[varqu,varl]{inconsolata}% a typewriter font must be defined
\usepackage[libertine,vvarbb]{newtxmath}
\usepackage[scr=rsfso,cal=cm]{mathalfa}
\usepackage{bm}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{enumerate}
\usepackage{thmtools}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{problem}[theorem]{Problem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{definitions}[theorem]{Definitions}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{block}[theorem]{}
\newtheorem*{myclaim}{Claim}
\newtheorem*{remark}{Remark}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{condition}{Condition}
\newtheorem{problem}{Problem}
\newtheorem{property}{Property}

\crefname{condition}{Condition}{Conditions}
\crefname{proposition}{Proposition}{Propositions}
\crefname{lemma}{Lemma}{Lemmas}
\crefname{theorem}{Theorem}{Theorems}

% \DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}
% \DeclareSymbolFont{largesymbols}{OMX}{cmex}{m}{n} % This command seems not compatible with the package {newtxmath}

\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\tuple}[1]{\left(#1\right)} \newcommand{\eps}{\varepsilon}
\newcommand{\inner}[2]{\langle #1,#2\rangle} \newcommand{\tp}{\tuple}
\renewcommand{\mid}{\;\middle\vert\;} \newcommand{\cmid}{\,:\,}
\newcommand{\numP}{\#\mathbf{P}} \renewcommand{\P}{\mathbf{P}}
\newcommand{\defeq}{\triangleq} \renewcommand{\d}{\,\-d}
\newcommand{\ol}{\overline}

\newcommand{\id}[1]{\mathbb{1}\left[#1\right]}

\usepackage[ruled,linesnumbered,vlined]{algorithm2e}
%\renewcommand{\thealgorithm}{} %% disable the algorithm counter
\usepackage{algpseudocode}
\SetKwRepeat{Do}{do}{while}

\def\*#1{\mathbf{#1}} \def\+#1{\mathcal{#1}} \def\-#1{\mathrm{#1}} \def\^#1{\mathbb{#1}} \def\$#1{\mathtt{#1}}


\def\!#1{\mathtt{#1}}
\def\@#1{\mathscr{#1}}

\def\EG{\emph{e.g.}}
\def\IE{\emph{i.e.}}

% Zhidan's macros
\def\bad{\!{bad}}
\def\cp{\!{cp}}
\def\good{\!{good}}
\def\poly{\mathrm{poly}}

\def\symbolwidth{\phantom{{}={}}}

\newcommand{\wt}[1]{\widetilde{#1}}
\newcommand{\wh}[1]{\widehat{#1}}
\newcommand{\Couple}{\!{Couple}}
\newcommand{\Ham}{\!{Ham}}
\newcommand{\Holant}{\mathrm{Holant}}
\newcommand{\Par}{\!{Par}}
\newcommand{\Pick}{\!{Pick}}

\newcommand{\zero}{\boldsymbol{0}}
\newcommand{\vecf}{\boldsymbol{f}}
\newcommand{\seqS}{\boldsymbol{s}}

\usepackage{xifthen}
\usepackage{thm-restate}

\renewcommand{\Pr}[2][]{ \ifthenelse{\isempty{#1}}
  {\mathbf{Pr}\left[#2\right]} {\mathbf{Pr}_{#1}\left[#2\right]} }
\newcommand{\E}[2][]{ \ifthenelse{\isempty{#1}}
  {\mathbf{E}\left[#2\right]}
  {\mathop{\mathbf{E}}_{#1}\left[#2\right]} }
\newcommand{\Var}[2][]{ \ifthenelse{\isempty{#1}}
  {\mathbf{Var}\left[#2\right]}
  {\mathop{\mathbf{Var}}_{#1}\left[#2\right]} }


\newcommand{\zdtodo}[1]{\todo[color = blue!40, size = \tiny]{\textbf{zhidan:} #1}}
\newcommand{\qtodo}[1]{\todo[color = purple!40, size = \tiny]{\textbf{guoliang:} #1}}

\newcommand{\qgl}[1]{{\color{purple}{#1}}}
\newcommand{\hktodo}[1]{{\color{blue}{#1}}}

\newcommand{\zd}[1]{{\color{green} #1}}
\newcommand{\zdnew}[1]{{\color{cyan} #1}}
%%% article specific macros %%%

\title{Deterministic Counting}
\author{}
%\address{Shanghai Jiao Tong University}

%\date{Last modified on Oct 28, 2020}
\date{\today}
\begin{document}

\maketitle

\section{Introduction}

\subsection{$b$-matchings}

Given a graph $G = (V, E)$, let $E_v \defeq \set{e \in E \cmid \mbox{$e$ is incident to $v$}}$ be the collection of all edges incident to $v$ for each $v \in V$. For a positive integer $b \ge 1$, we say that $S \subseteq E$ is a \emph{$b$-matching of $G$} if $\abs{S \cap E_v} \le b$ for every $v \in V$. Let $\Omega = \Omega_{G, b}$ be the set of all $b$-matchings and $\mu = \mu_{G, b}$ be the uniform distribution over $\Omega$. 
% \qgl{Moreover, we remark here that the definitions above can be safely extended to graphs with dangling edges.}\qtodo{In introduction, it is not necessary to introduce the notion of dangling edges.}

\zdnew{State SOTA of $b$-matchings here, and following is our main result on it.}

\begin{theorem} \label{thm:counting-b-matchings}
    Given a positive integer $\Delta \ge 1$ and a tolerance error $\varepsilon \in (0, 1/4)$, there is a deterministic algorithm such that for every graph $G = (V, E)$ with maximum degree $\le \Delta$ and every positive integer $b \le \Delta$, in time $\poly\left(\left(\abs{E} \cdot \varepsilon^{-1}\right)^{C_{\ref{thm:counting-b-matchings}}}\right)$, it outputs a number $\wh{Z}$ such that
    \begin{align*}
        (1 - \varepsilon) \abs{\Omega_{G, b}} \le \wh{Z} \le (1 + \varepsilon) \abs{\Omega_{G, b}}
    \end{align*}
    where the coefficients of the polynomial and $C_{\ref{thm:counting-b-matchings}}$ depends only on $\Delta$.
\end{theorem}

\subsection{Holant problems with positive log-concave signatures}

Beyond counting the number of $b$-matchings, we also consider a family of Holant problems with binary symmetric positive log-concave signatures. Given a function $f$ of arity $d > 0$, we say that $f$ is \emph{binary symmetric} if $f : \set{0, 1}^d \to \mathbb R_{\ge 0}$ and its value only depends on the hamming weight of the input. By symmetry, it is safe to identity $f$ as $f = [f(0), f(1), \ldots, f(d)]$.

For a family of binary symmetric functions $\+F$, the Holant problem, denoted by $\Holant(\+F)$, is a family of counting problems defined on graphs by assigning a signature in $\+F$ to each vertex in the graph. Specifically, an instance of $\Holant(\+F)$ can be specified by $ \Phi = \left(G = (V, E), \vecf = \set{f_v}_{v \in V}\right)$ where for each $v \in V$, $f_v \in \+F$ is called the \emph{signature at $v$}. \qtodo{We should introduce the external field here?}
The counting problem is to (approximately) compute the partition function $Z= Z_\Phi$ defined as follows:
$$
   Z\defeq \sum_{\sigma\in \set{0,1}^{E}} \prod_{v \in V} f_v\left(\abs{\sigma \vert_{E_v}}\right),
$$ where $\sigma \vert_{E_v}$ is the restriction of $\sigma$ on the edges $E_v$.
%\qtodo{Use $\sigma \vert_{E_v}$ or $\sigma_{E_v}$?}
% Then the corresponding Gibbs distribution $\mu = \mu_\Phi$ can be defined as
% % \begin{align*}
% %     \mu(\sigma) = \mu_\Phi(\sigma) \defeq \frac{1}{Z} \prod_{v \in V} f_v\left(\abs{\sigma_{E_v}}\right), \quad \forall \sigma \in \set{0,1}^{E},
% % \end{align*} 
% \begin{align*}
%     \mu(S) \defeq \frac{1}{Z} \prod_{v \in V} f_v(\abs{S \cap E_v}), \quad \forall S \subseteq E,
% \end{align*} 
% where $Z= Z_\Phi$ is the partition function defined as $ Z  \defeq \sum_{S \subseteq E} \prod_{v \in V} f_v(\abs{S \cap E_v})$. 
Furthermore, it is natural to induce the probability distribution $\mu=\mu_\Phi$ over $\Omega = \Omega_\Phi=\set{0,1}^{E}$ where 
\begin{equation}\label{def-holant-mu}
\begin{aligned}
\forall \sigma\in \Omega, \quad \mu(\sigma)\propto  \prod_{v \in V} f_v\left(\abs{\sigma \vert_{E_v}}\right)
\end{aligned}
\end{equation}
% \begin{align*}
%    \mu(\sigma)\triangleq \frac{1}{Z} \prod_{v \in V} f_v\left(\abs{\sigma_{E_v}}\right).
% \end{align*}
% \begin{align*}
%     Z  \defeq \sum_{S \subseteq E} \prod_{v \in V} f_v(\abs{S \cap E_v}).
% \end{align*}
% $$
% \begin{gathered}
% 	\mu(S) = \mu_\Phi(S) \defeq \frac{1}{Z} \prod_{v \in V} f_v(\abs{S \cap E_v}), \quad \forall S \subseteq E, \\
% 	Z = Z_\Phi \defeq \sum_{S \subseteq E} \prod_{v \in V} f_v(\abs{S \cap E_v}).
% \end{gathered}
% $$
\qgl{When $f_v(k) = \id{k \le b}$, it can be shown that the Holant problem becomes an instance of $b$-matchings by regarding any subset of edges $S\subseteq E$ as the binary vector.} \qtodo{maybe add more famous problem here to show the expressibility of the holant.}

%Note that, for a Holant instance $(G = (V, E), \set{f_v}_{v \in V})$, 
In this paper, we focus on the Holant problems with all signatures being positive and log-concave. A binary symmetric signature $f = [f(0), f(1), \ldots, f(d)]$ of arity $d$ is \emph{positive and log-concave} if and only if it satisfies: (1) $f(0) > 0$; (2) $f(k)^2 \ge f(k - 1) f(k + 1)$ for every $1 \le k \le d - 1$; (3) If $f(k) > 0$ for some $k \le d$, then for every $0 \le j \le k$, $f(j) > 0$. \qgl{state that the log-concave signature is powerful enough to express many counting problems.}
% \begin{itemize}
% 	\item $f(0) > 0$.
%     \item $f(k)^2 \ge f(k - 1) f(k + 1)$ for every $1 \le k \le d - 1$.
%     \item If $f(k) > 0$ for some $k \le d$, then for every $0 \le j \le k$, $f(j) > 0$.
% \end{itemize}
% \qgl{Similarly to the $b$-matchings, the concepts are well-defined as well when the underlying graphs contain dangling edges.}\qtodo{also put this in pre.}

\zdnew{
We consider a family of Holant instances satisfying the following conditions.
\begin{condition} \label{cond:Holant-condition}
    The following holds for the instance $(\Phi = (G, \vecf), \Delta, r)$:
    \begin{itemize}
        \item $G = (V, E)$ is a graph of maximum degree $\Delta$.
        \item $\vecf = \set{f_v}_{v \in V}$ is a family of binary symmetric positive log-concave signatures and $f_v(1) \le r \cdot f_v(0)$ for every $v \in V$.
    \end{itemize}
\end{condition}
\zdtodo{put the family of instances we focus on here.}
}

\zdnew{State SOTA of Holant problems with log-concave signatures here, and following is our main result on it.}

\begin{theorem} \label{thm:counting-Holant}
    Given a positive integer $\Delta > 0$, a positive real $r > 0$ and a tolerance error $\varepsilon \in (0, 1/4)$, there exists a deterministic algorithm such that for every instance $(\Phi = (G = (V, E), \vecf), \Delta, r)$ satisfying~\Cref{cond:Holant-condition}, with running time $\poly\left(\left(\abs{E} \cdot \varepsilon^{-1}\right)^{C_{\ref{thm:counting-Holant}}}\right)$, it outputs a number $\wh{Z}$ such that
    $$
        (1 - \varepsilon) Z_\Phi \le \wh{Z} \le (1 + \varepsilon) Z_{\Phi}
    $$
    where the coefficients of the polynomial and $C_{\ref{thm:counting-Holant}}$ depend only on $\Delta$ and $r$.
\end{theorem}

\section{Preliminaries}

% \zdnew{
% Collection of symbols:
% \begin{enumerate}
%     \item Mathematical symbols (put the sequence symbols here, suggest using different notations with sets).
%     \item Graph notations.
%     \item $b$-matchings and Holant notations.
% \end{enumerate}
% }

\subsection{Notations}

For a natural number $n \in \mathbb N$, we use $[n]$ to denote the set $\set{1, 2, \ldots, n}$.
We use $\log(\cdot)$ to denote the logarithm with the natural base.
For an event $\+E$, we use $\id{\+E}$ to denote the indicator of it. 
We will use boldface type, e.g., $\boldsymbol{S},\boldsymbol{c}$, for vectors.
Specifically, we use $\boldsymbol{0}$ and $\boldsymbol{1}$ to denote the all-zero vector and the all-one vector, respectively.
For any sequence $\seqS = (s_1, \ldots, s_\ell)$ and any element $t$, let $\seqS \circ t$ denote $(s_1, \ldots, s_\ell,t)$, the concatenation of $\seqS$ and $t$.
With a slight abuse of notation, we will occasionally use the sequence $\seqS= (s_1, \ldots, s_\ell)$ to denote the set $\{s_1, \ldots, s_\ell\}$ when it is clear from the context.
% Let $\seqS = \set{s_1, \ldots, s_\ell}$ be an ordered sequence of length $\ell$ and $\alpha$ be an element, we use $\seqS \circ \alpha$ to denote the concatenation of $\seqS$ and $\alpha$ defined as $\seqS \circ e \defeq \set{s_1, \ldots, s_\ell, s_{\ell + 1} = \alpha}$. 
Moreover, we use the notation $\varnothing$ to denote an empty sequence.

We consider the graphs with ``half-edges'' in this paper.
Formally, given a set of nodes $V$, an \emph{normal} edge on $V$ is a pair of different nodes $\{u,v\}$ where $u,v\in V$,
and a half-edge on $v$ is a singleton tuple $\{u\}$ where $u \in V$.
A graph with half-edges is a pair $(V,E_1\cup E_2)$ where $V$ is a set of nodes, $E_1$ is a set of normal edges,
and $E_2$ is a set of half-edges. 
% \hktodo{
% For convenience, we allow the ``half-edges'' in the graph $G=(V,E)$ we considered. These half-edges refer to the edges in the graph which have only one endpoint. Then we write $E = E_1 \cup E_2$ where $E_1$ is the set of all normal edges and $E_2$ collects all half-edges in $G$. }
Given a graph $G$, we always use $V(G)$ to denote the vertex set of $G$, $E(G)$ to denote the edge set, $E_1(G) \subseteq E(G)$ to denote the collection of normal edges, $E_2(G)\subseteq E(G)$ to denote the set of half-edges, and $\Delta(G)$ to denote the maximum degree. 
For every $v \in V$, we use $\deg_G(v)$ to denote the degree of $v$ in $G$, \IE, the number of edges incident to $v$.
% We also let $\Delta(G)$ be the maximum degree of $G$. 
If $G$ is clear from the context, we may omit $G$ from these notations.
% When the context is clear, we omit explicitly indicating the graph $G$. 
For convenience, we suppose that an arbitrary order is assumed over all the edges in $E_1$.
% For convenience, we fix an arbitrary order to the edges.

% Given a rooted tree $T = (V(T), E(T))$, we use $\rho = \rho(T)$ to denote its root. For every vertex $v \in V(T)$, we use $\Par(v)$ to denote the parent of $v$ and $\+C(v)$ to denote the collection of children of $v$ with convention $\Par(\rho(T)) = \varnothing$.\qtodo{recheck here}

We call $\sigma: E\rightarrow \set{0,1}$ an assignment of $G$.
For any subset $S\subseteq E$ and any assignment $\sigma$, we use $\sigma(S)$ to denote the assignments of $\sigma$ on $S$.
For simplicity, let $\sigma(e)$ denote $\sigma(\{e\})$ for each $e\in E$. For any $S\subseteq E$, we call $\sigma: S\rightarrow \set{0,1}$ a \emph{partial assignment} of $G$ defined on $S$.
For any subset $S \subseteq E$ and any partial assignment $\sigma$ defined on $S$, 
let $\Lambda(\sigma)$ denote $S$.
For any assignment $\sigma$ and any partial assignment $\tau$,
we abuse the notation $\sigma\in \tau$ to denote the event that $\sigma(\Lambda(\tau)) =\tau$.

Given any partial assignment $\sigma$ and any vertex $v \in V$, we use $\Ham(\sigma, E_v)$ to denote $\abs{\sigma(E_v \cap \Lambda(\sigma))}$, i.e., the Hamming weight of $\sigma$ restricted to $E_v \cap \Lambda(\sigma)$.
For two partial assignments $\sigma$ and $\tau$ where $\Lambda(\sigma)\cap \Lambda(\tau) = \emptyset$, we use $\sigma \land \tau : \Lambda(\sigma) \cup \Lambda(\tau) \to \set{0, 1}$ to denote the concatenation of $\sigma$ and $\tau$, \IE, $(\sigma \land \tau)(e)=\sigma(e)$ for each $e\in \Lambda(\sigma)$ and $(\sigma \land \tau)(e)=\tau(e)$ for each $e\in \Lambda(\tau)$. 
\qgl{Additionally, for any $S\subseteq E$ and any $\boldsymbol{c} \in \{0,1\}^S$, we use $S \gets \boldsymbol{c}$ to denote the partial assignment $\sigma$ where $\Lambda(\sigma) = S$ and $\sigma(S) = \boldsymbol{c}$.}

Fix an instance $ \Phi = \left(G = (V, E), \vecf = \set{f_v}_{v \in V}\right)$ of the Holant problem.
Recall the distribution $\mu \triangleq \mu_{\Phi}$ defined in \eqref{def-holant-mu}.
We say an assignment $\sigma$ of $G$ is feasible if $\mu(\sigma)>0$. 
We say a partial assignment $\tau$ is \emph{feasible} if there exists a feasible assignment $\sigma\in \tau$.



% We say that $\sigma_{\Lambda}$ is \emph{a feasible partial assignment} if $\set{\sigma\in \set{0,1}^E \ \vert \ \sigma \vert_\Lambda=\sigma_{\Lambda}\land  \mu(\sigma)>0} \neq \emptyset$.

 
% Given a distribution $\mu$ defined among $\set{0,1}^E$, we say an assignment $\sigma: E\rightarrow \set{0,1}$ is feasible if $\mu(\sigma)>0$. For any subset $\Lambda \subseteq E$, we use $\sigma_\Lambda : \Lambda \to \set{0, 1}$ to denote the partial assignment defined on $\Lambda$. 
% We say that $\sigma_{\Lambda}$ is \emph{a feasible partial assignment} if $\set{\sigma\in \set{0,1}^E \ \vert \ \sigma \vert_\Lambda=\sigma_{\Lambda}\land  \mu(\sigma)>0} \neq \emptyset$.
% \hktodo{ Moreover, we abuse the notation $\sigma_{\Lambda}$ to denote the event that $\set{\sigma \vert_\Lambda=\sigma_{\Lambda} \ \vert \ \mu(\sigma)>0}$.}

% With this notation, it is convenient to indicate that the restriction of the assignment $\sigma$ coincides with $\sigma_\Lambda$ by expressing it as $\sigma\in \sigma_{\Lambda}$. 


For any partial assignment $\tau$ and any vertex $v\in V$, we use $E_v^\tau \defeq E_v \setminus \Lambda(\tau)$ to denote the set of unpinned edges incident to $v$ under $\tau$. 
Given any feasible partial assignment $\tau$ and any subset $S \subseteq E \setminus \Lambda(\tau)$, we denote by $\mu_S^\tau$ the marginal probability distribution of $\mu$ projected to $S$ conditional on $\tau$.
Similarly, let $\mu_S$ denote the marginal probability distribution of $\mu$ projected to $S$.
Furthermore, given any partial assignment $\sigma$ where $S\subseteq \Lambda(\sigma)$ and any $e\in E$ where $e\not\in \Lambda(\tau)$, we will use the following simplified notations:
\begin{itemize}
\item Let $\mu_S^\tau(\sigma),\mu_S(\sigma)$ denote $\mu_S^\tau(\sigma(S)),\mu_S(\sigma(S))$, respectively;
\item Let $\mu^\tau(\sigma),\mu(\sigma)$ denote $\mu_{\Lambda(\sigma)\setminus\Lambda(\tau)}^\tau(\sigma),\mu_{\Lambda(\sigma)}(\sigma)$, respectively;
\item Let $\mu_e^\tau(\sigma),\mu_e(\sigma)$ denote $\mu_{\{e\}}^\tau(\sigma),\mu_{\{e\}}(\sigma)$, respectively.
\end{itemize}






\iffalse
\subsection{$b$-matchings and notations}
    Consider a graph $G=(V,E)$ with an integer $b\geq 1$. A $b$-matching of $G$ is a subset of edges $S\subseteq E$ such that for each $v\in V$, we have $\abs{S\cap E_v}\leq b$ where $E_v\triangleq \set{e\in E \mid v\in e}$ is the collection of incident edges of $v$. Let $\Omega=\Omega_{G,b}$ be the set of all $b$-matchings of $G$ and $\mu=\mu_{G,b}$ be the uniform distribution over $\Omega$. 


    For convenience, we use a binary indicator vector $\sigma: E\rightarrow \set{0,1}$ to represent any subset of edge $S\subseteq E$ where $\sigma(e)=1$ if $e\in S$ for each $e\in E$. 
    Moreover, we use $\sigma_S:S\rightarrow \set{0,1}$ to denote the partial assignment defined on $S\subseteq E$ or the partial assignment defined on $S\subseteq E$ induced from $\sigma$, i.e., for any variable $e$ in $S$, we have $\sigma_S(e)=\sigma(e)$. \zdtodo{If necessary: change the notation here. Suggest $\sigma \vert_S$.} \qgl{We sometimes use the notation $v\gets a$ to denote the partial assignment on a single edge.}

    Given any partial assignment $\tau: E'\rightarrow \set{0,1}$ defined on $E'\subseteq E$, we abuse $\tau$ to denote the event that $\set{\sigma_S=\tau \ \vert \ \mbox{$\sigma$ is a $b$-matching}}$. By this notation, we call a partial assignment $\tau$ is feasible if $\tau\neq \emptyset$. (\qgl{and we can use $\tau_1\land \tau_2$ to denote the concatenation of the partial assignments}) Furthermore, given any subset of edges $S\subseteq E\setminus E'$, we use $\mu_S^{\tau}$ to denote the marginal distribution on $S$ conditioned on the event $\tau$. \qgl{In particular $\mu_v=\mu_{\set{v}}$.} \qgl{here, we also define $E^{\tau}_v$ as the unpinned edges in $E_v$.} 
    % We can equivalently view the instance with pinned partial assignment as a instance with no pinning.

    In our later discussion, {we also allow the existence of the \qgl{dangling edges}}. Specifically, we represent the graph by $G=(V,E=E_1\cup E_2)$ where $E_1$ is the set of normal edges and $E_2$ is the set of dangling edges.
    {One can generalize the previous notations and notions in the graph with dangling edges similarly.} 

\subsection{Holant problems with log-concave signatures}

\iffalse
    In this paper, we consider the Holants problem with binary symmetric signatures.
    Given a set of signature $\+F$ where each $f\in \+F$ of arity $d>0$ is a binary symmetric function $f: \set{0,1}^d\rightarrow \^R$ that the value depends only on the hamming weight of the input, the Holant problem, denoted by $\HH(\+F)$ is a family of counting problems defined on the graphs by associating the signature with each vertex in $\+F$. Specifically, an instance of $\HH(\+F)$ can be specified by the tuple $\left(G=(V,E),\set{f_v}_{v\in V}\right)$ where $f_v\in \+F$, and our goal is to compute the following partition function
    \begin{align*}
        \HH(G,\set{f_v}_{v\in V})\triangleq \sum_{\sigma\in \set{0,1}^{E}} \prod_{v\in V} f_v(\sigma_{E_v})
    \end{align*}
    where $f_v(\sigma_{E_v})$ is well-defined by the symmetry of the signature. \qgl{continue here.}
\fi

In this paper, we consider the Holant problem with binary symmetric log-concave signatures. Given a function $f$ of arity $d > 0$, we say that $f$ is \emph{binary symmetric} if $f : \set{0, 1}^d \to \mathbb R_{\ge 0}$ and its value only depends on the hamming weight of the input. By symmetry, it is safe to identity $f$ as $f = [f(0), f(1), \ldots, f(d)]$.

For a family of binary symmetric functions $\+F$, the Holant problem, denoted by $\Holant(\+F)$, is a family of counting problems defined on graphs by assigning a signature in $\+F$ to each vertex in the graph. Specifically, an instance of $\Holant(\+F)$ can be specified by the tuple $\left(G = (V, E), \set{f_v}_{v \in V}\right)$ where for each $v \in V$, $f_v \in \+F$ is called the \emph{signature at $v$}. Our goal is to (approximately) compute the following partition function\qtodo{Note that the notation $E_v$ has been defined when we introduce the b-matching problem}
$$
    \Holant(G, \set{f_v}_{v \in V}) \defeq \sum_{\sigma : E \to \set{0, 1}} \prod_{v \in V} f_v\left(\abs{\sigma_{E_v}}\right),
$$ where $\abs{\sigma_{E_v}}$ is the hamming weight of the partial assignment $\sigma_{E_v}$.
\qtodo{Use $\sigma \vert_{E_v}$ or $\sigma_{E_v}$?}
\qgl{When $f_v(k) = \id{k \le b}$, it can be shown that the Holant problem becomes an instance of $b$-matchings.} \qtodo{maybe add more famous problem here to show the expressibility of the holant.}

Note that, for a Holant instance $(G = (V, E), \set{f_v}_{v \in V})$, it is natural to induce the following probability distribution over $\Omega = \set{0,1}^{E}$:
\begin{align*}
    \mu(\sigma) = \mu\left(G,\set{f_v}_{v\in V};\sigma\right)\triangleq \frac{1}{\Holant(G, \set{f_v}_{v \in V})} \prod_{v \in V} f_v\left(\abs{\sigma_{E_v}}\right).
\end{align*}

In this paper, we put our attention on the Holant problems with all signatures being positive and log-concave. A binary symmetric signature $f = [f(0), f(1), \ldots, f(d)]$ of arity $d$ is \emph{positive and log-concave} if and only if it satisfies:
\begin{itemize}
	\item $f(0) > 0$.
    \item $f(k)^2 \ge f(k - 1) f(k + 1)$ for every $1 \le k \le d - 1$.
    \item If $f(k) > 0$ for some $k \le d$, then for every $0 \le j \le k$, $f(j) > 0$.
\end{itemize}

\fi

\subsection{Properties of Holant instances}
\qgl{check this sec}
% \qtodo{This part should be changed}
% \qtodo{change it to the holant version.}
% \begin{lemma}
%     Consider any graph $G=(V,E)$ of maximum degree $\Delta$ with an integer $b\geq 1$ and a feasible partial assignment $\tau:E'\rightarrow \set{0,1}$ where $E'\subseteq E$. For each $v\in V$, it holds that  
%     \begin{align*}
%        \sigma \sim \mu^{\tau}, \quad \quad   \mu^{\tau}(\sigma_{E_v\setminus E'}=\boldsymbol{0})\geq 1/{\Delta^b}.
%     \end{align*}
% \end{lemma}

% For a positive binary symmetric signature $f = [f(0), f(1), \ldots, f(d)]$ with $f(0) > 0$, we consider the following \emph{local polynomial} introduced in \zd{[GLLZ18, CG23]}:
% \begin{align} \label{eq:local-polynomial}
% 	P_f(x) \defeq \frac{1}{f(0)}\sum_{i = 0}^d \binom{d}{i}\cdot f(i) \cdot x^i.
% \end{align}
% For a binary symmetric Holant instance $\Phi = \left(G = (V, E), \vecf = \set{f_v}_{v \in V}\right)$ with positive log-concave signatures, the following quantities defined in~\zd{[CG23]} might be helpful:
% \begin{align} \label{eq:Holant-quantities}
% 	r_{\max} \defeq \max_{v \in V} \frac{f_v(1)}{f_v(0)}, \quad P_{\max} \defeq \max_{v \in V} P_{f_v}(r_{\max}).
% \end{align}
% We state here the following marginal lower bound established in~\zd{[CG23]}, which is of great significance in our analysis.

Now we state some properties of Holant instances here. Given a binary symmetric Holant instance $\Phi = \left(G = (V, E), \vecf = \set{f_v}_{v \in V}\right)$ with positive log-concave signatures and its Gibbs distribution $\mu = \mu_\Phi$, for every edge $e \in E$, define its \emph{marginal ratio} as
\begin{align*}
    R_{\Phi}(e) \defeq \frac{\Pr[X \sim \mu]{X(e) = 1}}{\Pr[X \sim \mu]{X(e) = 0}} = \frac{\mu_e(1)}{\mu_e(0)}.
\end{align*}
For $e \in E$ and $c \in \set{0, 1}$, we use $Z_{\Phi}^{(e \gets c)}$ to denote the partition function conditional on assigning $c$ to $e$, \IE,
\begin{align*}
    Z_{\Phi}^{(e \gets c)} \defeq \sum_{\sigma \in \set{0, 1}^E : \sigma(e) = c} \prod_{v \in V} f_v\left(\abs{\sigma \vert_{E_v}}\right).
\end{align*}
Then it holds that $R_{\Phi}(e) = Z_{\Phi}^{(e \gets 1)} / Z_{\Phi}^{(e \gets 0)}$. The following lemma shows that we can efficiently check the feasibility of a partial assignment.

\begin{lemma} \label{lem:partial-assignment-feasibility}
    Given an instance $\left(\Phi = (G = (V, E), \vecf = \set{f_v}_{v \in V}\right), \Delta, r)$ satisfying~\Cref{cond:Holant-condition}, for a partial assignment $\sigma$, its feasibility can be checked in time $\poly(\abs{\Lambda(\sigma)})$.
\end{lemma}

We postpone the proof of~\Cref{lem:partial-assignment-feasibility} in~\Cref{sec:proof-Holant-properties}.

\subsubsection{Marginal and ratio bounds}

To bound the marginal probability and ratio, we associate with $\Phi$ the following quantity
$$
    B_{\min} = B_{\min}(\Phi) \defeq (1 + r_{\max}^2)^{-\Delta(G)}
$$
where $r_{\max} = r_{\max}(\Phi) \defeq \max_{v \in V} \frac{f_v(1)}{f_v(0)}$. The following lemma in \zd{[CG23]} bounds the marginal probability of $\mu$.

% \hktodo{check the lemma, $\mu_{E_v^\sigma}^{\sigma}(\zero)$ is unrelated to $\sigma$ }

\begin{lemma}[Lemma 18 and Proposition 20 in~\zd{[CG23]}] \label{lem:marginal-bound}
    Given a binary symmetric Holant instance $\Phi = \left(G = (V, E), \vecf = \set{f_v}_{v \in V}\right)$ with positive log-concave signatures, for every feasible partial assignment $\sigma$ and $v \in V$, it holds that
    \begin{align} \label{eq:marginal-bound}
        \mu_{E_v^\sigma}^{\sigma}(\zero) \ge B_{\min}(\Phi).
    \end{align}
\end{lemma}

% \zdnew{For completeness, we prove~\Cref{lem:marginal-bound} in appendix.}
%For the sake of simplicity, we use $B_{\min} = B_{\min}(\Phi) \defeq (r_{\max}^2 + 1)^{-\Delta(G)}$ to denote the lower bound in~\Cref{lem:marginal-bound}.
%\begin{lemma}\label{lem:marginal-bound}
%    Consider any graph $G=(V,E=E_1\cup E_2)$ of maximum degree $\Delta$ with a positive integer $b$ and a partial assignment $\sigma$. For each $v\in V$, it holds that  
%    \begin{align*}
%         \mu^{\sigma}_{E^{\sigma}_v}(\boldsymbol{0})\geq {2^{-\Delta}}.
%    \end{align*} 
%\end{lemma}
%\begin{remark}
%    The statement of this lemma indicates its applicability to the graph with dangling edges.
%\end{remark}
%\begin{proof}
%    \qgl{immediately from czc's paper.}
%\end{proof}

% We also need the monotonicity of the uniform distribution over all $b$-matchings.

% \begin{lemma}
%     Consider any graph $G=(V,E)$ of maximum degree $\Delta$ with an integer $b\geq 1$. Given an edge $e\in E$ and feasible partial assignments $\tau_1,\tau_2:\set{e}\rightarrow \set{0,1}$ where $\tau_1(e)=0$ and $\tau_2(e)=1$, we have 
%     \begin{align*}
%         d
%     \end{align*}
% \end{lemma}
The following upper bound for the marginal ratio of a half-edge is useful in our design.
\begin{lemma} \label{lem:marginal-ratio-upper-bound}
    Given a binary symmetric Holant instance $\Phi = \left(G = (V, E = E_1 \cup E_2), \vecf = \set{f_v}_{v \in V}\right)$ with positive log-concave signatures, for each half-edge $e \in E_2$, it holds that
    \begin{align*}
        R_{\Phi}(e) \le r_{\max}(\Phi).
    \end{align*}
\end{lemma}
% \zdtodo{merge these two lemmas.}

For completeness, we prove~\Cref{lem:marginal-bound,lem:marginal-ratio-upper-bound} in~\Cref{sec:proof-Holant-properties}.

\section{The Coupling Process}

In the subsequent discussion, we consider instances satisfying the following condition.
\begin{condition}\label{cond-instancepair}
    The following holds for the tuple $\left(\Phi = (G, \vecf),  \sigma_\bot, \tau_\bot,v_{\bot}\right)$:
    \begin{itemize}
		\item $G = (V, E = E_1 \cup \set{e_\bot})$ is a graph with a unique half edge $e_\bot = (v_{\bot})$ where $v_\bot \in V$;
		\item $\vecf = \set{f_v}_{v \in V}$ is a family of binary symmetric positive log-concave signatures;
		\item $\sigma_\bot = (e_\bot \gets 1)$ and $\tau_\bot = (e_\bot \gets 0)$.
    \end{itemize}
\end{condition}

% \zdtodo{we should interpret instances satisfying Condition 2 are equivalent to instances satisfying Condition 1.}
    
For an instance $(\Phi = (G, \vecf), \sigma_\bot, \tau_\bot, v_\bot)$ satisfying~\Cref{cond-instancepair}, recall that an arbitrary order is assumed over all the edges in $E_1$.
Also recall the distribution $\mu \triangleq \mu_{\Phi}$ defined in \eqref{def-holant-mu}.
    
    % Let $\sigma, \tau$ be the partial assignments defined on the same subset of edges. We say that a vertex $v \in V$ is \emph{disagreeing under $(\sigma, \tau)$} if $\abs{{\!{Ham}\left(\sigma,{E_v}\right)}-{\!{Ham}\left(\tau,{E_v}\right)}}=1$.
    Let $\sigma, \tau$ be two partial assignments of $G$ where $\Lambda(\sigma) = \Lambda(\tau)$.
    We say that a vertex $v \in V$ is \emph{disagreeing under $(\sigma, \tau)$} if ${\!{Ham}\left(\sigma,{E_v}\right)}\neq{\!{Ham}\left(\tau,{E_v}\right)}$. Furthermore, a vertex $v$ is \emph{1-disagreeing} under $(\sigma, \tau)$ if $\abs{{\!{Ham}\left(\sigma,{E_v}\right)}-{\!{Ham}\left(\tau,{E_v}\right)}}=1$.
    %\qgl{qgl: The notion of disagreeing vertex is confusing and should be changed in the new version. Because the in-disagreeing vertices include the vertex $v$ that $\abs{{\!{Ham}\left(\sigma',{E_v}\right)}-{\!{Ham}\left(\tau',{E_v}\right)}}=0$ or $\abs{{\!{Ham}\left(\sigma',{E_v}\right)}-{\!{Ham}\left(\tau',{E_v}\right)}}>1$. However, when we say that there is a unique disagreeing vertex, we always want to say that all other vertices satisfy $\abs{{\!{Ham}\left(\sigma',{E_v}\right)}-{\!{Ham}\left(\tau',{E_v}\right)}}=0$.}
    We say that $(\sigma, \tau)$ is a pair of \emph{1-discrepancy partial assignments} if there is a unique disagreeing vertex $v \in V$ and $v$ is 1-disagreeing.

    Given a pair of 1-discrepancy partial assignments $(\sigma,\tau)$,
    the following algorithm from \qgl{[CG23]} defines a 
    coupling of $\mu^{\sigma}$ and $\mu^{\tau}$.
    
    % We first review the coupling process for the marginal distribution of the $b$-matchings conditioned on the 1-discrepancy partial assignments $\sigma$ and $\tau$ designed by \qgl{[CG23]} in Algorithm~\ref{algo:Couple}. For our subsequent analysis, we state the description of the coupling process in the following equivalent way.

	\begin{algorithm}[th]
		\caption{$\!{Couple}(\Phi, \sigma, \tau, v)$}
		\label{algo:Couple}
             % \qgl{$L\gets 0$; \quad \tcp{a global variable. I don't know how to write this appropriately in latex.} }
             \KwIn{A binary symmetric Holant instance $\Phi = \left(G = (V, E = E_1\cup \set{e_\bot}), \vecf = \set{f_v}_{v \in V}\right)$ with positive log-concave signatures, a pair of 1-discrepancy partial assignments $(\sigma, \tau)$ of $G$ where $\sigma(e_\bot) = 1, \tau(e_\bot) = 0$, and the unique {disagreeing} vertex $v \in V$ under $(\sigma, \tau)$.}
		% \KwIn{A graph $G=(V,E=E_1\cup \set{e_0})$ where the edges are labeled by an arbitrary order with partial assignments $\sigma'$ and $\tau'$ and a unique \qgl{disagreeing} vertex $v\in V$.}
		  \KwOut{A pair of assignments drawn from a coupling between $\mu^{\sigma}$ and $\mu^{\tau}$.}	
           
            % \If{$v$ is isolated}
            % {
                
            % } 
            \While{
                $E_v^{\sigma} \neq \emptyset$\label{line-while}
            }
            {
                \If{${\!{Ham}\left(\sigma, {E_v}\right)} < {\!{Ham}\left(\tau, {E_v}\right)}$}
                {
                    let $e = \set{u,v}$ be the first edge satisfying $e \in E_v^{\sigma}$ and   $\mu^{\sigma}_e(1) \geq \mu^{\tau}_e(1)$\; \label{line:pick-dominating-edge-1}
                }
                \Else{
                    let $e = \set{u,v}$ be the first edge satisfying $e\in E_v^{\sigma}$ and 
                    $\mu^{\sigma}_e(1) \leq \mu^{\tau}_e(1)$\; \label{line:pick-dominating-edge-2}
                }
                sample $(\sigma_e, \tau_e)$ from an optimal coupling of $(\mu^{\sigma}_e, \mu^{\tau}_e)$\label{line-edge-sample}\;
                $\sigma \gets \sigma \land \sigma_e$ and $\tau \gets \tau \land \tau_e$\; 
                \If{$\sigma_e \neq \tau_e$}
                {
                    % $L\gets L+1$\;
                    % $(\sigma,\tau)\gets \!{Couple}(G,\sigma',\tau',u)$; \quad 
                    % \tcp{The disagreeing vertex has changed}
                    % \hktodo{\Return{$(\sigma,\tau )$;}}
                    \hktodo{\Return $ \!{Couple}(\Phi,\sigma, \tau, u)$; \quad 
                    \tcp{The disagreeing vertex has been changed}\label{line-recursive-call}}
                }
                % \tcp{The disagreeing vertex remains the same}    
            }
           \Return an optimal coupling of $(\mu^{\sigma},\mu^{\tau})$\;     
		%\Return{$(\sigma', \tau')$;}
	\end{algorithm}

    % The correctness of the coupling relies on the following fact. 
    % \begin{lemma}\label{lem-marignal-dominance}
    %     Here we should introduce the lemma concerning the marginal dominance phenomenon.
    % \end{lemma}

\zdnew{
The following proposition demonstrates the correctness of the coupling process, which has been proved in~\zd{[CG23].
% Now we show some properties of the coupling process and the correctness of the coupling process follows immediately. The following proposition has been shown in~\zd{[CG23]. For completeness, we state it here.
}
\begin{proposition}[Proposition 16 in \zd{[CG23]}] \label{prop:coupling-correctness}
    The procedure $\!{Couple}(\Phi, \sigma, \tau, v)$ 
    satisfies the following properties:
    \begin{enumerate}
        \item \textbf{(Termination)} It always terminates.
        
        \item \textbf{(Validity of the recursive call)} At each call of $\!{Couple}(\Phi, \sigma, \tau, u)$ in Line \ref{line-recursive-call}, $(\sigma, \tau)$ is always a pair of 1-discrepancy partial assignments where $\sigma(e_\bot) = 1, \tau(e_\bot) = 0$, and $u$ is always the unique disagreeing vertex under $(\sigma, \tau)$. Moreover, $\sigma$ and $\tau$ are feasible.
        
        \item \textbf{(Existence of edges satisfying monotonicity)} When $E_v^\sigma \neq \emptyset$ in Line \ref{line-while}, if $\!Ham(\sigma, E_v) < \!Ham(\tau, E_v)$, then there always exists some $e \in E_v^\sigma$ such that $\mu_e^\sigma(1) \ge \mu^\tau_e(1)$; otherwise, there always exists some $e \in E_v^\sigma$ such that $\mu_e^\sigma(1) \le \mu^\tau_e(1)$.
        
        \item \textbf{(Correctness of the Coupling)} The outcome of $\!{Couple}(\Phi, \sigma, \tau, v)$ is a coupling of $(\mu^{\sigma}, \mu^{\tau})$.
    \end{enumerate}
\end{proposition}
\zdtodo{if necessary, prove it in appendix}
}


%\hktodo{the following statement is strange}
    
% According to~\Cref{prop:coupling-correctness}, \qgl{we claim that the partial assignments $(\sigma, \tau)$ are always 1-discrepancy} throughout the algorithm and thus the coupling process is well-defined. 
%    In the subsequent discussion, we consider the instance satisfying the following condition.
%    \begin{condition}\label{cond-instancepair}
%    The following holds for the tuple $\left(\Phi = (G, \vecf),  \sigma_\bot, \tau_\bot,v_{\bot}\right)$:
%        % Consider a binary symmetric Holant instance $\Phi = \left(G, \vecf\right)$ with a pair of partial assignments $\sigma_\bot$ and $\tau_\bot$.
%        % It holds that
%        \begin{itemize}
%            \item $G = (V, E = E_1 \cup \set{e_\bot})$ is a graph with a unique half edge $e_\bot = (v_{\bot})$ where $v_\bot \in V$;
%            \item $\vecf = \set{f_v}_{v \in V}$ is a family of binary symmetric positive log-concave signatures;
%            \item $\sigma_\bot = (e_\bot \gets 1)$ and $\tau_\bot = (e_\bot \gets 0)$. 
%        \end{itemize}
%    \end{condition}

    % We can prove the following lemma by similar analysis in \zd{[CG23]}.
Given any instance $\left(\Phi, \sigma_\bot, \tau_\bot,v_{\bot}\right)$ satisfying \Cref{cond-instancepair}, the subroutine $\!{Couple}(\cdot)$ would not be called recursively many times in the coupling procedure $\!{Couple}(\Phi, \sigma_\bot, \tau_\bot, v_\bot)$ by the following fact. The proof can be found in~\Cref{sec:omitted Proofs}.


\iffalse
    \begin{lemma}\label{lem:decay}
        Given any graph $G = (V, E = E_1 \cup \set{e_0})$ of maximum degree $\Delta$ with an integer $b\geq 1$ and a vertex $v\in V$ where the dangling edge $e_0$ is incident to $v$, let $\sigma_\perp, \tau_\perp : \set{e_0} \rightarrow \set{0, 1}$ be the partial assignments where $\sigma_\perp = e_0 \gets 0$ and $\tau_\perp = e_0 \gets 1$. For any non-negative integer $\ell$, we have
        \begin{align*}
            \Pr{L \geq \ell} \leq \left(1 - {2^{-\Delta}}\right)^{\ell}
        \end{align*}
        in the coupling procedure $\!{Couple}(G, \sigma_\perp, \tau_\perp, v)$.
    \end{lemma}
    \begin{proof}
        We prove it by induction. 
        % Note that it suffices to prove that for any positive integer $\ell$, we have 
        % \begin{align*}
        %     \Pr{L= \ell}\leq \left(1-\frac{1}{2^{\Delta}}\right)^{\ell}
        % \end{align*}
        The base case where $\ell=0$ holds immediately. Given that the statement holds for any non-negative integer $\ell$, we claim that the statement holds for $\ell+1$. Observe that the event $\set{L\geq \ell}$ is equivalent to the event that subroutine $\!{Couple}(\cdot)$ was called with at least $\ell$ times. At the beginning of the $\ell$-th call of the subroutine $\!{Couple}(\cdot)$, a new call of the subroutine $\!{Couple}(\cdot)$ happens with probability at most $1-1/2^{\Delta}$ according to~\Cref{lem:marginal-bound}. \qgl{this is because we execute an optimal coupling} Consequently, we have
        \begin{align*}
            \frac{ \Pr{L\geq \ell+1}}{ \Pr{L=\ell}}\leq \frac{1-2^{-\Delta}}{2^{-\Delta}}.
        \end{align*}
        Note that 
        \begin{align*}
            \Pr{L\geq \ell}&= \Pr{L\geq \ell+1} + \Pr{L= \ell}\\
            &\geq \Pr{L\geq \ell+1} + \Pr{L\geq \ell+1} \cdot 
            \frac{2^{-\Delta}}{1-2^{-\Delta}}\\
            &\geq \Pr{L\geq \ell+1} \cdot 
            \frac{1}{1-2^{-\Delta}}.
        \end{align*}
        Combined with the induction hypothesis, it implies that $\Pr{L\geq \ell+1}\leq \left(1-{2^{-\Delta}}\right)^{\ell+1}$ and the proof is complete.
    \end{proof}
\fi



\begin{restatable}{lemma}{OneroundDecay}\label{lem:one-round-decay}
For any input $(\Phi, \sigma, \tau, v)$ of Algorithm \ref{algo:Couple}, we have 
\[\Pr{\!{Couple}(\cdot) \text{ is recursively called in Line \ref{line-recursive-call} of $\!{Couple}(\Phi, \sigma, \tau, v)$}}\leq 1 - B_{\min}(\Phi).\]
\end{restatable}

According to~\Cref{lem:one-round-decay}, we can establish the following tail bounds for the maximum depth of recursive calls of the subroutine $\!{Couple}(\cdot)$ in $\!{Couple}(\Phi, \sigma_\bot, \tau_\bot, v_\bot)$.
\hktodo{the proof of this lemma}

\begin{lemma} \label{lem:decay}
	Given any instance $\left(\Phi,  \sigma_\bot, \tau_\bot,v_{\bot}\right)$ satisfying~\Cref{cond-instancepair}  and  any integer $\ell \ge 0$, we have
	\begin{align}\label{eq-lem-decay}
		\Pr{L \ge \ell} \le \left(1 - B_{\min}(\Phi)\right)^{\ell},
	\end{align}
        where $L$ is the {maximum depth of recursive calls} of the subroutine $\!{Couple}(\cdot)$ in $\!{Couple}(\Phi, \sigma_\bot, \tau_\bot, v_\bot)$.
        % the times that the subroutine $\!{Couple}(\cdot)$ is called in $\!{Couple}(G, \sigma_\bot, \tau_\bot, v_\bot)$.
\end{lemma}
\begin{proof}
	We prove the lemma by induction. The induction basis is when $\ell = 0$, in which case we have 
 $$\Pr{L \ge \ell} =\Pr{L \ge 0}  = 1 = \left(1 - B_{\min}\right)^{0} = \left(1 - B_{\min}\right)^{\ell}.$$
Thus, \eqref{eq-lem-decay} is proved.

For the induction step,  we have $\Pr{L \ge \ell-1} \le \left(1 - B_{\min}\right)^{\ell-1}$.
Observe that the event $\set{L \ge \ell}$ is equivalent to the event that subroutine $\Couple(\cdot)$ was called with at least $\ell$ times. At the $\ell$-th call of the subroutine $\Couple(\cdot)$ identified as $\Couple(G, \sigma, \tau, v)$, a new call happens when there exists some $e \in E_v^\sigma$ such that $\sigma_e \neq \tau_e$. By~\Cref{prop:coupling-correctness,lem:marginal-bound}, it holds that
	\begin{align*}
		\Pr{\exists e \in E_v^\sigma \cmid \sigma_e \neq \tau_e \mid \sigma, \tau} &\le 1 - \Pr{\sigma_{E_v^\sigma} = \tau_{E_v^\sigma} = \zero \mid \sigma, \tau} \\
		&\le 1 - \min\set{\mu_{E_v^\sigma}^\sigma(\zero), \mu_{E_v^\sigma}^\tau(\zero)} \\
		&\le 1 - B_{\min}.
	\end{align*}
	Consequently, we have
	$$
		\Pr{L \ge \ell + 1 \mid L \ge \ell} \le 1 - B_{\min}.
	$$
	Then it holds that
	$$
		\Pr{L \ge \ell + 1} \le (1 - B_{\min}) \Pr{L \ge \ell} \le (1 - B_{\min})^{\ell + 1},
	$$
	and the proof is immediate.
\end{proof}

In the subsequent discussion, we always consider the tuples satisfy the following conditions.
\begin{condition}\label{condition-sigma-tau}
The tuples $(\sigma,\tau,\seqS)$, $(\sigma,\tau,\seqS,e)$,  $(\sigma,\tau,\seqS, v, L)$ and $(\sigma,\tau,\seqS, v, L,e)$ satisfy the following:
\begin{itemize}
\item $(\sigma, \tau)$ is a pair of 1-discrepancy partial assignments and $\sigma(e_\bot) = 1, \tau(e_\bot) = 0$;
\item $\seqS$ records the sequence of assigned edges in $\Lambda(\sigma)\setminus \{e_\bot\}$;
\item $v$ is the unique disagreeing vertex under $(\sigma, \tau)$;
\item $L = \abs{\{e'\in \Lambda(\sigma)\mid (e'\neq e_{\bot})\land (\sigma(e')\neq \tau(e'))\}}$.
\end{itemize}
\end{condition}
For any tuple generated by certain processes, we will also ensure that \Cref{condition-sigma-tau} is satisfied, including those processes defined by Algorithm \ref{algo:Couple}, and Definitions  \ref{def:truncated-random-process} and \ref{def:truncated-coupling-tree}.
Given any $(\sigma,\tau)$, let $v(\sigma,\tau)$ denote the unique disagreeing vertex under $(\sigma, \tau)$ and $L(\sigma,\tau)$ denote $\abs{\{e\in \Lambda(\sigma)\mid (e\neq e_{\bot})\land (\sigma(e)\neq \tau(e))\}}$.
    

\subsection{Random process simulating the truncated coupling procedure}

In this section, given an instance $(\Phi = (G, \vecf), \sigma_\bot, \tau_\bot, v_\bot)$ satisfying~\Cref{cond-instancepair}, we construct a random process to simulate $\!{Couple}(\Phi, \sigma_\bot, \tau_\bot, v_\bot)$ with truncation.



% Let $(\Phi = (G, \vecf), \sigma_\bot, \tau_\bot, v_\bot)$ be an instance satisfying~\Cref{cond-instancepair}.
% % Let $\mu = \mu_\Phi$ be the Gibbs distribution of $\Phi$. 
% % Define two initial partial assignments $\sigma_\bot, \tau_\bot : \set{e_\bot} \to \set{0, 1}$ as $\sigma_\bot = e_\bot \gets 1, \tau_\bot = e_\bot \gets 0$.
% Now we simulate the random coupling procedure $\!{Couple}(\Phi, \sigma_\bot, \tau_\bot, v_\bot)$ with truncation. Given any positive integer $\ell$, we define the truncated random process $P^{\!{cp}} = P^{\!{cp}}_\ell = \set{(\sigma_t, \tau_t, \seqS_t, v_t, L_t)}_{t \geq 0}$, where $\seqS_t$ denote the updated sequence of the edges, starting from the initial state $(\sigma_0, \tau_0, \seqS_0, v_0, L_0) = (\sigma_\bot, \tau_\bot, \varnothing, v_\bot, 0)$, by repeating the following operations:
% 	% Given any graph $G = (V, E = E_1 \cup \set{e_0})$ of maximum degree $\Delta$ with an integer $b \geq 1$ and a vertex $v_0 \in V$ where $e_0\in E_{v_0}$, let $\sigma_\perp = e_0 \gets 0$ and $\tau_\perp = e_0 \gets 1$. We simulate the random coupling procedure $\!{Couple}(G, \sigma_\perp, \tau_\perp, v_0)$ with truncation.
%     % Given any positive integer $\ell$, we define the truncated random process $P^{\!{cp}}=P^{\!{cp}}_\ell=\set{(\sigma_t,\tau_t,S_t,v_t,L_t)}_{t\geq 0}$, where $S_t$ denote the updated sequence of the edges, starting from the initial state $(\sigma_0, \tau_0, S_0, v_0, L_0)=(\sigma_\perp, \tau_\perp, \bot, v_0,0)$, where $\bot$ represents the empty sequence, by repeating the following operations:
%     \begin{enumerate}
%         \item If $L_t \geq \ell$ or there are no unpinned incident edges of the disagreeing vertex under the partial assignments $\sigma_t$ and $\tau_t$, the process stops and $(\sigma_t, \tau_t, \seqS_t, v_t, L_t)$ is the outcome of the random process. 
%         \item Otherwise, let $e=\set{u,v_{t}}$ be the edge with smallest index satisfying $\mu_{e}^{\sigma_t}(1)\geq \mu_{e}^{\tau_t}(1)$ when ${\!{Ham}\left(\sigma_t,{E_v}\right)}<{\!{Ham}\left(\tau_t,{E_v}\right)}$ or $\mu_{e}^{\sigma_t}(1)\leq  \mu_{e}^{\tau_t}(1)$ when ${\!{Ham}\left(\sigma_t,{E_v}\right)}>{\!{Ham}\left(\tau_t,{E_v}\right)}$. Sample $(\sigma_e,\tau_e)$ from an optimal coupling of $(\mu_e^{\sigma_t},\mu_e^{\tau_t})$. We update $\sigma_{t + 1} \gets \sigma_t \land \sigma_e,\tau_{t + 1} \gets \tau_t \land \tau_e$, $\seqS_{t + 1} \gets \seqS_t \circ e$ and 
%         \begin{itemize}
%             \item if $\sigma_e=\tau_e$, $v_{t+1}\gets v_t$ and $L_{t+1}\gets L_t$;
%             \item otherwise, $v_{t+1}\gets u$ and $L_{t+1}\gets L_t+1$.
%         \end{itemize}
%     \end{enumerate}


    




\begin{definition}[$\ell$-truncated random process] \label{def:truncated-random-process}
    \emph{
    For any instance $\left(\Phi = (G, \vecf),  \sigma_\bot, \tau_\bot,v_{\bot}\right)$ satisfying \Cref{cond-instancepair} and  any positive integer $\ell$, the $\ell$-truncated 
    random process $P^{\!{cp}} \triangleq P^{\!{cp}}_\ell(\Phi, \sigma_\bot, \tau_\bot, v_\bot) = \set{(\sigma_t, \tau_t, \seqS_t, v_t, L_t)}_{0\leq t \leq T}$ repeat the following operations:
    \begin{enumerate}
        \item The initial state is $(\sigma_0, \tau_0, \seqS_0, v_0, L_0) = (\sigma_\bot, \tau_\bot, \varnothing, v_\bot, 0)$.
        \item For $i = 0, 1, \cdots$: for the state $(\sigma_t, \tau_t, \seqS_t, v_t, L_t)$:
        \begin{enumerate}
            \item          
            If $L_t \geq \ell$ or $E_{v_t}^{\sigma_t}=\emptyset$, then the process lets $T \leftarrow t$, outcomes $(\sigma_t, \tau_t, \seqS_t, v_t, L_t)$, and stops.
            \item Otherwise, $E_{v_t}^{\sigma_t}\neq \emptyset$. 
            If ${\!{Ham}\left(\sigma_t, {E_{v_t}}\right)} < {\!{Ham}\left(\tau_t, {E_{v_t}}\right)}$,
            let $e = \set{u,v_t}$ be the first edge in $E_{v_t}^{\sigma_t}$ with 
            $\mu^{\sigma_t}_e(1) \geq \mu^{\tau_t}_e(1)$.
            Otherwise, let $e = \set{u,v_t}$ be the first edge in $E_{v_t}^{\sigma_t}$ with $\mu^{\sigma_t}_e(1) \leq \mu^{\tau_t}_e(1)$.
                \begin{enumerate}[(i)]
                    \item Sample $(\sigma_e,\tau_e)$ from an optimal coupling of $(\mu_e^{\sigma_t},\mu_e^{\tau_t})$.
                    \item Let $\sigma_{t + 1} \gets \sigma_t \land \sigma_e,\tau_{t + 1} \gets \tau_t \land \tau_e$, $\seqS_{t + 1} \gets \seqS_t \circ e$.
                    If $\sigma_e=\tau_e$, let $v_{t+1}\gets v_t, L_{t+1}\gets L_t$; otherwise, let $v_{t+1}\gets u$ and $L_{t+1}\gets L_t+1$.
                \end{enumerate} \label{item:construction-expanding-step}
        \end{enumerate}
    \end{enumerate} 
    }
\end{definition}

Intuitively, for any state $(\sigma_t, \tau_t, \seqS_t, v_t, L_t)$ in $P^{\!{cp}}$, $\seqS_t$ records the sequence of chosen edges and $L_t$ records the total number of edges $e$ in $\seqS_t$ where $\sigma_t(e)\neq \tau_t(e)$.
Moreover, one can verify the following lemma by \Cref{def:truncated-random-process} and the induction.


% Thus, one can verify that 
% $L_t = L(\sigma_t,\tau_t)$.
% Moreover, by comparing \Cref{def:truncated-random-process} with Algorithm \ref{algo:Couple},
% one can verify that $P^{\!{cp}}$ simulates $\!{Couple}(\Phi, \sigma_\bot, \tau_\bot, v_\bot)$.
% Combining with \Cref{prop:coupling-correctness},
% we have that $(\sigma_t, \tau_t)$ is a pair of 1-discrepancy partial assignments where $\sigma_t(e_\bot) = 1, \tau_t(e_\bot) = 0$, and $v_t = v(\sigma_t,\tau_t)$.
% In summary, we have the following lemma.

\begin{lemma}\label{lemma-trp-correctness}
In the $\ell$-truncated random process $P^{\!{cp}} \triangleq P^{\!{cp}}_\ell(\Phi, \sigma_\bot, \tau_\bot, v_\bot) = \set{(\sigma_t, \tau_t, \seqS_t, v_t, L_t)}_{0\leq t \leq T}$, we have 
$(\sigma_t, \tau_t,\seqS_t, v_t, L_t)$ satisfies \Cref{condition-sigma-tau} for each $0\leq t \leq T$.
\end{lemma}\qtodo{where use this lemma}

% The following property about $(\sigma_{t},\tau_{t},\seqS_t,v_t,L_t)$ is immediate by \Cref{def:truncated-random-process}.

% \begin{lemma}
% For each $0<t\leq T$, we have 
% \[
% \sigma_t = \sigma_{t-1} \land (e \gets a),\tau_t = \tau_{t-1} \land (e \gets b), \seqS = \seqS_{t-1} \circ e, e = \{u,v_t\}.\] 
% for some $u\in V(G)$ and $a,b\in \{0,1\}$.
% Moreover, if $a =  b$, then $v_{t-1}=v_t,L_{t-1} = L_{t}$.
% Otherwise, $v_{t-1}=u,L_{t-1} = L_{t}-1$.
% \end{lemma}

% \begin{corollary}\label{cor-trp-vt}
% In the $\ell$-truncated random process $P^{\!{cp}} \triangleq P^{\!{cp}}_\ell(\Phi, \sigma_\bot, \tau_\bot, v_\bot) = \set{(\sigma_t, \tau_t, \seqS_t, v_t, L_t)}_{t \geq 0}$, $(\sigma_t, \tau_t)$ is always a pair of 1-discrepancy partial assignments where $\sigma_t(e_\bot) = 1, \tau_t(e_\bot) = 0$, and $v_t$ is always the unique disagreeing vertex under $(\sigma_t, \tau_t)$ for each $t\geq 0$.
% \end{corollary}



% Combining with \Cref{cor-trp-vt} and \eqref{eq-trp-lt},
% we have $v_t = v(\sigma_t, \tau_t)$ and $L_t = L(\sigma_t, \tau_t)$.

% Combining \Cref{cor-trp-vt} with \eqref{eq-trp-lt},
% we have $v_t,L_t$ are uniquely determined by $\sigma_t$ and $ \tau_t$.
% Therefore,  given any $(\sigma, \tau, \seqS, v, L)\in \+V^{\!{cp}}$, 
% one can recover $v, L$ from $\sigma, \tau$.
% Thus, we may omit $v$ and $L$ from the notations when they are clear from the context. 

Given a tuple $(\sigma,\tau,\seqS, v,L)$, we call the tuple \emph{feasible} if $\sigma$ and $\tau$ are feasible partial assignments. By comparing \Cref{def:truncated-random-process} with Algorithm \ref{algo:Couple}, one can verify that $P^{\!{cp}} \triangleq P^{\!{cp}}_\ell(\Phi, \sigma_\bot, \tau_\bot, v_\bot)$ simulates $\!{Couple}(\Phi, \sigma_\bot, \tau_\bot, v_\bot)$ while ensuring that the depth of recursion does not exceed $\ell$.
Thus, by the second item of \Cref{prop:coupling-correctness}, we have the following lemma. 

\begin{lemma}\label{lemma-trp-correctness-feasible}
In the $\ell$-truncated random process $P^{\!{cp}} \triangleq P^{\!{cp}}_\ell(\Phi, \sigma_\bot, \tau_\bot, v_\bot) = \set{(\sigma_t, \tau_t, \seqS_t, v_t, L_t)}_{0\leq t \leq T}$, we have 
$(\sigma_t, \tau_t,\seqS_t, v_t, L_t)$ is feasible for each $0\leq t \leq T$.
\end{lemma}\qtodo{where use this lemma}



The following notations related to the random process $P^{\!{cp}} \triangleq P^{\!{cp}}_\ell(\Phi, \sigma_\bot, \tau_\bot, v_\bot) = \set{(\sigma_t, \tau_t, \seqS_t, v_t, L_t)}_{0\leq t \leq T}$ will be used in its analysis. 
\begin{definition}\label{def-notation-trp}
Define $\mu^{\!{cp}},\+L^{\!{cp}},\+V^{\!{cp}},\+L_{\ell}^{\!{cp}}$ and $\mathbf{Pr}_{\!{cp}}$ as follows.
\begin{itemize}
\item Let $\mu^{\!{cp}}\triangleq\mu_\ell^{\!{cp}}$ denote the distribution of the outcome of the random process $P^{\!{cp}}$.
% \item 
% Let $\rho \triangleq \rho^{\!{cp}}$ denote the length of $P^{\!{cp}}$. Formally, $\rho = r$ if  $(\sigma_r, \tau_r, \seqS_r, v_r, L_r)$ is the outcome of $P^{\!{cp}}$.
\item For any $(\sigma, \tau, \seqS, v, L)$, let $\Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)}$ denote the probability that $(\sigma, \tau, \seqS, v, L)$ is a state in the random process $P^{\!{cp}}$.
Formally, for each $(\sigma, \tau, \seqS, v, L)$  where  $\abs{\seqS}=t$,
\begin{align}\label{eq-def-pro-stsvl}
    \Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)} \triangleq \Pr{\left(T\geq t\right)\land \left((\sigma, \tau, \seqS, v, L) = (\sigma_t, \tau_t, \seqS_t, v_t, L_t)\right)}.
\end{align}
For each $(\sigma, \tau, \seqS)$, 
let $\Pr[\!{cp}]{(\sigma, \tau, \seqS)}$ denote $\Pr[\!{cp}]{(\sigma, \tau, \seqS, v(\sigma,\tau), L(\sigma,\tau))}$.
% let $\Pr[\!{cp}]{(\sigma, \tau, \seqS)}$ denote $\max_{v,L}\Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)}$.\qtodo{semms strange? is there a unique pair $(v,L)$ meaningful?}
\item For any $(\sigma, \tau, \seqS, v, L)$ and $e\in E(G)$, let $\Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L,e)}$ denote the probability that $(\sigma, \tau, \seqS, v, L)$ is a state in the random process $P^{\!{cp}}$ and $e$ is the chosen edge at that state.
Formally, for each $(\sigma, \tau, \seqS, v, L)$  where  $\abs{\seqS}=t$ and each $e\in E(G)$,
{\begin{align}\label{eq-def-pro-stsvle}
\Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L,e)} \triangleq \Pr{\left(T > t\right)\land \left((\sigma, \tau, \seqS, v, L) = (\sigma_t, \tau_t, \seqS_t, v_t, L_t)\right)\land \left(\seqS_{t+1} =\seqS\circ e\right) }.
\end{align}}
For each $(\sigma, \tau, \seqS)$ and each $e\in E(G)$, let $\Pr[\!{cp}]{(\sigma, \tau, \seqS,e)}$ denote $\Pr[\!{cp}]{(\sigma, \tau, \seqS, v(\sigma,\tau), L(\sigma,\tau),e)}$.
\item Let $\+L^{\!{cp}}$ and $\+V^{\!{cp}}$ denote all possible outcomes and states of $P^{\!{cp}}$, respectively. Formally, 
$$
    \+L^{\!{cp}} = \set{(\sigma, \tau, \seqS, v, L) \mid \mu_\ell^{\!{cp}}((\sigma, \tau, \seqS, v, L))>0},\quad  \+V^{\!{cp}} = \set{(\sigma, \tau, \seqS, v, L) \mid \Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)}>0}.
$$
\item Let $\+L_{\ell}^{\!{cp}}$ denote the set of outcomes $(\sigma, \tau, \seqS, v, L)$ of $P^{\!{cp}}$ with $L = \ell$, i.e.:
$$
    \+L_{\ell}^{\!{cp}} = \set{(\sigma, \tau, \seqS, v, L) \mid \mu_\ell^{\!{cp}}((\sigma, \tau, \seqS, v, L))>0 \land L = \ell}.
$$
% \item Let $\+W^{\!{cp}} \subseteq \+V^{\!{cp}} \setminus \+L^{\!{cp}}$ denote all possible states $(\sigma_t, \tau_t, \seqS_t, v_t, L_t)$ of $P^{\!{cp}}$ where $0\leq t \leq \rho$ and $v_t\neq v_{t-1}$. Formally,
% $$
%     \+W_{\ell}^{\!{cp}} = \set{(\sigma_t, \tau_t, \seqS_t, v_t, L_t) \mid (t>0)\land (\sigma_t, \tau_t, \seqS_t, v_t, L_t)\in \+V^{\!{cp}} \setminus \+L^{\!{cp}} \land L = \ell}.
% $$
\end{itemize}
\end{definition}

% \hktodo{check here}  \qgl{check the definiotion of $\+W^{\!{cp}}$.}

% Let $\+W^{\!{cp}} \subseteq \+V^{\!{cp}} \setminus \+L^{\!{cp}}$ denote all possible states $(\sigma_t, \tau_t, \seqS_t, v_t, L_t)$ of $P^{\!{cp}}$ where $0\leq t \leq \rho$ and $v_t\neq v_{t-1}$. Formally,
% % $$
% %     \+W_{\ell}^{\!{cp}} = \set{(\sigma_t, \tau_t, \seqS_t, v_t, L_t) \mid (t>0)\land (\sigma_t, \tau_t, \seqS_t, v_t, L_t)\in \+V^{\!{cp}} \setminus \+L^{\!{cp}} \land L = \ell}.

By \Cref{def-notation-trp}, for any $(\sigma, \tau, \seqS, v, L)$ we have 
\begin{align}\label{eq-pr-sts-eq-pr-stsvl}
\Pr[\!{cp}]{(\sigma, \tau, \seqS)} = \Pr[\!{cp}]{(\sigma, \tau, \seqS, v(\sigma,\tau), L(\sigma,\tau))} = \Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)},
\end{align}
where the last equality is by \Cref{condition-sigma-tau}.
Similarly, for any $(\sigma, \tau, \seqS, v, L,e)$ we have 
\begin{align}\label{eq-pr-stse-eq-pr-stsvle}
\Pr[\!{cp}]{(\sigma, \tau, \seqS,e)} = \Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L,e)},
\end{align}

The following property on the random process $P^{\!{cp}}$ will be used in its analysis. For completeness, we provide its proof in~\Cref{sec:omitted Proofs}.


\begin{restatable}{lemma}{PropertyDefrpc}\label{lemma-property-def-rpc}
In \Cref{def:truncated-random-process}, the following properties hold:
\begin{enumerate}
\item For each $(\sigma, \tau, \seqS, v, L)$, we have 
\begin{align}\label{eq-def-trp-perperty-sigma}
\Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)}\leq \mu^{\sigma_{\bot}}_{\seqS}(\sigma), \quad \Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)}\leq\mu^{\tau_{\bot}}_{\seqS}(\tau).
\end{align}
\item For each $(\sigma, \tau, \seqS, v, L,e)$, we have 
\begin{align}\label{eq-def-trp-perperty-sigma-e}
\Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L,e)}\leq \mu^{\sigma_{\bot}}_{\seqS}(\sigma),\quad \Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L,e)}\leq\mu^{\tau_{\bot}}_{\seqS}(\tau).
\end{align}
\end{enumerate}
\end{restatable}



\hktodo{polish this paragraph}

\hktodo{show that every state is feasible}

Since the random process is to simulate $\!{Couple}(G, \sigma_\bot, \tau_\bot, v_\bot)$, we have the following fact immediately.

\begin{lemma} \label{lem:random-process-decay}
	Given any instance $(\Phi, \sigma_\bot, \tau_\bot, v_\bot)$ satisyfing~\Cref{cond-instancepair}, for any non-negative integer $\ell$, we have
	\begin{align*}
		\mu^{\!{cp}}(\+L_\ell^{\!{cp}}) \leq \left(1 - B_{\min}\right)^{\ell}.
	\end{align*}
\end{lemma}



%\begin{lemma} \label{lem:random-process-decay}
%	Consider a binary symmetric Holant instance $\Phi = (G, \vecf = \set{f_v}_{v \in V})$ with positive log-concave signatures and a graph $G = (V, E = E_1 \cup \set{e_\bot})$ of maximum degree $\Delta$ containing a unique dangling edge $e_\bot$ incident to $v_\bot \in V$. Let $\sigma_\bot, \tau_\bot : \set{e_\bot} \to \set{0, 1}$ be two partial assignments where $\sigma_\bot = e_\bot \gets 1$, $\tau_\bot = e_\bot \gets 0$. For any non-negative integer $\ell$, we have
%	\begin{align*}
%		\mu^{\!{cp}}(\+L_\ell^{\!{cp}}) \leq \left(1 - B_{\min}\right)^{\ell}
%	\end{align*}
%	in the truncated coupling process $P^{\!{cp}}$ starting from $(\sigma_\bot, \tau_\bot, \varnothing, v_\bot, 0)$.
%\end{lemma}

%\begin{lemma}\label{lem:random-process-decay}
%    Given any graph $G = (V, E = E_1 \cup \set{e_\bot})$ of maximum degree $\Delta$ with a unique dangling edge $e_0$ incident to $v_0 \in V$ and an integer $b\geq 1$, let $\sigma_\perp, \tau_\perp: \set{e_0}\rightarrow \set{0,1}$ be the partial assignments where $\sigma_\perp = e_0 \gets 0$ and $\tau_\perp = e_0 \gets 1$. For any non-negative integer $\ell$, we have
%        \begin{align*}
%            \mu^{\!{cp}}(\+L_\ell^{\!{cp}})\leq \left(1-{2^{-\Delta}}\right)^{\ell}
%        \end{align*}
%        in the truncated coupling process $P^{\!{cp}}$ starting from $(\sigma_\perp, \tau_\perp, \varnothing, v_0, 0)$.
%\end{lemma}

\subsection{Truncated coupling tree} 

\hktodo{polish the following paragraph}

\zdnew{To estimate the marginal probability, we employ Moitra's method. First we construct a truncated coupling tree, and then we set up linear programming on it.} \qgl{However, since the choice of the edge in Line~\ref{line:pick-dominating-edge-1} and~\ref{line:pick-dominating-edge-2} is unknown, the coupling tree is implicit}\qtodo{to clarify it more carefully}. To overcome this barrier, we define the following recursion tree to mimic the random process $P^{\!{cp}}$.

%    In this section, we construct a coupling tree including the whole support $\+V^{\!{cp}}$ of the random truncated process $P^{\!{cp}}$. \qgl{add more words here. This is the best we can expect since we can not simulate the random process faithfully.}
%    We define the following recursion tree to mimic the random process $P^{\!{cp}}$.

\hktodo{Here, for each node, only three children are added. How about adding four children?}



\begin{definition}[$\ell$-truncated coupling tree] \label{def:truncated-coupling-tree}
    \emph{
    For any instance $\left(\Phi = (G, \vecf),  \sigma_\bot, \tau_\bot,v_{\bot}\right)$ satisfying \Cref{cond-instancepair} and  any positive integer $\ell$, the $\ell$-truncated coupling tree $\+T \triangleq \+T_{\ell}(\Phi, \sigma_\bot, \tau_\bot, v_\bot)$ is a rooted tree constructed as follows:      \begin{enumerate}
        \item The root of $\+T$ is the node with label $(\sigma_\bot, \tau_\bot, \varnothing, v_\bot, 0)$ of depth $0$.
        \item For $i = 0, 1, \cdots$: for each node $u$  of depth $i$ in the current $\+T$, suppose the label of $u$ is $(\sigma, \tau, \seqS, v, L)$.
        \begin{enumerate}
            \item          
            If $L \geq \ell$ or $E_v^{\sigma}=\emptyset$ or $(\sigma, \tau, \seqS, v, L)$ is infeasible, then $u$ is a leaf node in $\+T$;
            \item Otherwise, for each $e = \set{v,v'} \in E_v^{\sigma}$,
                \begin{enumerate}[(i)]
                    \item If ${\!{Ham}\left(\sigma,{E_v}\right)}<{\!{Ham}\left(\tau,{E_v}\right)}$, add three nodes with labels $(\sigma \land (e \gets 0), \tau \land (e \gets 0), \seqS \circ e, v, L)$, $(\sigma \land (e\gets 1), \tau \land (e \gets 0), \seqS \circ e, v', L+1)$, and $(\sigma \land (e \gets 1), \tau \land (e \gets 1), \seqS \circ e, v, L)$ as children of $u$;
                    \item Otherwise, add three nodes with labels $(\sigma \land (e \gets 0), \tau \land (e \gets 0), \seqS \circ e,v,L)$, $(\sigma \land (e \gets 0), \tau \land (e \gets 1), \seqS \circ e, v', L + 1)$, and $(\sigma \land e \gets 1, \tau \land (e \gets 1), \seqS\circ e,v,L)$ as children of $u$.
                \end{enumerate} \label{item:construction-expanding-step}
        \end{enumerate}
    \end{enumerate} 
    \qgl{For simplicity, let $V(\+T)$ denote the set of nodes in $\+T$.}\qtodo{seems not necessary, $\+T$ is a graph, an we have define the notation $V()$ for graphs.}    
    }
\end{definition}

% \hktodo{For each node $(\sigma, \tau, \seqS, v, L)\in V(\+T)$, we call it feasible if $\sigma$ and $\tau$ are feasible partial assignments.
    % Define $\+V$ as the set of \emph{feasible} nodes in the coupling tree $\+T$.
    % Let $\+L$ be the set of \emph{feasible} leaf nodes in $\+T$, $\+L_{\!{good}}\triangleq \set{(\sigma,\tau, \seqS,v,L) \mid L<\ell}$, and $\+L_{\!{bad}}\triangleq \+L\setminus \+L_{\!{good}}$. One can verify that $\+V^{\!{cp}}\subseteq \+V$.}

% \hktodo{the following paragraph should be changed}

% Observe that for a node $(\sigma, \tau, \seqS, v, L) \in V(\+T)$, the values of $v$ and $L$ are determined by $(\sigma, \tau, \seqS)$. Then for convenience, we sometimes use $(\sigma, \tau, \seqS)$ to index the node $(\sigma, \tau, \seqS, v, L)$. The following fact shows that $\+T$ is well-defined and contains the \qgl{real coupling tree.}\qtodo{change this.}


By \Cref{def:truncated-coupling-tree} and the induction, one can verify the following properties:
\begin{itemize}
\item For each node $u$ with label $(\sigma,\tau,\seqS,v,L)$ and depth $i$, we have $\abs{\seqS} = i$.
Thus, any two nodes at different depths have distinct labels.
\item For each node $u\in V(\+T)$, the children of $u$ have distinct labels. Moreover, for any two nodes $u,v\in V(\+T)$ with distinct labels, the child $u'$ of $u$ and the child $v'$ of $v$ have distinct labels.
Thus, by using induction on depth, we can also prove that any two nodes at the same depth have distinct labels.
\end{itemize}
In summary, we have the following lemma.
\begin{lemma}\label{lemma-truncated-coupling-tree}
Any two different nodes in the $\ell$-truncated coupling tree have distinct labels.
\end{lemma}

By \Cref{lemma-truncated-coupling-tree}, one can use a label $(\sigma, \tau, \seqS, v, L)$ to refer to the unique node with this label in the $\ell$-truncated coupling tree $\+T$. 
Thus, we will say `the node $(\sigma, \tau, \seqS, v, L)$', `the feasible node $(\sigma, \tau, \seqS, v, L)$' rather than `the node with label $(\sigma, \tau, \seqS, v, L)$' and `the node with feasible label $(\sigma, \tau, \seqS, v, L)$', respectively.

By comparing \Cref{def:truncated-random-process} with \Cref{def:truncated-coupling-tree} and recalling that all the states in \Cref{def:truncated-random-process} are feasible as shown in \Cref{lemma-trp-correctness-feasible},
one can verify that each possible state sequence of the $\ell$-truncated random process $P^{\!{cp}}$ is recorded as a path from the root to one leaf in the $\ell$-truncated coupling tree $\+T$.
Thus, we have the following lemma. 
\begin{lemma}\label{lemma-states-trp-path-tct}
For the random process $P^{\!{cp}} = P^{\!{cp}}_\ell(\Phi, \sigma_\bot, \tau_\bot, v_\bot) = \set{(\sigma_t, \tau_t, \seqS_t, v_t, L_t)}_{0\leq t \leq T}$, any integer $r$, and any fixed tuples $((\sigma'_1, \tau'_1, \seqS'_1, v'_1, L'_1),\cdots,(\sigma'_r, \tau'_r, \seqS'_r, v'_t, L'_r))$
where 
\[\Pr{(T=r)\land\left((\sigma_1, \tau_1, \seqS_1, v_1, L_1)=(\sigma'_1, \tau'_1, \seqS'_1, v'_1, L'_1)\right)\land\cdots\land \left((\sigma_r, \tau_r, \seqS_r, v_r, L_r)=(\sigma'_r, \tau'_r \seqS'_r, v'_r, L'_r)\right)}>0,\]
there exists a path $(\sigma_\bot, \tau_\bot, \varnothing, v_\bot, 0),(\sigma'_1, \tau'_1, \seqS'_1, v'_1, L'_1),\cdots,(\sigma'_r, \tau'_r \seqS'_r, v'_r, L'_r)$ in the $\ell$-truncated coupling tree $\+T$ where  $(\sigma'_r, \tau'_r \seqS'_r, v'_r, L'_r)$ is a leaf.
\end{lemma}





The following property on the nodes in $V(\+T)$ is immediate by \Cref{def:truncated-coupling-tree} and the induction.
\begin{lemma}\label{lemma-property-tct-vl}
Each node $(\sigma, \tau, \seqS, v, L)\in V(\+T)$ satisfies \Cref{condition-sigma-tau}.
\end{lemma}


% The following property is immediate by \Cref{def:truncated-coupling-tree}.
% \qgl{check from here}
% \begin{lemma}\label{lemma-property-def-tct}
% In \Cref{def:truncated-coupling-tree}, the following properties hold:
% \begin{enumerate}
% \item The label $(\sigma , \tau, \seqS, v, L)$ of 
% each non-root node $u$ in $\+T$ satisfies
% \[
% \sigma = \sigma' \land (e \gets a),\tau = \tau' \land (e \gets b), \seqS = \seqS' \circ e, e = \{v,v'\}\] 
% for some $\sigma',\tau',\seqS',v'\in V(G)$ and some $a,b\in \{0,1\}$.
% Moreover, if $a =  b$, then 
% $u$ is a child of the node with label $(\sigma', \tau', \seqS', v, L)$.
% Otherwise, $u$ is a child of the node with label $(\sigma', \tau', \seqS', v', L-1)$.

% \item For any node $u$ with label $(\sigma , \tau, \seqS, v, L)$ in $\+T$, any $w\in V(G)$, and any $a,b\in \{0,1\}$,
% at most one child of $u$ has the label $(\sigma',\tau' ,\seqS' , v', L')$ for some  $v', L'$ where 
% \[\sigma' = \sigma\land(e\leftarrow a), \tau'=\tau\land (e\leftarrow b),\seqS'=\seqS\circ e, e = \{v,w\}.\]
% Moreover, if $a =  b$, we have $v'=v$ and $L'=L$.
% Otherwise, we have $v'=w$ and $L'=L+1$.
% \end{enumerate}
% \end{lemma}


% By \Cref{lemma-property-def-tct}, we have the following property of the $\ell$-truncated coupling tree.
% \begin{corollary}\label{cor-truncated-coupling-tree}
% Any two different nodes in the $\ell$-truncated coupling tree have different labels.
% \end{corollary}
% % \begin{proof}
% % Given an $\ell$-truncated coupling tree $\+T$,
% % let $\+T^{i}$ be the truncation of $\+T$ by deleting all the nodes in $\+T$ with depths larger than $i$.
% % Specifically, $\+T^{0}$ is the truncated tree with only the root of $\+T$. 
% % To prove this corollary, it is sufficient to prove the claim that for each $i\geq 0$, any two different nodes in $\+T^{i}$ have different labels.
% % In the following, we prove this claim by induction.

% % The induction basis is when $i=0$, in which case $\+T^{0}$ has only one node, and the claim is immediate.

% % For the induction step, for any node $u$ in $\+T^{i+1}$, we show that any node $v\neq u$ in  $\+T^{i+1}$ have different label with $u$.
% % Assume that $u$ and $v$ have the same label for contradiction.
% % By the second item of \Cref{lemma-property-def-tct}, 
% % we have $u,v$ have different fathers.
% % Let $u',v'$ denote the fathers of $u,v$, respectively.
% % By the first item of \Cref{lemma-property-def-tct} and $u,v$ have the same label,
% % we have $u',v'$ have the same label.
% % In addition, by $u$ is in $\+T^{i+1}$ and $u'$ is the father of $u$,
% % we have $u'$ is in $\+T^{i}$.
% % Similarly, $v'$ is also in $\+T^{i}$.
% % Combining with the induction hypothesis, we have 
% % $u',v'$ have different labels.
% % This is contradictory with that $u',v'$ have the same label.
% % In summary, we have $u$ and $v$ have different labels,
% % which finishes the induction.
% % Then the lemma is proved.
% % \end{proof}

% By \Cref{cor-truncated-coupling-tree}, one can use a label $(\sigma, \tau, \seqS, v, L)$ to refer to the unique node with this label in the $\ell$-truncated coupling tree $\+T$. 
% Thus, we will say `the node $(\sigma, \tau, \seqS, v, L)$' rather than `the node with label $(\sigma, \tau, \seqS, v, L)$'.

% \zdnew{
% For a node $\alpha = (\sigma, \tau, \seqS, v, L) \in V(\+T)$, we use $\sigma^\alpha, \tau^\alpha, \seqS^\alpha, v^\alpha$ and $L^\alpha$ to denote the values revealed by $\alpha$ respectively. \zdtodo{use a simpler notation for it.}
% Moreover, define the subset $\+W$ as
% \begin{align*}
%     \+W \defeq \set{\alpha \in V(\+T) \setminus \+L \mid \alpha \neq \rho(\+T) \land v^{\alpha}\neq v^{\Par(\alpha)}}.
% \end{align*}
% }\qtodo{this can be moved.}


The following notations related to $\+T$ will be used in its analysis.
\begin{definition}\label{def-notation-v-tct}
Define the sets $\+V,\+L,\+L_{\!{good}}$, the function $\+D(\cdot)$ as follows:
\begin{itemize}
\item Let $\+V$ denote the set of \emph{feasible} nodes in $V(\+T)$;
\item Let $\+L$ denote the set of \emph{feasible} leaf nodes in $V(\+T)$;
\item Define $\+L_{\!{good}}\triangleq \set{(\sigma,\tau, \seqS,v,L) \in \+L\mid L<\ell}$ and $\+L_{\!{bad}}\triangleq \+L\setminus \+L_{\!{good}}$.
\item Given any $(\sigma,\tau,\seqS,v,L)\in V(\+T)$, define 
\[
\+D(\sigma,\tau,\seqS,v,L)\triangleq \left\{(\sigma',\tau',\seqS',v',L')\in V(\+T) \mid \sigma' = \sigma \land (E^{\sigma}_v \gets \boldsymbol{0}),\tau' = \tau \land (E^{\sigma}_v \gets \boldsymbol{0})\right\}.
\]
\end{itemize}
\end{definition}
We remark the following facts about $\+D(\sigma,\tau,\seqS,v,L)$:
\begin{itemize}
\item if $E^{\sigma}_v = \emptyset$, then $\+D(\sigma,\tau,\seqS,v,L) = \{(\sigma,\tau,\seqS,v,L)\}$;
\item if $(\sigma,\tau,\seqS,v,L)$ is infeasible and $E^{\sigma}_v \neq \emptyset$, then 
$\+D(\sigma,\tau,\seqS,v,L) =\emptyset$; 
\item if $\abs{E^{\sigma}_v} > 1$, then $\abs{\+D(\sigma,\tau,\seqS,v,L)}$ can be larger than $1$, because each permutation $e_1,e_2,\cdots,e_{\abs{E^{\sigma}_v}}$ of the edges in $E^{\sigma}_v$ results in a possible sequence $\seqS' = \seqS\circ e_1 \circ e_2\cdots\circ e_{\abs{E^{\sigma}_v}}$.
\end{itemize}

% that if $E^{\sigma}_v = \emptyset$, then $\+D(\sigma,\tau,\seqS,v,L) = \{(\sigma,\tau,\seqS,v,L)\}$.
% Moreover, if $\abs{E^{\sigma}_v} > 1$, then $\abs{\+D(\sigma,\tau,\seqS,v,L)}$ can be larger than $1$, because each permutation $e_1,e_2,\cdots,e_{\abs{E^{\sigma}_v}}$ of the edges in $E^{\sigma}_v$ results in a possible sequence $\seqS' = \seqS\circ e_1 \circ e_2\cdots\circ e_{\abs{E^{\sigma}_v}}$.



% Given any $(\sigma, \tau, \seqS, v, L)\in V(\+T)$, by \Cref{lemma-property-tct-vl},
% we have $v,L$ are uniquely determined by $\sigma$ and $ \tau$.
% Thus, we may omit $v$ and $L$ from the notations when they are clear from the context.

Recall that $\+V^{\!{cp}}$ denotes all possible states of $P^{\!{cp}}$. The following proposition provides some useful properties of the $\ell$-truncated coupling tree.


\begin{proposition} \label{prop:property-of-truncated-tree}
$\+T$ is of degree at most $3\Delta$, of depth at most $\Delta \ell$ and of size at most $\left(3\Delta\right)^{\Delta \ell + 1}$.
Moreover, we have $\+V^{\cp} \subseteq V(\+T)$, and each non-leaf node in $\+T$ is in the set $\+V\setminus\+L$.
\end{proposition}

\hktodo{polish the proof here}
\begin{proof}
	To see the depth of $\+T$, we focus on the evolution of $L$. For each node $(\sigma, \tau, \seqS, v, L) \in V(\+T)$, according to~\eqref{item:construction-expanding-step} in~\Cref{def:truncated-coupling-tree}, once $L$ increases, the vertex $v$ will turn to another vertex $u$ adjacent to $v$. Let the node after increasing $L$ be denoted by $(\sigma', \tau', \seqS', u, L + 1)$. Since each edge only occurs at most one time, there are at most $\Delta$ nodes in the path from $(\sigma, \tau, \seqS, v, L)$ to $(\sigma', \tau', \seqS', u, L + 1)$. Then by truncation, the depth of $\+T$ is at most $\Delta \ell$.
	
	The degree of $\+T$ comes directly from the fact that $\abs{E_v^\sigma} \le \abs{E_v} \le \Delta$.

    \hktodo{rewrite this paragraph}
    
    By \Cref{lemma-states-trp-path-tct}, 
    we have $\+V^{\cp} \subseteq V(\+T)$ immediately.
    In addition, by \Cref{def:truncated-coupling-tree},  each infeasible node is a leaf in $\+T$.
    Thus, each non-leaf node is feasible.
    Therefore, this node is in $ \+V\setminus\+L$.
    
\end{proof}

% By~\Cref{prop:correctness-of-truncated-tree}, the computation cost of $\+T$ is an immediate corollary.

% \begin{corollary} \label{cor:truncated-tree-computation-cost}
% 	The $\ell$-truncated coupling tree $\+T = \+T_\ell(G, \sigma_\bot, \tau_\bot, v_\bot)$ can be computed in time polynomial with $\Delta^{\Delta\ell}$.
% \end{corollary}

%\qgl{We should verify that the tree is well-defined. It suffices to show that there exist no nodes that occur multiple times. We also remark that $\+L^{\!{cp}}\subseteq \+L$ since we could choose each unpinned incident edge of the disagreeing vertex to update instead of the monotonicity edge}.
%
%
%\qgl{fromhere} Note that for any $(\sigma,\tau,S,v,L)\in \+V(\+T)$, the value of $v$ and $L$ are determined by the tuple $(\sigma,\tau,S)$. For convenience, we represent the nodes $(\sigma,\tau,S,v,L)$ by $(\sigma,\tau,S)$, and the value of $v$ and $L$ are denoted by $v(\sigma,\tau,S)$ and $L(\sigma,\tau,S)$, respectively. 
%
%\qgl{we should introduce a lemma here to specify the computational cost for the coupling tree construction. Moreover, we remark that the feasibility of the nodes can be decided in polynomial time.}
%\begin{lemma}
%    coupling tree construction computation cost.
%\end{lemma}

\subsection{The marginal quantities from the coupling procedure}

\hktodo{polish the following paragraph}

In this section, we shall define a collection of quantities and verify some useful properties among them in the random process $P^{\!{cp}}$. 


We define the following quantities:
\begin{itemize}
\item For each node $(\sigma, \tau, \seqS,v,L) \in \+V$, define
    \begin{align}\label{eqn-marginal-all}
        p^{\sigma}_{\sigma, \tau, \seqS} \defeq \Pr[\!{cp}]{(\sigma,\tau, \seqS)}/ \mu^{\sigma_{\bot}}_{\seqS}(\sigma), \quad p^{\tau}_{\sigma, \tau, \seqS} \defeq 
        \Pr[\!{cp}]{(\sigma, \tau, \seqS)}/\mu^{\tau_{\bot}}_{\seqS}(\tau).
    \end{align}
\item  For each node $(\sigma, \tau, \seqS, v, L) \in \+V \setminus \+L$ and $e\in E_{v}^{\sigma}$, define
\begin{align}\label{eqn-marginal-inner}
    p^{\sigma}_{\sigma, \tau, \seqS, e} \triangleq 
    \Pr[\!{cp}]{(\sigma, \tau, \seqS,e)}/\mu^{\sigma_{\bot}}_{\seqS}(\sigma),
     \quad p^{\tau}_{\sigma, \tau, \seqS, e} \triangleq 
     \Pr[\!{cp}]{(\sigma, \tau, \seqS,e) }/\mu^{\tau_{\bot}}_{\seqS}(\tau).
\end{align}
\item For each infeasible node $(\sigma, \tau, \seqS,v,L)$ in $V(\+T)$, define $ p^{\sigma}_{\sigma, \tau, \seqS}= p^{\tau}_{\sigma, \tau, \seqS} = 0$. 
% \item For each infeasible non-leaf node $(\sigma, \tau, \seqS, v, L) \in V(\+T)$ and each $e \in E_{v}^{\sigma}$,  define $p^{\sigma}_{\sigma, \tau, \seqS, e} = p^{\tau}_{\sigma,\tau, \seqS, e} = 0$.
\end{itemize}

\begin{remark}
For each $(\sigma, \tau, \seqS,v,L) \in \+V\setminus \+L$ and $e\in E_{v}^{\sigma}$, by the definition of $\+V$, we have $\sigma$ is feasible. Thus, $\mu(\sigma)>0$.
In addition, recall that $\sigma(e_{\bot}) = 1$, $\sigma_{\bot} = (e_{\bot}\leftarrow 1)$.
We have  
$\mu^{\sigma_{\bot}}_{\seqS}(\sigma) \geq \mu(\sigma)>0$.
Thus, $p^{\sigma}_{\sigma, \tau, \seqS}$ and $p^{\sigma}_{\sigma, \tau, \seqS,e}$
are well defined.
Similarly, we also have $p^{\tau}_{\sigma, \tau, \seqS}$ and $p^{\tau}_{\sigma, \tau, \seqS, e}$ are well defined.
\end{remark}

In the following, when we use the notations $p^{\sigma}_{\sigma, \tau, \seqS}$ and $p^{\tau}_{\sigma, \tau, \seqS}$,
we always assume  $(\sigma,\tau,\seqS,v(\sigma,\tau),L(\sigma,\tau))\in V(\+T)$.
Similarly, when we use the notations $p^{\sigma}_{\sigma, \tau, \seqS, e}$ and $p^{\tau}_{\sigma, \tau, \seqS, e}$, we always assume  $(\sigma,\tau,\seqS,v(\sigma,\tau),L(\sigma,\tau))\in \+V \setminus \+L$ and $e\in E_{v}^{\sigma}$.
    
One can verify that the following holds for the above quantities.

\hktodo{in the second item, whether only the nodes in $\+V$ are need or all the nodes in $V(\+T)$ are needed} \zdtodo{for $P^{\cp}$, we only consider the feasible nodes.}

    \begin{proposition} \label{prop:coupling-linear-constraint}
    The following holds for the random process $P^{\!{cp}}$:
        \begin{enumerate}[(1)]
            \item All $p^{\sigma}_{\sigma, \tau, \seqS}, p^{\tau}_{\sigma, \tau, \seqS},p^{\sigma}_{\sigma, \tau, \seqS,e}, p^{\tau}_{\sigma, \tau, \seqS,e}$ are in $[0, 1]$. 
            In particular, $p^{\sigma_\bot}_{\sigma_\bot, \tau_\bot, \varnothing} = p^{\tau_\bot}_{\sigma_\bot, \tau_\bot, \varnothing} = 1$.
            \item For each $(\sigma, \tau, \seqS, v, L)\in \+V\setminus\+L$, 
            \begin{align}\label{eqn-inter-sum1}
               p^{\sigma}_{\sigma,\tau,\seqS} = \sum_{e \in E_v^{\sigma}} p^{\sigma}_{\sigma, \tau, \seqS, e}, \quad  p^{\tau}_{\sigma,\tau,\seqS}=\sum_{e \in  E_v^{\sigma}} p^{\tau}_{\sigma,\tau, \seqS, e}.
            \end{align}
            \item For each $(\sigma, \tau, \seqS, v, L)\in \+V\setminus\+L$ and each $e\in E_v^{\sigma}$, if {${\!{Ham}\left(\sigma, {E_v}\right)} < {\!{Ham}\left(\tau,{E_v}\right)}$}, we have 
            \begin{align}\label{eqn-inner-child-sum1}
                p^{\sigma}_{\sigma,\tau, \seqS,e} = p^{\sigma \land (e\gets 0)}_{\sigma \land (e\gets 0),\tau\land (e\gets 0), \seqS \circ e}, \quad  p^{\sigma}_{\sigma, \tau, \seqS, e}=p^{\sigma\land (e\gets 1)}_{\sigma\land (e\gets 1),\tau\land (e\gets 0), \seqS\circ e} + p^{\sigma\land (e\gets 1)}_{\sigma\land (e\gets 1),\tau\land (e\gets 1),\seqS \circ e},
            \end{align}
            \begin{align}\label{eqn-inner-child-sum2}
                p^{\tau}_{\sigma, \tau, \seqS, e} = p^{\tau \land (e \gets 0)}_{\sigma \land (e\gets 0),\tau\land (e\gets 0), \seqS \circ e} + p^{\tau\land (e\gets 0)}_{\sigma\land (e\gets 1),\tau\land (e\gets 0), \seqS \circ e}, \quad  p^{\tau}_{\sigma, \tau, \seqS, e}=p^{\tau \land (e\gets 1)}_{\sigma \land (e\gets 1), \tau\land (e \gets 1), \seqS\circ e}.
            \end{align}
            Otherwise, we have
            \begin{align}\label{eqn-inner-child-sum3}
                p^{\sigma}_{\sigma,\tau, \seqS,e}=p^{\sigma \land (e\gets 0)}_{\sigma\land (e\gets 0),\tau\land (e\gets 0), \seqS\circ e}+p^{\sigma\land (e\gets 0)}_{\sigma\land (e\gets 0),\tau\land (e\gets 1), \seqS \circ e}, \quad  p^{\sigma}_{\sigma,\tau, \seqS,e}=p^{\sigma\land (e\gets 1)}_{\sigma\land (e\gets 1),\tau\land (e\gets 1), \seqS\circ e},
            \end{align}
            \begin{align}\label{eqn-inner-child-sum4}
                p^{\tau}_{\sigma,\tau, \seqS,e}=p^{\tau\land (e\gets 0)}_{\sigma\land (e\gets 0),\tau\land (e\gets 0), \seqS\circ e}, \quad  p^{\tau}_{\sigma,\tau,S,e}=p^{\tau\land (e\gets 1)}_{\sigma\land (e\gets 0),\tau\land (e\gets 1), \seqS \circ e} + p^{\tau \land (e\gets 1)}_{\sigma\land (e\gets 1),\tau\land (e\gets 1), \seqS\circ e}.
            \end{align}
            \item For each $(\sigma, \tau, S, v, L) \in \+V$, we have
            \begin{align}\label{eqn-ratio}
            	{p^{\sigma}_{\sigma,\tau, \seqS}} = p^{\tau}_{\sigma,\tau, \seqS} \cdot \frac{\mu_{e_{\bot}}(1)}{\mu_{e_{\bot}}(0)}\cdot \frac{ \mu(\tau)}{ \mu(\sigma)}.
            \end{align}
        \end{enumerate}
    \end{proposition}
    
    \begin{proof}
\underline{Proof of (1).} 
    We prove $p^{\sigma}_{\sigma, \tau, \seqS}\in [0,1]$ by considering two separate cases:
    \begin{itemize}
    \item $(\sigma, \tau, \seqS, v, L)$ is feasible. By \eqref{eqn-marginal-all} and \eqref{eq-def-trp-perperty-sigma}, we have 
    \begin{align*}
        p^{\sigma}_{\sigma, \tau, \seqS} = \frac{\Pr[\!{cp}]{(\sigma,\tau, \seqS)}}{\mu^{\sigma_{\bot}}_{\seqS}(\sigma)} =       
       \frac{\Pr[\!{cp}]{(\sigma,\tau, \seqS,v,L)}}{\mu^{\sigma_{\bot}}_{\seqS}(\sigma)}\leq 1.
    \end{align*}
    Moreover, by $\Pr[\!{cp}]{(\sigma,\tau, \seqS,v,L)}\geq 0$ and $\mu^{\sigma_{\bot}}_{\seqS}(\sigma)>0$,
    we also have $p^{\sigma}_{\sigma, \tau, \seqS}\geq 0$.
    Therefore, we have $p^{\sigma}_{\sigma, \tau, \seqS} \in [0,1]$.  Similarly, we also have $p^{\tau}_{\sigma, \tau, \seqS} \in [0,1]$. 
    \item $(\sigma, \tau, \seqS, v, L)$ is infeasible.  we have $p^{\sigma}_{\sigma, \tau, \seqS} =  p^{\tau}_{\sigma, \tau, \seqS} = 0$.
    \end{itemize}
    In summary, we always have $p^{\sigma}_{\sigma, \tau, \seqS} \in [0,1]$. Similarly, one can also prove that 
    $p^{\tau}_{\sigma, \tau, \seqS},p^{\sigma}_{\sigma, \tau, \seqS,e},p^{\tau}_{\sigma, \tau, \seqS,e}\in [0,1]$.
    Moreover, by \eqref{eq-pr-sts-eq-pr-stsvl}, we also  have
    $\Pr[\!{cp}]{(\sigma_\bot, \tau_\bot, \varnothing)} = \Pr[\!{cp}]{(\sigma_\bot, \tau_\bot, \varnothing,v_\bot,0)} = 1$.
    Combining with $\mu^{\sigma_{\bot}}_{\varnothing}(\sigma_\bot)$ = 1,
    we have 
    \[p^{\sigma_\bot}_{\sigma_\bot, \tau_\bot, \varnothing} = \frac{\Pr[\!{cp}]{(\sigma_\bot, \tau_\bot, \varnothing)}}{\mu^{\sigma_{\bot}}_{\varnothing}(\sigma_\bot)} = \frac{1}{1} = 1 .\]
    Similarly, we also have $p^{\sigma_\bot}_{\sigma_\bot, \tau_\bot, \varnothing} = 1$.

    % For the second item, similar to the proof of $p^{\sigma}_{\sigma, \tau, \seqS},p^{\tau}_{\sigma, \tau, \seqS} \in [0,1]$, one can also prove $p^{\sigma}_{\sigma, \tau, \seqS,e},p^{\tau}_{\sigma, \tau, \seqS,e} \in [0,1]$.
    
    \underline{Proof of (2).} 
    It is sufficient to prove 
    \begin{align}\label{eqn-inter-sum1-first}
    p^{\sigma}_{\sigma,\tau,\seqS} = \sum_{e \in E_v^{\sigma}} p^{\sigma}_{\sigma, \tau, \seqS, e}.
    \end{align}
    Similarly, one can also prove 
    \[
    p^{\tau}_{\sigma,\tau,\seqS}=\sum_{e \in  E_v^{\sigma}} p^{\tau}_{\sigma,\tau, \seqS, e}.
    \]
    Then \eqref{eqn-inter-sum1} is immediate.
    In the following, we prove \eqref{eqn-inter-sum1-first}.
    For each $(\sigma, \tau, \seqS, v, L)$ in $\+V\setminus \+L$ and each $e \in E_v^{\sigma}$, we claim that 
    \begin{align}\label{eq-pr-stsvl-sum-stsvle}
    \Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)} = \sum_{e\in E_v^{\sigma}}\Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L,e)}.
    \end{align}
    Combining with \eqref{eq-pr-sts-eq-pr-stsvl} and \eqref{eq-pr-stse-eq-pr-stsvle}, we have
    \[\Pr[\!{cp}]{(\sigma, \tau, \seqS)} = \Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)} = \sum_{e\in E_v^{\sigma}}\Pr[\!{cp}]{(\sigma, \tau, \seqS,v,L,e)} = \sum_{e\in E_v^{\sigma}}\Pr[\!{cp}]{(\sigma, \tau, \seqS, e)}.\]     
    Combining with \eqref{eqn-marginal-all} and \eqref{eqn-marginal-inner}, \eqref{eqn-inter-sum1-first} is immediate. 
    At last, we prove \eqref{eq-pr-stsvl-sum-stsvle}, which finishes the proof of \eqref{eqn-inter-sum1}.
     We prove \eqref{eq-pr-stsvl-sum-stsvle} by considering two separate cases.
    \begin{itemize}
    \item $(\sigma, \tau, \seqS, v, L)\not\in \+V^{\!{cp}}$.
    We have $\Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)} = 0$.
    Therefore, by \eqref{eq-def-pro-stsvl} and \eqref{eq-def-pro-stsvle} we have
    \[\forall e\in E_v^{\sigma},\quad \Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L,e)}\leq \Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)} = 0 .\]
    Thus, \eqref{eq-pr-stsvl-sum-stsvle} is immediate.
    \item $(\sigma, \tau, \seqS, v, L)\in \+V^{\!{cp}}$. Assume \emph{w.l.o.g.} that ${\!{Ham}\left(\sigma, {E_{v}}\right)} < {\!{Ham}\left(\tau, {E_{v}}\right)}$.
    By \Cref{def:truncated-random-process},
    if $e$ is the first edge in $E_{v}^{\sigma}$ with 
            $\mu^{\sigma}_e(1) \geq \mu^{\tau}_e(1)$,
    then $\Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L,e)} =\Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)}$.
    Otherwise, $\Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L,e)} = 0$.
    Thus, \eqref{eq-pr-stsvl-sum-stsvle} is immediate.
    \end{itemize}

    \underline{Proof of (3).} It is sufficient to prove \eqref{eqn-inner-child-sum1}. Then \eqref{eqn-inner-child-sum2}, \eqref{eqn-inner-child-sum3} and \eqref{eqn-inner-child-sum4} can be proved similarly.
   In the following, we prove \eqref{eqn-inner-child-sum1}.
   
   Assume 
   ${\!{Ham}\left(\sigma, {E_v}\right)} < {\!{Ham}\left(\tau,{E_v}\right)}$ and $\abs{\seqS} =t$.
   Let $\sigma^0 = \sigma \land (e\gets 0)$, 
   $\sigma^1 = \sigma \land (e\gets 1)$, $\tau^0 = \tau \land (e\gets 0)$, 
   $\tau^1 = \tau \land (e\gets 1)$, $\seqS'=\seqS\circ e$.
   By \Cref{def:truncated-random-process}, under the condition that $(\sigma_t,\tau_t,\seqS_t,v_t,L_t) = (\sigma,\tau,\seqS,v,L)$ and the chosen edge at this state is $e$, we have $\mu^{\sigma_t}_e(1) \geq \mu^{\tau_t}_e(1)$.
   Thus, for each $(\sigma_e,\tau_e)$ sampling from an optimal coupling of $(\mu_e^{\sigma_t},\mu_e^{\tau_t})$,
   we have 
   \[\Pr{\sigma_e = \tau_e = 0} = \mu^{\sigma_t}_e(0) = \mu^{\sigma}_e(0),\quad \Pr{(\sigma_e = \tau_e = 1)\lor(\sigma_e = 1,\tau_e = 0)} = \mu^{\sigma_t}_e(1) = \mu^{\sigma}_e(1).\]
   Formally,
   \[\Pr{(\sigma_{t+1},\tau_{t+1}) = (\sigma^0,  \tau^0)\mid \left(T > t\right)\land \left((\sigma, \tau, \seqS, v, L) = (\sigma_t, \tau_t, \seqS_t, v_t, L_t)\right)\land \left(\seqS_{t+1} =\seqS\circ e\right) } = \mu^{\sigma}_e(0),\]
   \[\Pr{(\sigma_{t+1},\tau_{t+1}) \text{ is } (\sigma^1,  \tau^1)  \text{ or } (\sigma^1,  \tau^0) \mid \left(T > t\right)\land \left((\sigma, \tau, \seqS, v, L) = (\sigma_t, \tau_t, \seqS_t, v_t, L_t)\right)\land \left(\seqS_{t+1} =\seqS\circ e\right) } = \mu^{\sigma}_e(1).\]
   Therefore, we have 
   \begin{equation}\label{eq-expansion-tplus1-te}
   \begin{aligned}
   &\Pr{\left(T> t\right)\land \left((\sigma^0, \tau^0, \seqS', v(\sigma^0,\tau^0), L(\sigma^0,\tau^0)) = (\sigma_{t+1}, \tau_{t+1}, \seqS_{t+1}, v_{t+1}, L_{t+1})\right)} \\
   =&\Pr{\left(T > t\right)\land \left((\sigma, \tau, \seqS, v, L) = (\sigma_t, \tau_t, \seqS_t, v_t, L_t)\right)\land \left(\seqS_{t+1} =\seqS\circ e\right) }\cdot \mu^{\sigma}_e(0),
   \end{aligned}
   \end{equation}
    \begin{equation}\label{eq-expansion-tplus1-te-one}
   \begin{aligned}
   &\Pr{\left(T> t\right)\land \left( (\sigma_{t+1}, \tau_{t+1}, \seqS_{t+1}, v_{t+1}, L_{t+1}) \text{ is } (\sigma^1, \tau^1, \seqS', v(\sigma^1,\tau^1), L(\sigma^1,\tau^1)) \text{ or } (\sigma^1, \tau^0, \seqS', v(\sigma^1,\tau^0), L(\sigma^1,\tau^0))\right)} \\
   =&\Pr{\left(T > t\right)\land \left((\sigma, \tau, \seqS, v, L) = (\sigma_t, \tau_t, \seqS_t, v_t, L_t)\right)\land \left(\seqS_{t+1} =\seqS\circ e\right) }\cdot \mu^{\sigma}_e(1),
   \end{aligned}
   \end{equation}

   At first, we prove 
   \[p^{\sigma}_{\sigma,\tau, \seqS,e} = p^{\sigma \land (e\gets 0)}_{\sigma \land (e\gets 0),\tau\land (e\gets 0), \seqS \circ e}.\]
   Note that 
   \begin{equation}\label{eq-pr-stsprime-pr-stse}
    \begin{aligned}
   &\Pr[\!{cp}]{(\sigma^0, \tau^0, \seqS')}\\
  (\text{by \eqref{eq-pr-sts-eq-pr-stsvl}})\quad= &\Pr[\!{cp}]{(\sigma^0, \tau^0, \seqS',v(\sigma^0, \tau^0),L(\sigma^0, \tau^0)}\\
   (\text{by \eqref{eq-def-pro-stsvl}})\quad= &\Pr{\left(T\geq t+1\right)\land \left((\sigma^0, \tau^0, \seqS', v(\sigma^0,\tau^0), L(\sigma^0,\tau^0)) = (\sigma_{t+1}, \tau_{t+1}, \seqS_{t+1}, v_{t+1}, L_{t+1})\right)}\\
   (\text{by \eqref{eq-expansion-tplus1-te}})\quad= &\Pr{\left(T > t\right)\land \left((\sigma, \tau, \seqS, v, L) = (\sigma_t, \tau_t, \seqS_t, v_t, L_t)\right)\land \left(\seqS_{t+1} =\seqS\circ e\right) }\cdot \mu^{\sigma}_e(0)\\
   (\text{by \eqref{eq-def-pro-stsvle}})\quad   =&\Pr[\!{cp}]{(\sigma, \tau, \seqS,e,v,L)}\cdot \mu^{\sigma}_e(0)\\
   (\text{by \eqref{eqn-marginal-inner}})\quad   =&\Pr[\!{cp}]{(\sigma, \tau, \seqS,e)}\cdot \mu^{\sigma}_e(0).
   \end{aligned}
   \end{equation}
   Moreover, we also have 
   \begin{align}\label{eq-xsimmu-sigmaprime-sigma}
   \mu^{\sigma_{\bot}}_{\seqS'}(\sigma^0) =\mu^{\sigma_{\bot}}_{\seqS}(\sigma) \cdot \mu^{\sigma}_e(0).
   \end{align}
   Thus, we have 
   \begin{align*}
    &\quad p^{\sigma}_{\sigma,\tau, \seqS,e} \\
(\text{by \eqref{eqn-marginal-inner}})\quad    &= \Pr[\!{cp}]{(\sigma, \tau, \seqS,e)}/\mu^{\sigma_{\bot}}_{\seqS}(\sigma)\\
 (\text{by \eqref{eq-pr-stsprime-pr-stse} and \eqref{eq-xsimmu-sigmaprime-sigma}})\quad&= \Pr[\!{cp}]{(\sigma^0, \tau^0, \seqS')}/\mu^{\sigma_{\bot}}_{\seqS'}(\sigma^0) 
 \\(\text{by \eqref{eqn-marginal-all}})\quad&= p^{\sigma^0}_{\sigma^0,\tau^0, \seqS'}\\
 &= p^{\sigma \land (e\gets 0)}_{\sigma \land (e\gets 0),\tau\land (e\gets 0), \seqS \circ e}.
   \end{align*}

    In the next, we prove 
    \[
    p^{\sigma}_{\sigma, \tau, \seqS, e}=p^{\sigma\land (e\gets 1)}_{\sigma\land (e\gets 1),\tau\land (e\gets 0), \seqS\circ e} + p^{\sigma\land (e\gets 1)}_{\sigma\land (e\gets 1),\tau\land (e\gets 1),\seqS \circ e}.\]
    Note that 
    \begin{equation}\label{eq-pr-stsprime-pr-stse-11}
    \begin{aligned}
   &\Pr[\!{cp}]{(\sigma^1, \tau^1, \seqS')}\\
  (\text{by \eqref{eq-pr-sts-eq-pr-stsvl}})\quad= &\Pr[\!{cp}]{(\sigma^1, \tau^1, \seqS',v(\sigma^1, \tau^1),L(\sigma^1, \tau^1)}\\
   (\text{by \eqref{eq-def-pro-stsvl}})\quad= &\Pr{\left(T\geq t+1\right)\land \left((\sigma^1, \tau^1, \seqS', v(\sigma^1,\tau^1), L(\sigma^1,\tau^1)) = (\sigma_{t+1}, \tau_{t+1}, \seqS_{t+1}, v_{t+1}, L_{t+1})\right)}
   \end{aligned}
   \end{equation}
   Similarly, we also have 
     \begin{equation}\label{eq-pr-stsprime-pr-stse-10}
    \begin{aligned}
   \Pr[\!{cp}]{(\sigma^1, \tau^0, \seqS')}
   = \Pr{\left(T\geq t+1\right)\land \left((\sigma^1, \tau^0, \seqS', v(\sigma^1,\tau^0), L(\sigma^1,\tau^0)) = (\sigma_{t+1}, \tau_{t+1}, \seqS_{t+1}, v_{t+1}, L_{t+1})\right)}
   \end{aligned}
   \end{equation}
   Thus, we have
     \begin{equation}\label{eq-pr-stsprime-pr-stse-11-10}
    \begin{aligned}
   &\Pr[\!{cp}]{(\sigma^1, \tau^1, \seqS')} + \Pr[\!{cp}]{(\sigma^1, \tau^0, \seqS')}\\
   (\text{by \eqref{eq-pr-stsprime-pr-stse-11}, \eqref{eq-pr-stsprime-pr-stse-10} and \eqref{eq-expansion-tplus1-te-one}})\quad= &\Pr{\left(T > t\right)\land \left((\sigma, \tau, \seqS, v, L) = (\sigma_t, \tau_t, \seqS_t, v_t, L_t)\right)\land \left(\seqS_{t+1} =\seqS\circ e\right) }\cdot \mu^{\sigma}_e(1)\\
   (\text{by \eqref{eq-def-pro-stsvle}})\quad   =&\Pr[\!{cp}]{(\sigma, \tau, \seqS,e,v,L)}\cdot \mu^{\sigma}_e(1)\\
   (\text{by \eqref{eqn-marginal-inner}})\quad   =&\Pr[\!{cp}]{(\sigma, \tau, \seqS,e)}\cdot \mu^{\sigma}_e(1).
   \end{aligned}
   \end{equation}
   Moreover, we also have 
   \begin{align}\label{eq-xsimmu-sigmaprime-sigma-1}
   \mu^{\sigma_{\bot}}_{\seqS'}(\sigma^1)
   = \mu^{\sigma_{\bot}}_{\seqS}(\sigma) \cdot \mu^{\sigma}_e(1).
   \end{align}
   Thus, we have 
   \begin{align*}
    &\quad p^{\sigma}_{\sigma,\tau, \seqS,e} \\
(\text{by \eqref{eqn-marginal-inner}})\quad    &= \Pr[\!{cp}]{(\sigma, \tau, \seqS,e)}/\mu^{\sigma_{\bot}}_{\seqS}(\sigma)\\
 (\text{by \eqref{eq-pr-stsprime-pr-stse-11-10} and \eqref{eq-xsimmu-sigmaprime-sigma-1}})\quad&= \left(\Pr[\!{cp}]{(\sigma^1, \tau^1, \seqS')}+\Pr[\!{cp}]{(\sigma^1, \tau^0, \seqS')}\right)/\mu^{\sigma_{\bot}}_{\seqS'}(\sigma^1)
 \\(\text{by \eqref{eqn-marginal-all}})\quad&= p^{\sigma^1}_{\sigma^1,\tau^1, \seqS'}+p^{\sigma^1}_{\sigma^1,\tau^0, \seqS'}\\
 &=p^{\sigma\land (e\gets 1)}_{\sigma\land (e\gets 1),\tau\land (e\gets 0), \seqS\circ e} + p^{\sigma\land (e\gets 1)}_{\sigma\land (e\gets 1),\tau\land (e\gets 1),\seqS \circ e}.
   \end{align*}
   
 \underline{Proof of (4).} By \eqref{eqn-marginal-all}, we have 
\begin{align*}
    p^{\tau}_{\sigma, \tau, \seqS} \cdot \frac{\mu_{e_{\bot}}(1)}{\mu_{e_{\bot}}(0)}\cdot \frac{ \mu(\tau)}{ \mu(\sigma)} = \frac{\Pr[\!{cp}]{(\sigma, \tau, \seqS)}\cdot\mu_{e_{\bot}}(1)\cdot\mu(\tau)}{\mu^{\tau_{\bot}}_{\seqS}(\tau)\cdot\mu_{e_{\bot}}(0) \cdot\mu(\sigma)}= \frac{\Pr[\!{cp}]{(\sigma, \tau, \seqS)}\cdot\mu_{e_{\bot}(1)}}{\mu(\sigma)}= \frac{\Pr[\!{cp}]{(\sigma,\tau, \seqS)}}{\mu^{\sigma_{\bot}}_{\seqS}(\sigma)}= p^{\sigma}_{\sigma, \tau, \seqS}.
\end{align*}
\end{proof}


    \hktodo{polish this paragraph}
    \qgl{check from here}
    The next lemma provides the key property for bounding the coupling error.
    Recall the definition of $\+D(\cdot)$ in \Cref{def-notation-v-tct}.

    \begin{lemma} \label{lem:coupling-error}
       For each $(\sigma, \tau, \seqS, v, L) \in V(\+T)$,
       let $\+D \triangleq \+D(\sigma, \tau, \seqS, v, L)$.
       Then we have
       \begin{align}\label{eqn-error-bound}
           \sum_{(\sigma', \tau', \seqS',v',L') \in \+D} p^{\sigma'}_{\sigma', \tau', \seqS'}\geq  B_{\min} \cdot p^{\sigma}_{\sigma, \tau, \seqS}, \quad \sum_{(\sigma',\tau', \seqS',v',L') \in \+D} p^{\tau'}_{\sigma', \tau', \seqS} \geq  B_{\min} \cdot p^{\tau}_{\sigma,\tau, \seqS}.
       \end{align}
    \end{lemma}
    \begin{proof}
    We prove this lemma by considering three separate cases.
    \begin{itemize}
    \item $(\sigma,\tau,\seqS,v,L)$ is infeasible.
    If $E^{\sigma}_v = \emptyset$, then we have $\+D = \{(\sigma,\tau,\seqS,v,L)\}$.
    Thus, we have 
    \[\sum_{(\sigma', \tau', \seqS',v',L') \in \+D} p^{\sigma'}_{\sigma', \tau', \seqS'} = p^{\sigma}_{\sigma, \tau, \seqS} \geq  B_{\min} \cdot p^{\sigma}_{\sigma, \tau, \seqS},\]
    where the last equality is by 
   
    \item If $(\sigma,\tau,\seqS,v,L)$ is infeasible and $E^{\sigma}_v \neq \emptyset$, then 
    $\+D =\emptyset$.
    Thus, we have 
    \[\left(\sum_{(\sigma', \tau', \seqS',v',L') \in \+D} p^{\sigma'}_{\sigma', \tau', \seqS'}\right) = 0 = p^{\sigma}_{\sigma, \tau, \seqS} =  B_{\min} \cdot p^{\sigma}_{\sigma, \tau, \seqS}.\]
    \item $(\sigma,\tau,\seqS,v,L)$ is feasible.
    
    \end{itemize}
 



    
        For $(\sigma, \tau, \seqS, v, L) \in \+W \setminus \+W^{\!{cp}}$, the inequality holds immediately since the associated quantity is $0$. Therefore, it suffices to show~\eqref{eqn-error-bound} for the nodes $(\sigma, \tau, \seqS, v, L)\in  \+W^{\!{cp}}$. Since the selection of the updated edges involves no randomness, it implies that $\abs{U} = 1$. Suppose $U = \set{(\sigma', \tau', \seqS')}$. We have
        \begin{align*}
            \frac{p^{\sigma'}_{\sigma', \tau', \seqS'}}{p^{\sigma}_{\sigma, \tau,\seqS}} &= \frac{\Pr{(\sigma', \tau', \seqS')}}{\Pr[X \sim \mu]{X \in \sigma' \mid X(e_\bot) = 1}} \cdot \frac{\Pr[X \sim \mu]{X \in \sigma \mid X(e_\bot) = 1}}{\Pr{(\sigma, \tau, S)}}\\
            &=\frac{\Pr{(\sigma', \tau', \seqS')}}{\Pr{(\sigma, \tau, S)}} \cdot \frac{\Pr[X \sim \mu]{X \in \sigma \mid X(e_\bot) = 1}}{\Pr[X \sim \mu]{X\in \sigma' \mid X(e_\bot) = 1}}  \\
            &\geq  B_{\min} \cdot \frac{\Pr[X\sim \mu]{X\in \sigma \mid X(e_\bot) = 1}}{\Pr[X\sim \mu]{X\in \sigma' \mid X(e_\bot) = 1}} \geq  B_{\min}. \tag{by \Cref{lem:marginal-bound}}
        \end{align*}
    	Similarly, we can show that $ p^{\tau'}_{\sigma',\tau',\seqS'} \geq  B_{\min} \cdot p^{\tau}_{\sigma,\tau,\seqS}$. Combining these facts, the proof is immediate.
    \end{proof}


    
   
    
\section{Derandomization}

In this section, we show how to design a deterministic algorithm to efficiently approximate the marginal ratio for any Holant instance satisfying~\Cref{cond:Holant-condition}. We firstly set up and analyze a linear program built on the truncated coupling tree induced by an instance $(\Phi, \sigma_\bot, \tau_\bot, v_\bot)$ satisfying~\Cref{cond-instancepair}, and show how to design a marginal estimator based on this linear program.

%    Throughout this section, we consider any graph $G=(V,E=E_1\cup \set{e_0})$ of maximum degree $\Delta$ with $b\geq 1$, a vertex $v_0\in V$ where $e_0\in E_{v_0}$, and partial assignments $\sigma',\tau': \set{e_0}\rightarrow \set{0,1}$ where $\sigma'(e_0)=0$ and $\tau'(e_0)=1$.
%    We design an efficient algorithm to compute the marginal ratio $\frac{\Pr[X\sim \mu]{X(e_0)=0}}{\Pr[X\sim \mu]{X(e_0)=1}}$. 

% In this section, we consider the binary symmetric Holant problem $\Phi = (G, \vecf)$ with its induced Gibbs distribution $\mu = \mu_\Phi$ where $G = (V, E = E_1 \cup \set{e_\bot})$ is a graph of maximum degree $\Delta$ with a unique half-edge $e_\bot$ incident to $v_\bot$ and $\vecf = (f_v)_{v \in V}$ are positive log-concave signatures assigned to vertices in $G$. Let $\sigma_\bot, \tau_\bot : \set{e_\bot} \to \set{0, 1}$ be two partial assignments where $\sigma_\bot = e_\bot \gets 1$ and $\tau_\bot = e_\bot \gets 0$. Now we design an efficient algorithm to compute the marginal ratio $\frac{\Pr[X \sim \mu]{X(e_\bot) = 1}}{\Pr[X \sim \mu]{X(e_\bot) = 0}}$.

\subsection{Setting up the linear program}

In this section, we introduce a linear program built on the truncated coupling tree where any node in the coupling tree is associated with variables.
% Later, we will show that the coupling process ensures the feasibility of a linear program concerning these variables.

\hktodo{check whether the set of variables is correct}


\begin{definition}[Linear Program Induced by the Coupling] \label{def:induced-LP}
\emph{
    For any positive integer $\ell$, let $\+T = \+T_{\ell}(\Phi, \sigma_\bot, \tau_\bot, v_\bot)$ be the $\ell$-truncated coupling tree and $0 \leq r^- \leq r^+\leq r_{\max}(\Phi)$ be two parameters satisfying
    \begin{align*}
        r^- \leq \frac{\mu_{e_{\bot}}(1)}{\mu_{e_{\bot}}(0)} \leq r^+.
    \end{align*}
    For each $(\sigma, \tau, \seqS, v, L) \in V(\+T)$, we associate it with variables $\widehat{p}_{\sigma, \tau, \seqS}^{\sigma}$, $\widehat{p}_{\sigma,\tau, \seqS}^{\tau}$; for each non-leaf node $(\sigma,\tau,\seqS,v,L)$ in $V(\+T)$, we associate it with variables $\widehat{p}_{\sigma,\tau,\seqS,e}^{\sigma}$, and $\widehat{p}_{\sigma,\tau,\seqS,e}^{\tau}$ for each $e\in E_v^{\sigma}$. The linear program can be established as follows:
    \begin{enumerate}
        \item All $p^{\sigma}_{\sigma, \tau, \seqS}, p^{\tau}_{\sigma, \tau, \seqS},p^{\sigma}_{\sigma, \tau, \seqS,e}, p^{\tau}_{\sigma, \tau, \seqS,e}$ are in $[0, 1]$. In particular, $\widehat{p}^{\sigma_\bot}_{\sigma_\bot,\tau_\bot, \varnothing}=\widehat{p}^{\tau_\bot}_{\sigma_\bot,\tau_\bot,\varnothing} = 1$.
        \item For each $(\sigma, \tau, \seqS, v, L)\in \+V\setminus\+L$, 
            \begin{align}\label{eqn-hat-inter-sum1}
               \widehat{p}^{\sigma}_{\sigma, \tau, \seqS} = \sum_{e \in  E_v^{\sigma}} \widehat{p}^{\sigma}_{\sigma, \tau, \seqS, e}, \quad  \widehat{p}^{\tau}_{\sigma,\tau, \seqS}=\sum_{e\in  E_v^{\sigma}} \widehat{p}^{\tau}_{\sigma,\tau, \seqS,e}.
            \end{align}
        \item For each $(\sigma, \tau, \seqS, v, L)\in \+V\setminus\+L$ and each $e\in E_v^{\sigma}$,
            if ${\!{Ham}\left(\sigma,{E_v}\right)}<{\!{Ham}\left(\tau,{E_v}\right)}$, 
            \begin{align}\label{eqn-hat-inner-child-sum1}
                \widehat{p}^{\sigma}_{\sigma,\tau,\seqS,e}=\widehat{p}^{\sigma\land (e\gets 0)}_{\sigma\land (e\gets 0),\tau\land (e\gets 0), \seqS \circ e}, \quad  \widehat{p}^{\sigma}_{\sigma,\tau, \seqS,e}=\widehat{p}^{\sigma\land (e\gets 1)}_{\sigma\land (e\gets 1),\tau\land (e\gets 0), \seqS \circ e} + \widehat{p}^{\sigma \land (e\gets 1)}_{\sigma\land (e\gets 1),\tau \land (e\gets 1), \seqS \circ e},
            \end{align}
            \begin{align}\label{eqn-hat-inner-child-sum2}
                \widehat{p}^{\tau}_{\sigma,\tau, \seqS,e}=\widehat{p}^{\tau\land (e\gets 0)}_{\sigma\land (e\gets 0),\tau\land (e\gets 0), \seqS\circ e}+\widehat{p}^{\tau\land (e\gets 0)}_{\sigma\land (e\gets 1),\tau\land (e\gets 0), \seqS \circ e}, \quad  \widehat{p}^{\tau}_{\sigma,\tau,\seqS,e}=\widehat{p}^{\tau\land (e\gets 1)}_{\sigma\land (e\gets 1),\tau\land (e\gets 1),\seqS\circ e}.
            \end{align}
            Otherwise, 
            \begin{align}\label{eqn-hat-inner-child-sum3}
                 \widehat{p}^{\sigma}_{\sigma,\tau,\seqS,e}= \widehat{p}^{\sigma \land (e\gets 0)}_{\sigma\land (e\gets 0),\tau\land (e\gets 0),\seqS\circ e}+ \widehat{p}^{\sigma\land (e\gets 0)}_{\sigma\land (e\gets 0),\tau\land (e\gets 1), \seqS\circ e}, \quad  \widehat{p}^{\sigma}_{\sigma,\tau,\seqS,e}=\widehat{p}^{\sigma\land (e\gets 1)}_{\sigma\land (e\gets 1),\tau\land (e\gets 1), \seqS\circ e},
            \end{align}
            \begin{align}\label{eqn-hat-inner-child-sum4}
                 \widehat{p}^{\tau}_{\sigma,\tau,\seqS,e}= \widehat{p}^{\tau\land (e\gets 0)}_{\sigma\land (e\gets 0),\tau\land (e\gets 0),\seqS\circ e}, \quad   \widehat{p}^{\tau}_{\sigma,\tau,\seqS,e}= \widehat{p}^{\tau\land (e\gets 1)}_{\sigma\land (e\gets 0),\tau\land (e\gets 1),\seqS\circ e} +  \widehat{p}^{\tau \land (e\gets 1)}_{\sigma\land (e\gets 1),\tau\land (e\gets 1),\seqS\circ e}.
            \end{align}
        \item For any $(\sigma,\tau,\seqS,v,L)\in \+L_{\!{good}}$,
            \begin{align}\label{eqn-hat-ratio}
                 \frac{ \mu(\tau)}{ \mu(\sigma)}\cdot r^-\cdot {\widehat{p}^{\tau}_{\sigma,\tau, \seqS}}\leq {\widehat{p}^{\sigma}_{\sigma,\tau, \seqS}}\leq  \frac{ \mu(\tau)}{ \mu(\sigma)}\cdot  r^+ \cdot{\widehat{p}^{\tau}_{\sigma,\tau, \seqS}}.
            \end{align}
        \item For any $(\sigma,\tau, \seqS,v,L)\in V(\+T)$ and $\+D \triangleq \+D(\sigma,\tau, \seqS,v,L)$,
           \begin{align}\label{eqn-hat-error-bound}
               \sum_{(\sigma',\tau',\seqS',v',L')\in \+D}\widehat{p}^{\sigma'}_{\sigma',\tau', \seqS'} \geq  B_{\min} \cdot \widehat{p}^{\sigma}_{\sigma,\tau, \seqS}, \quad \sum_{(\sigma',\tau',\seqS',v',L')\in \+D}\widehat{p}^{\tau'}_{\sigma',\tau',\seqS'}\geq  B_{\min} \cdot \widehat{p}^{\tau}_{\sigma,\tau,\seqS}.
           \end{align}
        \item  For each infeasible node $(\sigma, \tau, \seqS, v, L)$ in $V(\+T)$, $ \widehat{p}^{\sigma}_{\sigma,\tau, \seqS}= \widehat{p}^{\tau}_{\sigma,\tau, \seqS}=0$. 
    \end{enumerate}
}
\end{definition}

% \begin{lemma}
%     The feasibility of the above LP. and show that the LP can be constructed since the ratio of the good leaves can be computed.
% \end{lemma}
% \begin{proof}
%     \qgl{It suffices to observe that we can embed the quantities induced from $\+V^{\!{cp}}$ and let those not in $\+V^{\!{cp}}$ be zero. Note that $\+L^{\!{cp}}\setminus \+L_{\ell}^{\!{cp}} \subseteq \+L_{\!{good}}$.} 
% \end{proof}

\subsection{The analysis of the linear program} \zdtodo{change notations in simpler ones}

We put some properties of the linear program in~\Cref{def:induced-LP} here. Firstly we show that the linear program can be built efficiently.

\begin{lemma}[Building Cost of LP] \label{lem:building-cost-of-LP}
    For an instance $(\Phi, \sigma_\bot, \tau_\bot, v_\bot)$ satisfying~\Cref{cond-instancepair}, a non-negative integer $\ell \ge 0$ and two real numbers $0 \le r^- \le r^+$,
    % a non-negative integer $\ell \ge 0$ and $0 \le r^{-} \le r^{+} \le r_{\max}(\Phi)$ where \qgl{seems wierd. since  }
    % $$
    %     r^{-} \le \frac{\mu_{e_{\bot}}(1)}{\mu_{e_{\bot}}(0)} \le r^+,
    % $$
    the linear program defined above can be built in time $\poly\left(\Delta^{\Delta \ell}\right)$. Thus we can check the feasibility of the LP in time $\poly\left(\Delta^{\Delta \ell}\right)$.
\end{lemma}
\begin{proof}
    By~\Cref{prop:property-of-truncated-tree}, we can build the $\ell$-truncated coupling tree in time $\poly\left(\Delta^{\Delta \ell}\right)$. Moreover, the number of constraints is $\poly\left(\Delta^{\Delta \ell}\right)$ since the size of $\+T$ is at most $(3\Delta)^{\Delta \ell}$ by~\Cref{prop:property-of-truncated-tree}. Then assuming all the constraints in hand, we can check the feasibility in time $\poly\left(\Delta^{\Delta \ell}\right)$.
    
    To show the cost of building the linear program, it suffices to show the cost to verify whether a node $(\sigma, \tau, \seqS)$ is feasible or not, and compute the cost for building the constraints on $(\sigma, \tau, \seqS, v, L) \in \+L_{\!{good}}$.

    For $(\sigma, \tau, \seqS, v, L) \in V(\+T)$, by~\Cref{lem:partial-assignment-feasibility,lemma-property-tct-vl}, we can check the feasibility of $\sigma, \tau$ in time $O(\abs{\Lambda(\sigma)}) = O(\abs{\seqS})$. By~\Cref{prop:property-of-truncated-tree}, it holds that $\abs{\seqS} \le \Delta \ell$, meaning that the time for checking feasibility is $O(\Delta \ell)$.
    
    For a node $(\sigma, \tau, \seqS, v, L) \in \+L_{\!{good}}$, to compute $\frac{\Pr[X \sim \mu_\Phi]{X \in \tau}}{\Pr[X \sim \mu_\Phi]{X \in \sigma}}$, we have
    \begin{align*}
        \frac{\Pr[X \sim \mu_\Phi]{X \in \tau}}{\Pr[X \sim \mu_\Phi]{X \in \sigma}} &= \frac{\sum_{x : E \to \set{0, 1} : x \in \tau} \prod_{u \in V} f_u\left(\abs{x \vert_{E_u}}\right)}{\sum_{x : E \to \set{0, 1} : x \in \sigma} \prod_{u \in V} f_u\left(\abs{x \vert_{E_u}}\right)}.
    \end{align*}
    Since $(\sigma, \tau, \seqS, v, L) \in \+L_{\!{good}}$, it holds that $E_v^{\sigma} = \emptyset$ and for every $u \in V \setminus \set{v}$, $\Ham(\sigma, E_u) = \Ham(\tau, E_u)$. For every $x : E \to \set{0, 1}$ where $x \in \tau$, let $y : E \to \set{0, 1}$ be defined as
    \begin{align*}
        y(e) = \begin{cases}
            x(e) & e \notin \Lambda(\sigma) \\
            \sigma(e) & \mbox{otherwise}
        \end{cases}\;.
    \end{align*}
    It is not hard to see that $y \in \sigma$ and this mapping is one-to-one, and
    \begin{align*}
        \frac{\prod_{u \in V} f_u\left(\abs{x \vert_{E_u}}\right)}{\prod_{u \in V} f_u\left(\abs{y \vert_{E_u}}\right)} = \frac{f_v\left(\abs{\tau \vert_{E_v}}\right)}{f_v\left(\abs{\sigma \vert_{E_v}}\right)}
    \end{align*}
    Hence we conclude that
    $$
        \frac{\Pr[X \sim \mu_\Phi]{X \in \tau}}{\Pr[X \sim \mu_\Phi]{X \in \sigma}} = \frac{f_v\left(\abs{\tau \vert_{E_v}}\right)}{f_v\left(\abs{\sigma \vert_{E_v}}\right)}.
    $$
    Since the ratio can be computed in time $O(\Delta)$, for every $(\sigma, \tau, \seqS) \in \+L_{\!{good}}$, we can build constraints in time $O(\Delta)$.

    Combining all together, we can build the LP and check its feasibility in time $\poly\left(\Delta^{\Delta\ell}\right)$.
\end{proof}

\hktodo{check the feasibility here}

The following lemma ensures the feasibility of the LP by the coupling process.
\begin{lemma}[Feasibility of LP] \label{lem:feasibility-of-LP}
    There is at least a solution to the linear program defined as~\Cref{def:induced-LP}.
\end{lemma}
\begin{proof}
    Recall the quantities associated with the random process $P^{\cp} = P_{\ell}^{\cp}(\Phi, \sigma_\bot, \tau_\bot, v_\bot)$. We show how to induce a solution to the LP from them.

    For every $(\sigma, \tau, \seqS) \in V(\+T)$, if $(\sigma, \tau, \seqS) \in \+V^{\cp}$\zdtodo{clarify this symbol in the above section}, we let
    $$
        \wh{p}_{\sigma, \tau, \seqS}^{\sigma} = p_{\sigma, \tau, \seqS}^{\sigma}, \quad \wh{p}_{\sigma, \tau, \seqS}^{\tau} = p_{\sigma, \tau, \seqS}^{\tau}
    $$
    and if additionally $(\sigma, \tau, \seqS) \in \+V^{\cp} \setminus \+L^{\cp}$, for every $e \in E_v^{\sigma}$,
    $$
        \wh{p}_{\sigma, \tau, \seqS, e}^{\sigma} = p_{\sigma, \tau, \seqS}^{\sigma, e}, \quad \wh{p}_{\sigma, \tau, \seqS, e}^{\tau} = p_{\sigma, \tau, \seqS, e}^{\tau};
    $$
    otherwise, we let all quantities associated with $(\sigma, \tau, \seqS)$ be $0$. By $\+V^{\cp} \subseteq V(\+T)$ from~\Cref{prop:property-of-truncated-tree}, the above assignments are well-defined. By~\Cref{prop:coupling-linear-constraint,lem:coupling-error}, the quantities satisfy all constraints in~\Cref{def:induced-LP}. That is to say, there exists at least one solution to it.
\end{proof}

Lastly, we show that a feasible solution to the LP gives both upper and lower bounds to the marginal ratio $R_{\Phi}(e_\bot)$.

\begin{theorem} \label{thm:ratio-bound-by-LP}
    Assume that all constraints in the linear program defined as~\Cref{def:induced-LP} hold. Then it holds that
    \begin{align*}
        \left(1 - \left(1 - B_{\min}^2\right)^{\ell}\right)r^{-} \le R_{\Phi}(e_\bot) \le \frac{r^+}{1 - \left(1 - B_{\min}^2\right)^{\ell}}.
    \end{align*}
\end{theorem}
% \begin{remark}
%     When $\left(1 - B_{\min}^2\right)^{\ell} < 1/2$, the bounds can be relaxed to
%     $$
%         \left(1 - 2\left(1 - B_{\min}^2\right)^{\ell}\right)r^{-} \le R_{\Phi}(e_\bot) \le \left(1 + 2\left(1 - B_{\min}^2\right)^{\ell}\right)r^{+}.
%     $$
% \end{remark}

To prove~\Cref{thm:ratio-bound-by-LP}, we show some properties of the feasible solution to the linear program.

\begin{lemma} \label{lem:ratio-identity}
    Given that the constraints~\eqref{eqn-hat-inter-sum1} to \eqref{eqn-hat-inner-child-sum4} are satisfied and all quantities associated with the infeasible nodes are set to 0, we have
    \begin{align*}
        \mu_{e_{\bot}}(1) = \sum_{(\sigma, \tau, \seqS,v,L) \in \+L}\mu(\sigma)\cdot\widehat{p}_{\sigma, \tau, \seqS}^{\sigma} 
    \end{align*}
    and
    \begin{align*}
        \mu_{e_{\bot}}(0)  = \sum_{(\sigma, \tau, \seqS,v,L) \in \+L}\mu(\tau)\cdot\widehat{p}_{\sigma, \tau, \seqS}^{\tau}.
    \end{align*}
\end{lemma}

The following lemma is the key ingredient to prove~\Cref{lem:ratio-identity}.

\hktodo{Whether the check of an infeasible node is efficient}

\hktodo{the condition of this lemma}

\begin{lemma} \label{lem:ratio-identity-partial}
     Given that the constraints~\eqref{eqn-hat-inter-sum1} to \eqref{eqn-hat-inner-child-sum4} are satisfied and all quantities associated with the infeasible nodes are set to 0, we have
    \begin{align}\label{eqn-sum-leftside}
        \forall x\in \sigma_\bot, \quad \sum_{(\sigma,\tau, \seqS,v,L)\in \+L: \ x\in \sigma} \widehat{p}^{\sigma}_{\sigma,\tau,\seqS}=1,
    \end{align}
    and
    \begin{align}\label{eqn-sum-rightside}
        \forall y\in \tau_\bot, \quad \sum_{(\sigma,\tau, \seqS,v,L)\in \+L: \ y\in \tau} \widehat{p}^{\tau}_{\sigma,\tau, \seqS}=1.
    \end{align}
\end{lemma}

\hktodo{whether we should prove this lemma on the feasible set $\+L$}

\begin{proof}
    To prove this lemma, it is sufficient to prove \eqref{eqn-sum-leftside}. Similarly, one can also prove \eqref{eqn-sum-rightside}.
    In the following, we prove \eqref{eqn-sum-leftside}.
    Recall that $\+L$ is the set of \emph{feasible} leaf nodes in $V(\+T)$, where $\+T$ is the $\ell$-truncated coupling tree. 
    Let $\+T^{i}$ be the subtree of $\+T$ obtained by deleting all nodes at a depth greater than $i$
    and $\+L^{i}$ be the leaf nodes of $\+T^{i}$.
    We will prove 
    \begin{align}\label{eqn-subtree-l-tct}
        \forall x\in \sigma_\bot,0\leq i\leq \ell, \quad \sum_{(\sigma,\tau, \seqS,v,L)\in \+L^i: \ x\in \sigma} \widehat{p}^{\sigma}_{\sigma,\tau,\seqS}=1,
    \end{align}
    Then \eqref{eqn-sum-leftside} is immediate.    
    We prove \eqref{eqn-subtree-l-tct} by induction on $i$.
    The induction basis is when $i = 0$.
    In this case, by \Cref{def:truncated-coupling-tree} we have $\+T^0$ is a tree with a unique node $(\sigma_\bot, \tau_\bot, \varnothing, v_\bot, 0)$.
    In addition, by \Cref{def:induced-LP}, we have 
    $\widehat{p}^{\sigma_\bot}_{\sigma_\bot,\tau_\bot, \varnothing} = 1$.
    Therefore, 
     \begin{align*}
        \forall x\in \sigma_\bot, \quad \sum_{(\sigma,\tau, \seqS,v,L)\in \+L^0: \ x\in \sigma} \widehat{p}^{\sigma}_{\sigma,\tau,\seqS}=\widehat{p}^{\sigma_\bot}_{\sigma_\bot,\tau_\bot. \varnothing} = 1,
    \end{align*}
    The base case is proved. For the induction step, we fix an $x\in \sigma_\bot$.    
    For each node $(\sigma,\tau,\seqS,v,L)\in \+L^i$,
    let $\+C \triangleq \+C((\sigma,\tau,\seqS,v,L))$ be the children of $(\sigma,\tau,\seqS,v,L)$ in $\+T^{i+1}$. 
    By $(\sigma,\tau,\seqS,v,L) \in \+L^{i}$, we have 
    $\+C\subseteq \+L^{i+1}$.
    Define 
    \begin{align}\label{def-cplus}
    \+C^{+} \triangleq \+C^{+}((\sigma,\tau,\seqS,v,L)) = \left(\+C\cup \{(\sigma,\tau,\seqS,v,L)\}\right)\cap \+L^{i+1}.
    \end{align}
    We claim that 
    \begin{align}\label{eq-extension-sum}
    \forall (\sigma,\tau,\seqS,v,L)\in \+L^i,\quad \widehat{p}^{\sigma}_{\sigma,\tau,\seqS}\cdot \id{x \in \sigma} = \sum_{(\sigma',\tau', \seqS',v',L')\in \+C^{+}} \widehat{p}^{\sigma'}_{\sigma',\tau',\seqS'}\cdot \id{\ x\in \sigma'}.
    \end{align}
    Thus, we have 
    \begin{align*}
        \sum_{(\sigma,\tau, \seqS,v,L)\in \+L^{i+1}} \widehat{p}^{\sigma}_{\sigma,\tau,\seqS}\cdot \id{\ x\in \sigma}&=\sum_{u = (\sigma,\tau, \seqS,v,L)\in \+L^{i}}\sum_{(\sigma',\tau', \seqS',v',L')\in \+C^{+}(u)} \widehat{p}^{\sigma'}_{\sigma',\tau',\seqS'}\cdot \id{ x\in \sigma'}\\
        &=\sum_{(\sigma,\tau, \seqS,v,L)\in \+L^{i}}\widehat{p}^{\sigma}_{\sigma,\tau,\seqS}\cdot \id{x \in \sigma}\\
        &=\sum_{(\sigma,\tau, \seqS,v,L)\in \+L^i:\  x\in \sigma} \widehat{p}^{\sigma}_{\sigma,\tau,\seqS}\\
        &=1,
    \end{align*}
    where the second equality is by \eqref{eq-extension-sum} and the last equality is by the induction hypothesis.
    This completes the induction step and \eqref{eqn-subtree-l-tct} is immediate.

    In the following, we prove \eqref{eq-extension-sum}, which completes the proofs of \eqref{eqn-subtree-l-tct} and the lemma. Fix a $(\sigma,\tau,\seqS,v,L)\in \+L^i$.  
    If $x\not \in \sigma$, we have $x\not\in \sigma'$ for each $(\sigma',\tau',\seqS',v',L')\in \+C$.
    Thus, 
    \begin{align*}
    \sum_{(\sigma',\tau', \seqS',v',L')\in \+C^{+} } \widehat{p}^{\sigma'}_{\sigma',\tau',\seqS'}\cdot \id{x\in \sigma'}=\sum_{(\sigma',\tau', \seqS',v',L')\in \+C} \widehat{p}^{\sigma'}_{\sigma',\tau',\seqS'}\cdot\id{ x\in \sigma'} =0 = \widehat{p}^{\sigma}_{\sigma,\tau,\seqS}\cdot \id{x \in \sigma}.
    \end{align*}
    Then \eqref{eq-extension-sum} is proved.
    Otherwise, $x \in \sigma$.     
    By \Cref{def:truncated-coupling-tree},
    there are only two possibilities.
    \begin{itemize}
    \item If $L\geq \ell$ or $E_v^{\sigma}=\emptyset$ or $(\sigma, \tau, \seqS, v, L)$ is infeasible, then $(\sigma,\tau,\seqS,v,L)$ is a leaf node in $\+T$.
    Therefore, we have $(\sigma,\tau,\seqS,v,L)\in \+L^{i+1}$ and $\+C = \emptyset$.
    Thus, $\+C^+ = (\sigma, \tau, \seqS, v, L)$.
    We have 
    \begin{align*}
    \sum_{(\sigma',\tau', \seqS',v',L')\in \+C^{+}} \widehat{p}^{\sigma'}_{\sigma',\tau',\seqS'}\cdot \id{x\in \sigma'} = \widehat{p}^{\sigma}_{\sigma,\tau,\seqS} = \widehat{p}^{\sigma}_{\sigma,\tau,\seqS}\cdot \id{x \in \sigma}.
    \end{align*}
    Then \eqref{eq-extension-sum} is proved.    
    \item Otherwise, $(\sigma,\tau,\seqS,v,L)$ is not a leaf node in $\+T$. Combining with $(\sigma,\tau,\seqS,v,L) \in \+L^{i}$, we have 
    $(\sigma,\tau,\seqS,v,L)\\ \not\in \+L^{i+1}$.
    Thus, by \eqref{def-cplus}, we have $(\sigma,\tau,\seqS,v,L) \not\in \+C^{+}$.
    We claim that 
    \begin{align}\label{eq-extension-sum-edge}
     \forall e = \set{u,v} \in E_v^{\sigma}, \quad \widehat{p}^{\sigma}_{\sigma,\tau,\seqS,e}\cdot \id{x \in \sigma} = \sum_{(\sigma',\tau', \seqS\circ e,v',L')\in \+C } \widehat{p}^{\sigma'}_{\sigma',\tau',\seqS\circ e}\cdot \id{x\in \sigma'}.
    \end{align}
    In addition, by $(\sigma,\tau,\seqS,v,L)$ is not a leaf node in $\+T$ and \Cref{prop:property-of-truncated-tree}, we have 
    $(\sigma,\tau,\seqS,v,L)\in \+V\setminus \+L$.
    Moreover, by \Cref{def:truncated-coupling-tree}, we have each $(\sigma',\tau', \seqS',v',L')\in \+C$ satisfies $\seqS' = \seqS \circ e$ for some $e \in E_v^{\sigma}$. Formally,
    \begin{align}\label{eq-c-ce}
    \+C = \left\{(\sigma',\tau', \seqS\circ e,v',L')\mid (\sigma',\tau', \seqS\circ e,v',L')\in \+C,e\in E_v^{\sigma}\right\}.
    \end{align}
    Thus, we have 
    \begin{align*}
       &\widehat{p}^{\sigma}_{\sigma, \tau, \seqS} \cdot \id{x \in \sigma} \\
       (\text{by $(\sigma,\tau,\seqS,v,L)\in \+V\setminus \+L$ and \eqref{eqn-hat-inter-sum1}})\quad = &\sum_{e \in  E_v^{\sigma}} \left(\widehat{p}^{\sigma}_{\sigma, \tau, \seqS, e}\cdot \id{x \in \sigma}\right)\\ 
       (\text{by \eqref{eq-extension-sum-edge}})\quad = &\sum_{e \in  E_v^{\sigma}}\sum_{(\sigma',\tau', \seqS\circ e,v',L')\in \+C} \widehat{p}^{\sigma'}_{\sigma',\tau',\seqS\circ e}\cdot\id{ x\in \sigma'}\\
       (\text{by \eqref{eq-c-ce}})\quad  =&\sum_{(\sigma',\tau', \seqS',v',L')\in \+C} \widehat{p}^{\sigma'}_{\sigma',\tau',\seqS'}\cdot\id{ x\in \sigma'}\\
(\text{by $(\sigma,\tau,\seqS,v,L) \not\in \+C^{+}$})\quad =&\sum_{(\sigma',\tau', \seqS',v',L')\in \+C^{+}} \widehat{p}^{\sigma'}_{\sigma',\tau',\seqS'}\cdot\id{ x\in \sigma'}.
    \end{align*}
    Then \eqref{eq-extension-sum} is immediate.
    In the following, we prove \eqref{eq-extension-sum-edge}, which completes the proof of \eqref{eq-extension-sum}.
    Fix an edge $ e = \set{u,v} \in E_v^{\sigma}$.
    Let $\sigma^{a} = \sigma \land (e \gets a)$ and $\tau^{a} = \tau \land (e \gets a)$ for each $a\in \{0,1\}$.    
    We prove \eqref{eq-extension-sum-edge} by considering two separate cases. 
        \begin{enumerate}[(i)]
            \item ${\!{Ham}\left(\sigma,{E_v}\right)}<{\!{Ham}\left(\tau,{E_v}\right)}$. In this case, by \Cref{def:truncated-coupling-tree}, $(\sigma,\tau,\seqS,v,L)$ has three children related to $e$ in $\+L^{i+1}$, i.e., $(\sigma^0, \tau^0, \seqS \circ e, v, L)$, $(\sigma^1, \tau^0, \seqS \circ e, u, L+1)$, and $(\sigma^1, \tau^1, \seqS \circ e, v, L)$ . Assume \emph{w.l.o.g.} $x(e) = 0$.
            We have 
            \[\left\{(\sigma',\tau', \seqS\circ e,v',L')\mid (\sigma',\tau', \seqS\circ e,v',L')\in \+C^{+},x\in \sigma'\right\} = \{ (\sigma^0, \tau^0, \seqS \circ e, v, L)\}.\]          
            Combining with \eqref{eqn-hat-inner-child-sum1}, we have 
             \begin{align*}
            \widehat{p}^{\sigma}_{\sigma,\tau,\seqS,e}\cdot \id{x \in \sigma} = \widehat{p}^{\sigma}_{\sigma,\tau,\seqS,e}=\widehat{p}^{\sigma^0}_{\sigma^0,\tau^0, \seqS \circ e} = \sum_{(\sigma',\tau', \seqS\circ e,v',L')\in \+C} \widehat{p}^{\sigma'}_{\sigma',\tau',\seqS\circ e}\cdot \id{ x\in \sigma'}. 
            \end{align*}
            Then \eqref{eq-extension-sum-edge} is proved. \item ${\!{Ham}\left(\sigma,{E_v}\right)}\geq{\!{Ham}\left(\tau,{E_v}\right)}$. In this case, by \Cref{def:truncated-coupling-tree},  $(\sigma,\tau,\seqS,v,L)$ has three children related to $e$ in $\+L^{i+1}$, i.e., $(\sigma^0, \tau^0, \seqS \circ e,v,L)$, $(\sigma^0, \tau^1, \seqS \circ e, u, L + 1)$, and $(\sigma^1, \tau^1, \seqS\circ e,v,L)$. Assume \emph{w.l.o.g.} $x(e) = 0$.
            We have 
            \[\left\{(\sigma',\tau', \seqS\circ e,v',L')\mid (\sigma',\tau', \seqS\circ e,v',L')\in \+C^{+},x\in \sigma'\right\} = \{ (\sigma^0, \tau^0, \seqS \circ e, v, L),(\sigma^0, \tau^1, \seqS \circ e, u, L+1)\}.\]          
            Combining with \eqref{eqn-hat-inner-child-sum3}, we have 
             \begin{align*}
            \widehat{p}^{\sigma}_{\sigma,\tau,\seqS,e}\cdot \id{x \in \sigma} = \widehat{p}^{\sigma}_{\sigma,\tau,\seqS,e}=\widehat{p}^{\sigma^0}_{\sigma^0,\tau^0, \seqS \circ e} +\widehat{p}^{\sigma^0}_{\sigma^0,\tau^1, \seqS \circ e} = \sum_{(\sigma',\tau', \seqS\circ e,v',L')\in \+C} \widehat{p}^{\sigma'}_{\sigma',\tau',\seqS\circ e}\cdot \id{ x\in \sigma'}.  
            \end{align*}
            Then \eqref{eq-extension-sum-edge} is proved.
        \end{enumerate} 
    \end{itemize}
  
    

    
  

   
    

    
    %For simplicity, we abuse the notation $\widehat{p}_{\sigma, \tau, S}^{\sigma} = 0$ for all infeasible configurations $(\sigma, \tau, S)$ (\IE, $(\sigma, \tau, S, v, L) \notin \+T$ for every $v \in V$ and $L \in \mathbb N$).
    \iffalse
    With convention $\wh{p}_{\sigma, \tau, S}^\sigma = 0$ for all invalid configurations $(\sigma, \tau, S)$ (\IE, $(\sigma, \tau, S, v, L) \notin \+T$ for all $v \in V$, $L \in \mathbb N$), we rewrite constraints~\eqref{eqn-hat-inner-child-sum1} and~\eqref{eqn-hat-inner-child-sum3} as: for each $e \in E_v^\sigma$,
    \begin{align*}
        \widehat{p}_{\sigma, \tau, S}^{\sigma}(e) = \widehat{p}_{\sigma \wedge e \gets 0, \tau \wedge e \gets 0, S \circ e}^{\sigma \wedge e \gets 0} + \widehat{p}_{\sigma \wedge e \gets 0, \tau \wedge e \gets 1, S \circ e}^{\sigma \wedge e \gets 0}
    \end{align*}
    and
    \begin{align*}
        \widehat{p}_{\sigma, \tau, S}^{\sigma}(e) = \widehat{p}_{\sigma \wedge e \gets 1, \tau \wedge e \gets 0, S \circ e}^{\sigma \wedge e \gets 1} + \widehat{p}_{\sigma \wedge e \gets 1, \tau \wedge e \gets 1, S \circ e}^{\sigma \wedge e \gets 1}.
    \end{align*}
    Then we can generalise that for every $x \in \sigma$,
    \begin{align} \label{eq:extended-inner-sum-identity}
        \wh{p}_{\sigma, \tau, S}^{\sigma}(e) = \widehat{p}_{\sigma \wedge e \gets x(e), \tau \wedge e \gets 0, S \circ e}^{\sigma \wedge e \gets x(e)} + \widehat{p}_{\sigma \wedge e \gets x(e), \tau \wedge e \gets 1, S \circ e}^{\sigma \wedge e \gets x(e)}.
    \end{align}

    To show~\eqref{eqn-sum-leftside}, we prove a stronger version. For a subset of assignment-nodes $\Lambda$, we say that $\Lambda$ is \emph{a boundary on $\+T$} if for every leaf-node $\gamma = (\sigma, \tau, S, v, L) \in \+L$, there is \emph{exactly} one node in $\Lambda$ lying in the path from the root to $\gamma$.
    For example, the collection of all leaves $\+L$ forms a boundary, and a single root is also a boundary.
    % Additionally, for two boundaries $\Lambda_1$ and $\Lambda_2$, we say that $\Lambda_1 \preceq \Lambda_2$ if every node in $\Lambda_2$ is the ancestor of some nodes in $\Lambda_1$. \zdtodo{it might be of convenience to define the `expand' operation.}
    
    {
    \color{blue}
    To state our proof clearly, for a boundary $\Lambda$ on $\+T$ and a node $\alpha = (\sigma, \tau, S, v, L) \in \Lambda$, we define the assignment-node set $\Expand(\Lambda, \alpha)$ expanded from $\Lambda$ and $\alpha$ as:
    \begin{itemize}
        \item If $\alpha \in \+L$ is a leaf-node, $\Expand(\Lambda, \alpha) = \Lambda$; otherwise
        \item Let $\+E \defeq \set{(\sigma, \tau, S, v, L, e) \cmid e \in E_v^\sigma}$ be all children of $\alpha$ in $\+T$, and let $\+C(\Lambda, \alpha)$ be the grandson nodes of $\alpha$ in $\+T$, \IE,
        $$
            \+C(\Lambda, \alpha) \defeq \set{\beta = (\wh{\sigma}, \wh{\tau}, \wh{S}, \wh{v}, \wh{L}) \in \+T \cmid \mbox{$\beta$ is the child of $\gamma$ for some $\gamma \in \+E$}}.
        $$
        We remark here that $\Lambda \cap \+C(\Lambda, \alpha) = \emptyset$. Let $\Expand(\Lambda, \alpha) = \Lambda \setminus \set{\alpha} \sqcup \+C(\Lambda, \alpha)$.
    \end{itemize}

    Directly from our construction, it is clear that $\Expand(\Lambda, \alpha)$ is also a boundary on $\+T$, and every boundary can be generated from $\set{(\sigma', \tau', \perp, v_0, 0)}$ after several expanding operations.
    % For two boundaries $\Lambda_1, \Lambda_2$ on $\+T$, we say that $\Lambda_1 \prec \Lambda_2$ if $\Lambda_1 \neq \Lambda_2$ and $\Lambda_2$ can be generated from $\Lambda_1$ after finite expanding operations.
    }
    
    To show~\eqref{eqn-sum-leftside}, it suffices to show that given $x \in \sigma'$, for every boundary $\Lambda$ on $\+T$, it holds that
    \begin{align} \label{eq:extend-version-of-identity}
        \sum_{(\sigma, \tau, S, v, L) \in \Lambda : x \in \sigma} \wh{p}_{\sigma, \tau, S}^{\sigma} = 1.
    \end{align}
    We prove it by induction hypothesis.
    \begin{itemize}
        \item \textbf{Base Case:} $\Lambda = \set{(\sigma', \tau', \perp, v_0, 0)}$. The identity holds trivially by the constraint $\wh{p}_{\sigma', \tau', \perp}^\sigma = 1$.

        \item \textbf{Induction Steps:} Assume that the identity holds for $\Lambda_1$. For every $\alpha = (\sigma_1, \tau_1, S_1, v_1, L_1) \in \Lambda_1$ with $x \in \sigma_1$, when $\Lambda_2 = \Expand(\Lambda_1, \alpha) \neq \Lambda_1$, by~\eqref{eqn-hat-inter-sum1} and~\eqref{eq:extended-inner-sum-identity}, it holds that
        \begin{align*}
            \wh{p}_{\sigma_1, \tau_1, S_1}^{\sigma_1} &= \sum_{e \in E_{v_1}^{\sigma_1}} \wh{p}_{\sigma_1, \tau_1, S_1}^{\sigma_1}(e) \tag{by~\eqref{eqn-hat-inter-sum1}}\\
            &= \sum_{e \in E_{v_1}^{\sigma_1}} \left(\wh{p}_{\sigma_1 \wedge e \gets x(e), \tau_1 \wedge e \gets 0, S_1 \circ e}^{\sigma_1 \wedge e \gets x(e)} + \widehat{p}_{\sigma_1 \wedge e \gets x(e), \tau_1 \wedge e \gets 1, S_1 \circ e}^{\sigma_1 \wedge e \gets x(e)}\right) \tag{by~\eqref{eq:extended-inner-sum-identity}} \\
            &= \sum_{(\sigma_2, \tau_2, S_2, v_2, L_2) \in \+C(\Lambda_1, \alpha) : x \in \sigma_2} \wh{p}_{\sigma_2, \tau_2, S_2}^{\sigma_2}
        \end{align*}
        Where the last identity holds from the observation that we only need to consider all valid configurations. Hence we conclude that
        \begin{align*}
            1 = \sum_{(\sigma, \tau, S, v, L) \in \Lambda_1 : x \in \sigma} \wh{p}_{\sigma, \tau, S}^{\sigma} &= \wh{p}_{\sigma_1, \tau_1, S_1}^{\sigma_1} + \sum_{(\sigma, \tau, S, v, L) \in \Lambda_1 : x \in \sigma \atop (\sigma, \tau, S, v, L) \neq \alpha} \wh{p}_{\sigma, \tau, S}^{\sigma} \\
            &= \sum_{(\sigma_2, \tau_2, S_2, v_2, L_2) \in \+C(\Lambda_1, \alpha) : x \in \sigma_2} \wh{p}_{\sigma_2, \tau_2, S_2}^{\sigma_2} + \sum_{(\sigma, \tau, S, v, L) \in (\Lambda_1 \setminus \set{\alpha}) : x \in \sigma} \wh{p}_{\sigma, \tau, S}^{\sigma} \\
            &= \sum_{(\sigma, \tau, S, v, L) \in \Lambda_2 : x \in \sigma} \wh{p}_{\sigma, \tau, S}^{\sigma}. \tag{$\Lambda_2 = \Lambda_1 \setminus \set{\alpha} \sqcup \+C(\Lambda_1, \alpha)$}
        \end{align*}
    \end{itemize}
    Following the principle of induction hypothesis,~\eqref{eq:extend-version-of-identity} holds for every boundary $\Lambda$ on $\+T$. Pick $\Lambda = \+L$ and we conclude what we desire.
    \fi
\end{proof}

\begin{proof}[Proof of~\Cref{lem:ratio-identity}]
    % put it outside
    % We claim that
    % \begin{align}\label{eqn-sum-leftside}
    %     \forall x\in \sigma', \quad \sum_{(\sigma,\tau,S,v,L)\in \+L: \ x\in \sigma} \widehat{p}^{\sigma}_{\sigma,\tau,S}=1,
    % \end{align} and
    % \begin{align}\label{eqn-sum-rightside}
    %     \forall y\in \tau', \quad \sum_{(\sigma,\tau,S,v,L)\in \+L: \ y\in \tau} \widehat{p}^{\tau}_{\sigma,\tau,S}=1.
    % \end{align}
    % Next, we only show \eqref{eqn-sum-leftside} holds. We prove it by induction

    
    % By iterating the equalities from~\eqref{eqn-hat-inter-sum1} to \eqref{eqn-hat-inner-child-sum4}.
    

    
    

    % Note that $\mu$ is a uniform distribution. Therefore, we have
    % \begin{align*}
    %     \frac{\Pr[X\sim \mu]{X(e_0)=0}}{\Pr[X\sim \mu]{X(e_0)=1}}&=\frac{\abs{\sigma'}}{\abs{\tau'}}=\frac{\sum_{x\in \sigma'}\sum_{(\sigma,\tau,S,v,L)\in \+L: \ x\models \sigma} \widehat{p}^{\sigma}_{\sigma,\tau,S}}{\sum_{y\in \tau'}\sum_{(\sigma,\tau,S,v,L)\in \+L: \ y\models \tau} \widehat{p}^{\tau}_{\sigma,\tau,S}} \tag{plugging \eqref{eqn-sum-leftside} and \eqref{eqn-sum-rightside}}\\
    %     &=\frac{\sum_{(\sigma,\tau,S,v,L)\in \+L}\widehat{p}_{\sigma,\tau,S}^{\sigma}\cdot\abs{\sigma}}{\sum_{(\sigma,\tau,S,v,L)\in \+L}\widehat{p}_{\sigma,\tau,S}^{\tau}\cdot \abs{\tau}}\\
    %     &=\frac{\sum_{(\sigma,\tau,S,v,L)\in \+L}\widehat{p}_{\sigma,\tau,S}^{\sigma}\cdot \Pr[X\sim \mu]{X\in \sigma}}{\sum_{(\sigma,\tau,S,v,L)\in \+L}\widehat{p}_{\sigma,\tau,S}^{\tau}\cdot \Pr[X\sim \mu]{X\in \tau}}.
    % \end{align*}

    {
    % When $\mu$ is not uniform:
   We have
    \begin{align*}
        \sum_{(\sigma, \tau, \seqS,v,L) \in \mathcal L} \mu(\sigma)\cdot \widehat{p}_{\sigma, \tau, \seqS}^{\sigma} &= \sum_{(\sigma, \tau, \seqS,v,L) \in \mathcal L} \widehat{p}_{\sigma, \tau, \seqS}^{\sigma} \sum_{x \in \sigma} \mu(x) \\
        &= \sum_{(\sigma, \tau, \seqS,v,L) \in \mathcal L} \widehat{p}_{\sigma, \tau, \seqS}^{\sigma} \sum_{x \in \sigma_\bot} \left(\mu(x) \cdot  \id{x \in \sigma}\right) \\
        &= \sum_{x \in \sigma_\bot} \mu(x) \sum_{(\sigma, \tau, \seqS,v,L) \in \mathcal L} \left(\widehat{p}_{\sigma, \tau, \seqS}^{\sigma} \cdot \id{x \in \sigma}\right) \\
        &= \sum_{x \in \sigma_\bot} \mu(x) \sum_{(\sigma, \tau, \seqS,v,L) \in \+L : \ x \in \sigma} \widehat{p}_{\sigma, \tau, \seqS}^{\sigma} \\
        &= \sum_{x \in \sigma_\bot} \mu(x) \tag{by~\eqref{eqn-sum-leftside}} \\
        &= \mu_{e_\bot}(1).
    \end{align*}
    Similarly, by \eqref{eqn-sum-rightside}, we also have 
    \begin{align*}
    \sum_{(\sigma, \tau, \seqS,v,L) \in \+L}\mu(\tau)\cdot\widehat{p}_{\sigma, \tau, \seqS}^{\tau} = \mu_{e_{\bot}}(0).
    \end{align*}
    The lemma is proved.
    }
\end{proof}


% We introduce a random process driven by the coupling tree.

%     \begin{algorithm}[th]
%         \caption{$\!{PseudoCouple}(G,\sigma',\tau',v)$}
%         \label{algo:randomCoupletree}
%         \qgl{$L\gets 0$; \quad \tcp{a global variable}} 
%         \KwIn{A graph $G=(V,E=E_1\cup \set{e_0})$ with partial assignments $\sigma'$ and $\tau'$ and a unique \qgl{disagreeing} vertex $v\in V$.}
%         \KwOut{A pair of assignments $\sigma,\tau: E\rightarrow \set{0,1}$ drawn from a coupling between $\mu^{\sigma'}$ and $\mu^{\tau'}$.}	
           
%             % \If{$v$ is isolated}
%             % {
                
%             % } 
%             \While{
%                 $E_v^{\sigma'}\neq \emptyset$
%             }
%             {
%             let $\nu$ be an arbitrary distribution with support ${E}_v^{\sigma',\tau'}$ and draw $e=\set{u,v}\sim \nu$\;
%             sample $(\sigma_e,\tau_e)$ from an optimal coupilng of $(\mu^{\sigma'}_e,\mu^{\tau'}_e)$\;
%             $\sigma' \gets \sigma' \land \sigma_e$ and $\tau'\gets \tau'\land \tau_e$\; 
%             \If{ $\sigma_e\neq \tau_e$}
%             {
%                 $L\gets L+1$\;
%                 $(\sigma,\tau)\gets \!{Couple}(G,\sigma',\tau',u)$; \quad 
%                 \tcp{The disagreeing vertex has changed}
                
%             }
%             % \tcp{The disagreeing vertex remains the same}    
%             }
%             sample $(\sigma,\tau)$ from an optimal coupling of $(\mu^{\sigma'},\mu^{\tau'})$\;       
%         \Return{$(\sigma,\tau )$.}	
%     \end{algorithm}

Given a feasible solution to the LP, by~\Cref{lem:ratio-identity}, to estimate the marginal ratio, it suffices to bound $\left(\mu(\sigma)\cdot \wh{p}_{\sigma, \tau, \seqS}^{\sigma}\right)/\left(\mu(\tau)\cdot\wh{p}_{\sigma, \tau, \seqS}^{\tau}\right)$ for every $(\sigma, \tau, \seqS,v,L) \in \+L$. For every $(\sigma, \tau, \seqS) \in \+L_{\!{good}}$, the bounds are clear by~\eqref{eqn-hat-ratio}. However, for $(\sigma, \tau, \seqS) \in \+L_{\!{bad}}$, the cost to compute the ratio might be exponential in the size of the instance. The following lemma shows that we can truncate all bad leaves with a small error.

\begin{lemma} \label{lem:LP-truncated-error}
    Given that the constraints~\eqref{eqn-hat-inter-sum1} to~\eqref{eqn-hat-error-bound} are satisfied, we have 
    \begin{align}\label{eqn-error1}
        \sum_{(\sigma,\tau, \seqS,v,L)\in \+L_{\!{bad}}} \widehat{p}_{\sigma,\tau, \seqS}^{\sigma}\cdot \mu^{\sigma_{\bot}}_{\seqS}(\sigma)\leq (1 - B_{\min}^2)^{\ell},
    \end{align}
    and 
    \begin{align}\label{eqn-error2}
        \sum_{(\sigma,\tau, \seqS,v,L)\in \+L_{\!{bad}}}\widehat{p}_{\sigma,\tau, \seqS}^{\tau}\cdot \mu^{\tau_{\bot}}_{\seqS}(\tau)\leq (1- B_{\min}^2)^{\ell}.
    \end{align}
\end{lemma}
\begin{proof}
    We prove~\eqref{eqn-error1} and~\eqref{eqn-error2} can be shown similarly.
    We define a random process with the outcome set $\+L$, where the probability of outputting $(\sigma, \tau, \seqS, v, L)\in \+L$ is given by
    $$
        \widehat{p}_{\sigma,\tau, \seqS}^{\sigma}\cdot \Pr[X\sim \mu]{X\in \sigma\mid X(e_\bot) = 1}.
    $$
    Specifically, our random process $\widehat{P} = \set{(\sigma_t,\tau_t, \seqS_t,v_t,L_t)}_{0 \le t \le T}$ starting from the root node $(\sigma_\bot, \tau_\bot, \varnothing, v_\bot,0)$ of $\+T$, i.e., $(\sigma_0, \tau_0, \seqS_0, v_0, L_0) = (\sigma_\bot, \tau_\bot, \varnothing, v_\bot, 0)$, unfolds by repeating the following operations: 
    \begin{enumerate}
        \item If $(\sigma_t,\tau_t, \seqS_t, v_t, L_t)$ is a leaf node in $\+L$, the process stops and $(\sigma_t,\tau_t, \seqS_t, v_t, L_t)$ is the outcome $\+O$ of the random process.
        \item Otherwise, first sample an edge $e = \set{u, v_t}\in E_{v_t}^{\sigma_t}$ with probability $\frac{\widehat{p}^{\sigma_t}_{\sigma_t,\tau_t,\seqS_t,e}}{\widehat{p}^{\sigma_t}_{\sigma_t,\tau_t,\seqS_t}}$. This operation is well-defined by the constraints~\eqref{eqn-hat-inter-sum1}. Then, we sample partial assignment $\sigma_e$ from $\mu^{\sigma_t}_{e}$. Let $\+C$ be the children $(\sigma,\tau,\seqS,v,L)$ of the node $(\sigma_t,\tau_t,\seqS_t,v_t,L_t)$ satisfying $\sigma = \sigma_t\land \sigma_e$. For any $(\sigma,\tau, \seqS, v, L)\in \+C$, let $(\sigma_{t+1},\tau_{t+1}, \seqS_{t+1},v_{t+1},L_{t+1})\gets (\sigma,\tau, \seqS,v,L)$ with probability $\frac{\widehat{p}^{\sigma}_{\sigma, \tau, \seqS}}{\widehat{p}^{\sigma_t}_{\sigma_t,\tau_t,\seqS_t,e}}$. This operation is well-defined by the constraints \eqref{eqn-hat-inner-child-sum1} to \eqref{eqn-hat-inner-child-sum4}.        
    \end{enumerate}
    For $(\sigma, \tau, \seqS, v, L) \in V(\+T)$, let $t = \abs{\seqS}$. Define
    $$
        \Pr[\wh{P}]{(\sigma, \tau, \seqS)} \defeq \Pr{(T \ge t) \land (\sigma_t, \tau_t, \seqS_t, v_t, L_t) = (\sigma, \tau, \seqS, v, L)}
    $$
    where the randomness on the right sides relies on the random process $\wh{P}$. Similarly for two partial assignments $\sigma, \tau$ with $\Lambda(\sigma) = \Lambda(\tau)$, define:
    $$
        \Pr[\wh{P}]{(\sigma, \tau)} \defeq \Pr{\exists \seqS, v, L, 0 \le t \le T: (\sigma_t, \tau_t, \seqS_t, v_t, L_t) = (\sigma, \tau, \seqS, v, L)}.
    $$

    % \qgl{from here} Let $\widehat{\mu}$ be the distribution of the states in the random process $\widehat{P}$.
    % For simplicity, we abuse the notation $(\sigma, \tau, \seqS)$ to denote the event that the random process $\wh{P}$ reaches the node $(\sigma, \tau, \seqS) \in V(\+T)$, and $(\sigma, \tau)$ to denote the event that $\wh{P}$ reaches $(\sigma, \tau, \seqS) \in V(\+T)$ for some edge sequence $\seqS$. Then it holds that 
    % Consider any $(\sigma,\tau, \seqS, v,L )\in V(\+T)$. We claim that
    % \begin{align}\label{eqn-simulate-outcome-prob}
    %     \widehat{\mu}(\sigma,\tau,S,v,L)=\widehat{p}_{\sigma,\tau,S}^{\sigma}\cdot \Pr[X\sim \mu]{X\in \sigma\mid X(e_0)=0}.
    % \end{align}
    We claim that:
    \begin{align}\label{eqn-simulate-outcome-prob}
        \Pr[\wh{P}]{(\sigma, \tau, \seqS)} = \widehat{p}_{\sigma, \tau, \seqS}^{\sigma} \cdot \Pr[X\sim \mu]{X\in \sigma\mid X(e_\bot)=1}.
    \end{align}
    % where the randomness on the left side depends on the process $\wh{P}$.
    In fact, let $(u_0, u_1, \cdots, u_k)$ be the path from the root to $(\sigma, \tau, \seqS, v, L)$ including the assignment-nodes in the coupling tree $\+T$ where $u_i = (\sigma_i, \tau_i, \seqS_i, v_i, L_i)$ for each $1 \le i \le k - 1$, $u_0 = (\sigma_\bot,\tau_\bot,\varnothing,v_\bot,0)$, and $u_k=(\sigma,\tau, \seqS,v,L)$. Assume that $\seqS=(e_1, e_2, \cdots, e_k)$. By definition, we have
    \begin{align*}
         \Pr[\wh{P}]{(\sigma, \tau, \seqS)} &= \widehat{p}^{\sigma_\bot}_{\sigma_\bot,\tau_\bot,\varnothing}\cdot \prod_{i = 1}^{k}  \left(\frac{\widehat{p}^{\sigma_{i-1}}_{\sigma_{i-1},\tau_{i-1}, \seqS_{i-1},e_{i}}}{\widehat{p}^{\sigma_{i-1}}_{\sigma_{i-1},\tau_{i-1}, \seqS_{i-1}}} \cdot \mu_{e_i}^{\sigma_{i - 1}}(\sigma_{i - 1}(e_i)) \cdot \frac{\widehat{p}^{\sigma_i}_{\sigma_i,\tau_i,\seqS_i}}{\widehat{p}^{\sigma_{i-1}}_{\sigma_{i-1},\tau_{i-1},\seqS_{i-1},e_{i}}}\right) \\
         &=\widehat{p}_{\sigma,\tau, \seqS}^{\sigma}\cdot \Pr[X \sim \mu]{X\in \sigma\mid X(e_\bot) = 1}.
    \end{align*}
    Combined with constraints~\eqref{eqn-hat-error-bound}, it implies that for each $(\sigma, \tau, \seqS, v, L) \in \+W \cap V(\+T)$ with $\sigma' = \sigma \land E^{\sigma}_v \gets \boldsymbol{0}$ and $\tau' = \tau \land E^{\sigma}_v \gets \boldsymbol{0}$,
    \begin{align*}
        \Pr[\wh{P}]{(\sigma',\tau') \mid (\sigma, \tau, \seqS, v, L)} &= \sum_{\seqS': (\sigma', \tau', S') \in V(\+T)}  \Pr[\wh{P}]{(\sigma', \tau', \seqS') \mid (\sigma,\tau, \seqS,v,L) }\\
        &=\sum_{\seqS' : (\sigma', \tau', \seqS')\in V(\+T)}  \frac{\Pr[\wh{P}]{(\sigma', \tau', \seqS')}}{\Pr[\wh{P}]{(\sigma, \tau, \seqS, v, L)}}\\
        &=\sum_{\seqS' : (\sigma', \tau', \seqS) \in V(\+T)}  \frac{\widehat{p}_{\sigma', \tau', \seqS'}^{\sigma'} \cdot \Pr[X \sim \mu]{X\in \sigma' \mid X(e_\bot) = 1}}{\widehat{p}_{\sigma, \tau, \seqS}^{\sigma} \cdot \Pr[X\sim \mu]{X\in \sigma\mid X(e_\bot) = 1}}\\
        &\geq B_{\min}^2.
    \end{align*} 
    
    By similar argument in the proof of~\Cref{lem:decay}, we have
    \begin{align*}
        \Pr{\+O \in \+L_{\!{bad}}} \leq (1 - B_{\min}^2)^{\ell},
    \end{align*}
    which implies~\eqref{eqn-error1} immediately.
\end{proof}

Now we prove~\Cref{thm:ratio-bound-by-LP} with the above properties.

\begin{proof}[Proof of~\Cref{thm:ratio-bound-by-LP}]
    By~\Cref{lem:ratio-identity}, it holds that
    \begin{align*}
        &\symbolwidth \Pr[X \sim \mu]{X(e_\bot) = 1} \\
        &= \sum_{(\sigma, \tau, \seqS) \in \+L} \wh{p}_{\sigma, \tau, \seqS}^{\sigma} \cdot \Pr[X \sim \mu]{X \in \sigma} \\
        &= \sum_{(\sigma, \tau, \seqS) \in \+L_{\good}} \wh{p}_{\sigma, \tau, \seqS}^{\sigma} \cdot \Pr[X \sim \mu]{X \in \sigma} + \sum_{(\sigma, \tau, \seqS) \in \+L_{\bad}} \wh{p}_{\sigma, \tau, \seqS}^{\sigma} \cdot \Pr[X \sim \mu]{X \in \sigma} \\
        &= \sum_{(\sigma, \tau, \seqS) \in \+L_{\good}} \wh{p}_{\sigma, \tau, \seqS}^{\sigma} \cdot \Pr[X \sim \mu]{X \in \sigma} + \Pr[X \sim \mu]{X(e_\bot) = 1} \sum_{(\sigma, \tau, \seqS) \in \+L_{\bad}} \wh{p}_{\sigma, \tau, \seqS}^{\sigma} \cdot \Pr[X \sim \mu]{X \in \sigma \mid X(e_{\bot}) = 1} \\
        &\le \sum_{(\sigma, \tau, \seqS) \in \+L_{\good}} \wh{p}_{\sigma, \tau, \seqS}^{\sigma} \cdot \Pr[X \sim \mu]{X \in \sigma} + \left(1 - B_{\min}^2\right)^{\ell} \Pr[X \sim \mu]{X(e_\bot) = 1}
    \end{align*}
    where the last inequality holds from~\Cref{lem:LP-truncated-error}. Thus we have
    $$
        \sum_{(\sigma, \tau, \seqS) \in \+L_{\good}} \wh{p}_{\sigma, \tau, \seqS}^{\sigma} \cdot \Pr[X \sim \mu]{X \in \sigma} \le \Pr[X \sim \mu]{X(e_\bot) = 1} \le \frac{1}{1 - \left(1 - B_{\min}^2\right)^{\ell}} \sum_{(\sigma, \tau, \seqS) \in \+L_{\good}} \wh{p}_{\sigma, \tau, \seqS}^{\sigma} \cdot \Pr[X \sim \mu]{X \in \sigma}.
    $$
    Similar inequalities hold for $\Pr[X \sim \mu]{X(e_\bot) = 0}$:
    $$
        \sum_{(\sigma, \tau, \seqS) \in \+L_{\good}} \wh{p}_{\sigma, \tau, \seqS}^{\tau} \cdot \Pr[X \sim \mu]{X \in \tau} \le \Pr[X \sim \mu]{X(e_\bot) = 0} \le \frac{1}{1 - \left(1 - B_{\min}^2\right)^{\ell}} \sum_{(\sigma, \tau, \seqS) \in \+L_{\good}} \wh{p}_{\sigma, \tau, \seqS}^{\tau} \cdot \Pr[X \sim \mu]{X \in \tau}.
    $$
    Hence we have
    \begin{align*}
        \frac{\Pr[X \sim \mu]{X(e_\bot) = 1}}{\Pr[X \sim \mu]{X(e_\bot) = 0}} &\le \frac{1}{1 - \left(1 - B_{\min}^2\right)^{\ell}} \frac{\sum_{(\sigma, \tau, \seqS) \in \+L_{\good}} \wh{p}_{\sigma, \tau, \seqS}^{\sigma} \cdot \Pr[X \sim \mu]{X \in \sigma}}{\sum_{(\sigma, \tau, \seqS) \in \+L_{\good}} \wh{p}_{\sigma, \tau, \seqS}^{\tau} \cdot \Pr[X \sim \mu]{X \in \tau}} \\
        &\le \frac{r^+}{1 - \left(1 - B_{\min}^2\right)^{\ell}}
    \end{align*}
    and
    \begin{align*}
        \frac{\Pr[X \sim \mu]{X(e_\bot) = 1}}{\Pr[X \sim \mu]{X(e_\bot) = 0}} &\ge \left(1 - \left(1 - B_{\min}^2\right)^{\ell}\right) \frac{\sum_{(\sigma, \tau, \seqS) \in \+L_{\good}} \wh{p}_{\sigma, \tau, \seqS}^{\sigma} \cdot \Pr[X \sim \mu]{X \in \sigma}}{\sum_{(\sigma, \tau, \seqS) \in \+L_{\good}} \wh{p}_{\sigma, \tau, \seqS}^{\tau} \cdot \Pr[X \sim \mu]{X \in \tau}} \\
        &\ge \left(1 - \left(1 - B_{\min}^2\right)^{\ell}\right) r^-.
    \end{align*}
\end{proof}

\subsection{The marginal ratio estimator}

Now we design a marginal ratio estimator for instances satisfying~\Cref{cond:Holant-condition} using the above linear program.

% \begin{theorem} \label{thm:Holant-marginal-estimator}
%     Given a positive integer $\Delta > 0$, a positive real $r > 0$ and a small tolerance error $\varepsilon \in (0, 1/4)$, there exists a deterministic algorithm $\+A$ such that for every instance $(\Phi = (G, \vecf), \Delta, r)$ satisfying~\Cref{cond:Holant-condition} and $e \in E(G)$, within time $\poly\left(\left(\frac{1}{\varepsilon}\right)^{C_{\ref{thm:Holant-marginal-estimator}}(\Delta, r)}\right)$, $\+A$ outputs a number $\wh{P}$ such that
%     $$
%         (1 - \varepsilon) \Pr[X \sim \mu_\Phi]{X(e) = 0} \le \wh{P} \le (1 + \varepsilon) \Pr[X \sim \mu_\Phi]{X(e) = 0}
%     $$
%     where $C_{\ref{thm:Holant-marginal-estimator}}(\Delta, r)$ and coefficients in the polynomial depend only on $\Delta$ and $r$.
% \end{theorem}

\begin{theorem} \label{thm:Holant-marginal-ratio-estimator}
    Given a positive integer $\Delta > 0$, a positive real $r > 0$ and a small tolerance error $\varepsilon \in (0, 1/4)$, there exists a deterministic algorithm $\+A$ such that for every instance $(\Phi = (G, \vecf), \Delta, r)$ satisfying~\Cref{cond:Holant-condition} and $e \in E(G)$, within time $\poly\left((\varepsilon^{-1})^{C_{\ref{thm:Holant-marginal-ratio-estimator}}(\Delta, r)}\right)$, $\+A$ outputs a number $\wh{R}$ such that
    $$
        (1 - \varepsilon) R_{\Phi}(e) \le \wh{R} \le (1 + \varepsilon) R_{\Phi}(e)
    $$
    where $C_{\ref{thm:Holant-marginal-ratio-estimator}}(\Delta, r)$ and coefficients in the polynomial depend only on $\Delta$ and $r$.
\end{theorem}

The key ingredient is the following estimator for the marginal ratio of a half-edge via linear programming.

\begin{theorem} \label{thm:LP-ratio-estimator}
    Given a positive integer $\wh{\Delta} > 0$, a positive number $\wh{r} > 0$ and a small tolerance error $\varepsilon \in (0, 1/4)$, there exists a deterministic algorithm $\+A_{\bot}$ such that for every instance $(\Phi = (G, \vecf), \sigma_\bot, \tau_\bot, v_\bot)$ satisfying~\Cref{cond-instancepair} and $\Delta(G) \le \wh{\Delta}$, $r_{\max}(\Phi) \le \wh{r}$, within time $\poly\left((\varepsilon^{-1})^{C_{\ref{thm:LP-ratio-estimator}}(\wh{\Delta}, \wh{r})}\right)$, $\+A_{\bot}$ outputs a number $\wh{R}$ such that
    $$
        (1 - \varepsilon) R_{\Phi}(e_\bot) \le \wh{R} \le (1 + \varepsilon) R_{\Phi}(e_\bot)
    $$
    where $C_{\ref{thm:LP-ratio-estimator}}(\wh{\Delta}, \wh{r})$ and the coefficients in the polynomial depend only on $\wh{\Delta}$ and $\wh{r}$.
\end{theorem}
\begin{proof}
    For simplicity, let $R = r_{\max}(\Phi)$ and $\Delta = \Delta(G)$. Given the tolerance error $\varepsilon \in (0, 1/4)$, we set
    $$
        \ell = \left\lceil \frac{3 + \log{r} + \log{\varepsilon^{-1}}}{\log\left(1 - B_{\min}(\Phi)^2\right)^{-1}} \right\rceil.
    $$
    With a binary search on the parameters $r^-$ and $r^+$, we can obtain an $\varepsilon$-approximation to $R_\Phi(e_\bot)$ by~\Cref{thm:ratio-bound-by-LP}. The number of rounds the binary search takes is $O\left(r\log{\frac{1}{\varepsilon}}\right)$.

    For the running time on each round of the LP, recall that $B_{\min}(\Phi) = (1 + r^2)^{-\Delta}$. Then we have
    \begin{align*}
        \Delta^{\Delta \ell} \le \exp\left(\Delta\left(1 + \frac{3 + \log{r} + \log{\varepsilon^{-1}}}{-\log\left(1 - (1 + r^2)^{-2\Delta}\right)}\right) \log{\Delta}\right) = \exp(C_1(\Delta, r)) \cdot \left(\varepsilon^{-1}\right)^{C_2(\Delta, r)}
    \end{align*}
    where $C_1(\Delta, r), C_2(\Delta, r)$ are functions on $\Delta, r$ defined as
    \begin{align*}
        C_1(\Delta, r) = \Delta \log{\Delta} \left(1 + \frac{3 + \log{r}}{-\log\left(1 - (1 + r^2)^{-2\Delta}\right)}\right), \quad C_2(\Delta, r) = \frac{\Delta \log \Delta}{-\log\left(1 - (1 + r^2)^{-2\Delta}\right)}.
    \end{align*}
    Note that $C_1(\Delta, r)$ and $C_2(\Delta, r)$ are both increasing functions to $\Delta$ and $r$. Then by~\Cref{lem:building-cost-of-LP}, the running time of the marginal ratio estimator is at most $\poly\left(C_1(\wh{\Delta}, \wh{r}) (\varepsilon^{-1})^{C_2(\wh{\Delta}, \wh{r})}\right)$. Hence we conclude the theorem.
\end{proof}

\begin{proof}[Proof of~\Cref{thm:Holant-marginal-ratio-estimator}]
    % Firstly we show how to design an estimator for the marginal ratio in the instance $\Phi$ satisfying~\Cref{cond:Holant-condition} by the algorithm in~\Cref{thm:Holant-marginal-ratio-estimator}, and then approximate the marginal probability via this ratio estimator.
    
    Given an instance $(\Phi = (G = (V, E), \vecf), \Delta, r)$ satisfying~\Cref{cond:Holant-condition} and an edge $e = \set{u, v} \in E$, when $f_u(1) = 0$ or $f_v(1) = 0$, it holds that $R_\Phi(e) = 0$ and nothing remains to do. Otherwise $\mu_{e}(1) > 0$. 
    We turn $\Phi$ to two instances $\Phi^{(1)}, \Phi^{(2)}$ satisfying~\Cref{cond-instancepair} by splitting the edge $e$, and estimate $R_{\Phi}(e)$ by applying $\+A_\bot$ in~\Cref{thm:LP-ratio-estimator} to $\Phi^{(1)}$ and $\Phi^{(2)}$.
    
    % Now we split $e$ into two half-edges and construct two instances satisfying~\Cref{cond-instancepair}. 
    Formally speaking, let $e_u = \set{u}$ and $e_v = \set{v}$ be two half-edges. Let $G^{(0)} = \left(V, E^{(0)} = E \setminus \set{e} \cup \set{e_u, e_v}\right)$, $G^{(1)} = \left(V, E^{(1)} = E \setminus \set{e} \cup \set{e_v}\right)$ and $G^{(2)} = \left(V, E^{(2)} = E \setminus \set{e} \cup \set{e_u}\right)$. Then we know that the maximum degrees of these three graphs $\Delta(G^{(0)}), \Delta(G^{(1)})$ and $\Delta(G^{(2)})$ are at most $\Delta$.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        % Left graph
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (lu) at (0, 0) {};
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (lv) at (3, 0) {};
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (l1) at (-1, 1) {};
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (l2) at (-1, -1) {};
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (l3) at (1, 1) {};
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (l4) at (1, -1) {};
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (l5) at (4, 1) {};
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (l6) at (4, -1) {};
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (l7) at (2, 1) {};
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (l8) at (2, -1) {};
    
        \draw (lu) -- (l1);
        \draw (lu) -- (l2);
        \draw (lu) -- (l3);
        \draw (lu) -- (l4);
        \draw (lu) -- (lv) node[midway, above] {$e$};
        \draw (lv) -- (l5);
        \draw (lv) -- (l6);
        \draw (lv) -- (l7);
        \draw (lv) -- (l8);

        \node at (0, -0.3) {$u$};
        \node at (3, -0.3) {$v$};

        % Arrow
        \node at (5.5, 0) {$\stackrel{\mbox{split $e$}}{\Longrightarrow}$};
    
        % Right graph (left part)
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (ru1) at (8, 0) {};
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (r1) at (7, 1) {};
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (r2) at (7, -1) {};
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (r3) at (9, 1) {};
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (r4) at (9, -1) {};
        % \node[draw, circle, fill=black] (re1) at (7, 0) {};

        \draw (ru1) -- (r1);
        \draw (ru1) -- (r2);
        \draw (ru1) -- (r3);
        \draw (ru1) -- (r4);
        \draw (ru1) -- (9, 0) node[midway, below] {$e_u$};

        \node at (8, -0.3) {$u$};
    
        % Right graph (right part)
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (rv1) at (11, 0) {};
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (r5) at (10, 1) {};
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (r6) at (10, -1) {};
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (r7) at (12, 1) {};
        \node[circle, fill, inner sep=1pt, minimum size=6pt] (r8) at (12, -1) {};
        % \node[draw, circle, fill=black] (re2) at (8, 0) {};

        \draw (rv1) -- (r5);
        \draw (rv1) -- (r6);
        \draw (rv1) -- (r7);
        \draw (rv1) -- (r8);
        \draw (rv1) -- (10, 0) node[midway, below] {$e_v$};

        \node at (11, -0.3) {$v$};
    \end{tikzpicture}
    \caption{an example of splitting the edge $e = \set{u, v}$.}
    \label{fig:splitting-edge}
\end{figure}
    
    Now we consider the following three instances:
    \begin{itemize}
        \item $\Phi^{(0)} = \left(G^{(0)}, \vecf\right)$.
        \item $\Phi^{(1)} = \left(G^{(1)}, \vecf^{(1)}\right)$ where $\vecf^{(1)} = \set{f_w^{(1)}}_{w \in V}$ is constructed as $f_w^{(1)} = f_w$ for $w \neq u$ and $f_u^{(1)}(i) = f_u(i + 1)$ for every $0 \le i < \deg_G(u)$.
        \item $\Phi^{(2)} = \left(G^{(2)}, \vecf\right)$.
    \end{itemize}
    Note that $f_u^{(1)}(1)$ might be $0$. In this case $f_u^{(1)}(i) = 0$ for every $1 \le i < \deg_G(u)$ since $f_u$ is positive log-concave. And we can safely remove $u$ and edges incident to $u$ from $G^{(1)}$ and the marginal ratio in $\Phi^{(1)}$ keeps invariant.
    Assume that $\Phi^{(1)}$ is the Holant instance after necessary removal. By definition, we have $\Phi^{(1)}$ and $\Phi^{(2)}$ both satisfy~\Cref{cond-instancepair} and $r_{\max}(\Phi^{(1)}) \le r$, $r_{\max}(\Phi^{(2)}) \le r$. By computation,
    \begin{align*}
        R_{\Phi}(e) &= \frac{\Pr[X \sim \mu_{\Phi}]{X(e) = 1}}{\Pr[X \sim \mu_{\Phi}]{X(e) = 0}} \\
        &= \frac{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = X(e_v) = 1 \mid X(e_u) = X(e_v)}}{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = X(e_v) = 0 \mid X(e_u) = X(e_v)}} \\
        &= \frac{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = X(e_v) = 1}}{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = X(e_v) = 0}} \\
        &= \frac{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = 1 \land X(e_v) = 1}}{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = 1 \land X(e_v) = 0}} \cdot \frac{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = 1 \land X(e_v) = 0}}{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = 0 \land X(e_v) = 0}} \\
        &= \frac{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_v) = 1 \mid X(e_u) = 1}}{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_v) = 0 \mid X(e_u) = 1}} \cdot \frac{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = 1 \mid X(e_v) = 0}}{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = 0 \mid X(e_v) = 0}} \\
        &= \frac{\Pr[X \sim \mu_{\Phi^{(1)}}]{X(e_v) = 1}}{\Pr[X \sim \mu_{\Phi^{(1)}}]{X(e_v) = 0}} \cdot \frac{\Pr[X \sim \mu_{\Phi^{(2)}}]{X(e_u) = 1}}{\Pr[X \sim \mu_{\Phi^{(2)}}]{X(e_u) = 0}} \\
        &= R_{\Phi^{(1)}}(e_v) \cdot R_{\Phi^{(2)}}(e_u).
    \end{align*}
    To obtain an $\varepsilon$-approximation to $R_\Phi(e)$, set $\varepsilon' = \varepsilon/3$. We employ $\+A_\bot$ in~\Cref{thm:LP-ratio-estimator} to obtain $\varepsilon'$-approximations $\wh{R}_1$ and $\wh{R}_2$ to $R_{\Phi^{(1)}}(e_v)$ and $R_{\Phi^{(2)}}(e_u)$ respectively. Then let $\wh{R} = \wh{R}_1 \cdot \wh{R}_2$. By~\Cref{thm:LP-ratio-estimator}, we have that
    \begin{align*}
        \wh{R} \ge (1 - \varepsilon/3)^2 R_{\Phi^{(1)}}(e_v) \cdot R_{\Phi^{(2)}}(e_u) \ge (1 - \varepsilon) R_{\Phi}(e)
    \end{align*}
    and
    \begin{align*}
        \wh{R} \le (1 + \varepsilon/3)^2 R_{\Phi^{(1)}}(e_v) \cdot R_{\Phi^{(2)}}(e_u) \le (1 + \varepsilon) R_{\Phi}(e).
    \end{align*}

    Now we consider the running time. Note that in the estimating process, we only make $2$ calls to $\+A_\bot$ with tolerance error $\varepsilon/3$ on input $\Phi^{(1)}, \Phi^{(2)}$ respectively to obtain $\wh{R}_1$ and $\wh{R}_2$. By~\Cref{thm:LP-ratio-estimator}, we can obtain $\wh{R} = \wh{R}_1 \cdot \wh{R}_2$ in time $\poly\left((3/\varepsilon)^{C(\Delta, r)}\right) = \poly\left((\varepsilon^{-1})^{C_{\ref{thm:Holant-marginal-ratio-estimator}}}\right)$ where the coefficients of the polynomial and $C_{\ref{thm:Holant-marginal-ratio-estimator}}$ depend only on $\Delta$ and $r$.

    Combining all things, we conclude the theorem.
    % Now we approximate $\Pr[X \sim \mu_{\Phi}]{X(e) = 0}$ from $\wh{R}$. Recall that $\wh{R}$ is an $(\varepsilon/2)$-approximation to $R_{\Phi}(e)$. Since $\Pr[X \sim \mu_{\Phi}]{X(e) = 0} = \frac{1}{1 + R_\Phi(e)}$, set $\wh{P} = \frac{1}{1 + \wh{R}}$ and it holds that
    % \begin{align*}
    %     \wh{P} \ge \frac{1}{1 + (1 + \varepsilon/2) R_{\Phi}(e)} \ge \frac{1}{1 + \varepsilon/2} \frac{1}{1 + R_{\Phi}(e)} \ge (1 - \varepsilon) \Pr[X \sim \mu_{\Phi}]{X(e) = 0}
    % \end{align*}
    % and
    % \begin{align*}
    %     \wh{P} \le \frac{1}{1 + (1 - \varepsilon/2) R_{\Phi}(e)} \le \frac{1}{1 - \varepsilon/2} \frac{1}{1 + R_{\Phi}(e)} \le (1 + \varepsilon) \Pr[X \sim \mu_{\Phi}]{X(e) = 0}
    % \end{align*}
    % where we use inequalities $\frac{1}{1 + x} \ge 1 - 2x$ for every $x \ge 0$ and $\frac{1}{1 - x} \le 1 + 2x$ for every $0 \le x \le \frac{1}{2}$. Hence we conclude an $\varepsilon$-approximation to the marginal probability.

    % Now we consider the running time. Note that in the estimating process, we only make $2$ calls to $\+A_\bot$ with tolerance error $\varepsilon/6$. Then we can obtain $\wh{P}$ within time $\poly\left((6/\varepsilon)^{C(\Delta, r)}\right) = \poly\left((1/\varepsilon)^{C_{\ref{thm:Holant-marginal-estimator}}}\right)$ where the coefficients of the polynomial and $C_{\ref{thm:Holant-marginal-estimator}}$ depend only on $\Delta$ and $r$.
\end{proof}

% \begin{proof}[Proof of~\Cref{thm:Holant-marginal-estimator}]
%     Firstly we show how to design an estimator for the marginal ratio in the instance $\Phi$ satisfying~\Cref{cond:Holant-condition} by the algorithm in~\Cref{thm:Holant-marginal-ratio-estimator}, and then approximate the marginal probability via this ratio estimator.

%     Given an instance $(\Phi = (G, \vecf), \Delta, r)$ satisfying~\Cref{cond:Holant-condition} and an edge $e = \set{u, v} \in E(G)$, when $f_u(1) = 0$ or $f_v(1) = 0$, it holds that $R_\Phi(e) = 0$ and nothing remains to do. Otherwise $\Pr[X \sim \mu_\Phi]{X(e) = 1} > 0$. Now we split $e$ into two half-edges and construct two instances satisfying~\Cref{cond-instancepair}. Formally speaking, let $e_u = \set{u}$ and $e_v = \set{v}$ be two half-edges. Let $G^{(0)} = \left(V(G), E^{(0)} = E(G) \setminus \set{e} \cup \set{e_u, e_v}\right)$, $G^{(1)} = \left(V(G), E^{(1)} = E(G) \setminus \set{e} \cup \set{e_v}\right)$ and $G^{(2)} = \left(V(G), E^{(2)} = E(G) \setminus \set{e} \cup \set{e_u}\right)$. Then we know that $\Delta(G^{(0)}), \Delta(G^{(1)})$ and $\Delta(G^{(2)})$ are at most $\Delta$.

% \begin{figure}[htbp]
%     \centering
%     \begin{tikzpicture}
%         % Left graph
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (lu) at (0, 0) {};
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (lv) at (3, 0) {};
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (l1) at (-1, 1) {};
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (l2) at (-1, -1) {};
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (l3) at (1, 1) {};
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (l4) at (1, -1) {};
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (l5) at (4, 1) {};
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (l6) at (4, -1) {};
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (l7) at (2, 1) {};
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (l8) at (2, -1) {};
    
%         \draw (lu) -- (l1);
%         \draw (lu) -- (l2);
%         \draw (lu) -- (l3);
%         \draw (lu) -- (l4);
%         \draw (lu) -- (lv) node[midway, above] {$e$};
%         \draw (lv) -- (l5);
%         \draw (lv) -- (l6);
%         \draw (lv) -- (l7);
%         \draw (lv) -- (l8);

%         \node at (0, -0.3) {$u$};
%         \node at (3, -0.3) {$v$};

%         % Arrow
%         \node at (5.5, 0) {$\stackrel{\mbox{split $e$}}{\Longrightarrow}$};
    
%         % Right graph (left part)
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (ru1) at (8, 0) {};
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (r1) at (7, 1) {};
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (r2) at (7, -1) {};
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (r3) at (9, 1) {};
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (r4) at (9, -1) {};
%         % \node[draw, circle, fill=black] (re1) at (7, 0) {};

%         \draw (ru1) -- (r1);
%         \draw (ru1) -- (r2);
%         \draw (ru1) -- (r3);
%         \draw (ru1) -- (r4);
%         \draw (ru1) -- (9, 0) node[midway, below] {$e_u$};

%         \node at (8, -0.3) {$u$};
    
%         % Right graph (right part)
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (rv1) at (11, 0) {};
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (r5) at (10, 1) {};
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (r6) at (10, -1) {};
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (r7) at (12, 1) {};
%         \node[circle, fill, inner sep=1pt, minimum size=6pt] (r8) at (12, -1) {};
%         % \node[draw, circle, fill=black] (re2) at (8, 0) {};

%         \draw (rv1) -- (r5);
%         \draw (rv1) -- (r6);
%         \draw (rv1) -- (r7);
%         \draw (rv1) -- (r8);
%         \draw (rv1) -- (10, 0) node[midway, below] {$e_v$};

%         \node at (11, -0.3) {$v$};
%     \end{tikzpicture}
%     \caption{an example of splitting the edge $e = \set{u, v}$.}
%     \label{fig:splitting-edge}
% \end{figure}
    
%     Now we consider the following three instances:
%     \begin{itemize}
%         \item $\Phi^{(0)} = \left(G^{(0)}, \vecf\right)$.
%         \item $\Phi^{(1)} = \left(G^{(1)}, \vecf^{(1)}\right)$ where $\vecf^{(1)} = \set{f_w^{(1)}}_{w \in V}$ is constructed as $f_w^{(1)} = f_w$ for $w \neq u$ and $f_u^{(1)}(i) = f_u(i + 1)$ for every $0 \le i < \deg_G(u)$.
%         \item $\Phi^{(2)} = \left(G^{(2)}, \vecf\right)$.
%     \end{itemize}
%     Note that $f_u^{(1)}(1)$ might be $0$. In this case $f_u^{(1)}(i) = 0$ for every $1 \le i < \deg_G(u)$ since $f_u$ is positive log-concave. And we can safely remove $u$ and edges incident to $u$ from $G^{(1)}$ and the marginal ratio in $\Phi^{(1)}$ keeps invariant.
%     Assume that $\Phi^{(1)}$ is the Holant instance after necessary removal. By definition, we have $\Phi^{(1)}$ and $\Phi^{(2)}$ both satisfy~\Cref{cond-instancepair} and $r_{\max}(\Phi^{(1)}) \le r$, $r_{\max}(\Phi^{(2)}) \le r$. By computation,
%     \begin{align*}
%         R_{\Phi}(e) &= \frac{\Pr[X \sim \mu_{\Phi}]{X(e) = 1}}{\Pr[X \sim \mu_{\Phi}]{X(e) = 0}} \\
%         &= \frac{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = X(e_v) = 1 \mid X(e_u) = X(e_v)}}{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = X(e_v) = 0 \mid X(e_u) = X(e_v)}} \\
%         &= \frac{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = X(e_v) = 1}}{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = X(e_v) = 0}} \\
%         &= \frac{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = 1 \land X(e_v) = 1}}{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = 1 \land X(e_v) = 0}} \cdot \frac{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = 1 \land X(e_v) = 0}}{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = 0 \land X(e_v) = 0}} \\
%         &= \frac{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_v) = 1 \mid X(e_u) = 1}}{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_v) = 0 \mid X(e_u) = 1}} \cdot \frac{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = 1 \mid X(e_v) = 0}}{\Pr[X \sim \mu_{\Phi^{(0)}}]{X(e_u) = 0 \mid X(e_v) = 0}} \\
%         &= \frac{\Pr[X \sim \mu_{\Phi^{(1)}}]{X(e_v) = 1}}{\Pr[X \sim \mu_{\Phi^{(1)}}]{X(e_v) = 0}} \cdot \frac{\Pr[X \sim \mu_{\Phi^{(2)}}]{X(e_u) = 1}}{\Pr[X \sim \mu_{\Phi^{(2)}}]{X(e_u) = 0}} \\
%         &= R_{\Phi^{(1)}}(e_v) \cdot R_{\Phi^{(2)}}(e_u).
%     \end{align*}
%     Now we get an $\varepsilon/2$-approximation to $R_\Phi(e)$ with $\+A_\bot$ in~\Cref{thm:Holant-marginal-ratio-estimator}.
%     Set $\varepsilon' = \varepsilon/6$. We employ $\+A_\bot$ in~\Cref{thm:Holant-marginal-ratio-estimator} to obtain $\varepsilon'$-approximations $\wh{R}_1$ and $\wh{R}_2$ to $R_{\Phi^{(1)}}(e_v)$ and $R_{\Phi^{(2)}}(e_u)$ respectively. Then let $\wh{R} = \wh{R}_1 \cdot \wh{R}_2$. By~\Cref{thm:Holant-marginal-ratio-estimator}, we have that
%     \begin{align*}
%         \wh{R} \ge (1 - \varepsilon/6)^2 R_{\Phi^{(1)}}(e_v) \cdot R_{\Phi^{(2)}}(e_u) \ge (1 - \varepsilon/2) R_{\Phi}(e)
%     \end{align*}
%     and
%     \begin{align*}
%         \wh{R} \le (1 + \varepsilon/6)^2 R_{\Phi^{(1)}}(e_v) \cdot R_{\Phi^{(2)}}(e_u) \le (1 + \varepsilon/2) R_{\Phi}(e).
%     \end{align*}

%     Now we approximate $\Pr[X \sim \mu_{\Phi}]{X(e) = 0}$ from $\wh{R}$. Recall that $\wh{R}$ is an $(\varepsilon/2)$-approximation to $R_{\Phi}(e)$. Since $\Pr[X \sim \mu_{\Phi}]{X(e) = 0} = \frac{1}{1 + R_\Phi(e)}$, set $\wh{P} = \frac{1}{1 + \wh{R}}$ and it holds that
%     \begin{align*}
%         \wh{P} \ge \frac{1}{1 + (1 + \varepsilon/2) R_{\Phi}(e)} \ge \frac{1}{1 + \varepsilon/2} \frac{1}{1 + R_{\Phi}(e)} \ge (1 - \varepsilon) \Pr[X \sim \mu_{\Phi}]{X(e) = 0}
%     \end{align*}
%     and
%     \begin{align*}
%         \wh{P} \le \frac{1}{1 + (1 - \varepsilon/2) R_{\Phi}(e)} \le \frac{1}{1 - \varepsilon/2} \frac{1}{1 + R_{\Phi}(e)} \le (1 + \varepsilon) \Pr[X \sim \mu_{\Phi}]{X(e) = 0}
%     \end{align*}
%     where we use inequalities $\frac{1}{1 + x} \ge 1 - 2x$ for every $x \ge 0$ and $\frac{1}{1 - x} \le 1 + 2x$ for every $0 \le x \le \frac{1}{2}$. Hence we conclude an $\varepsilon$-approximation to the marginal probability.

%     Now we consider the running time. Note that in the estimating process, we only make $2$ calls to $\+A_\bot$ with tolerance error $\varepsilon/6$. Then we can obtain $\wh{P}$ within time $\poly\left((6/\varepsilon)^{C(\Delta, r)}\right) = \poly\left((1/\varepsilon)^{C_{\ref{thm:Holant-marginal-estimator}}}\right)$ where the coefficients of the polynomial and $C_{\ref{thm:Holant-marginal-estimator}}$ depend only on $\Delta$ and $r$.
% \end{proof}

\section{Approximate Counting}

Now we give our deterministic algorithm to approximately count the partition function of instances satisfying~\Cref{cond:Holant-condition} via the marginal ratio estimator in~\Cref{thm:Holant-marginal-ratio-estimator}.

\begin{proof}[Proof of~\Cref{thm:counting-Holant}]
    For the Holant instance $\Phi = (G = (V, E), \vecf)$, recall that we order $E$ as $E = e_1, \ldots, e_m$ where $m = \abs{E}$. For every $1 \le i \le m + 1$, define $G_i = (V, E_i)$ where $E_i = E \setminus \set{e_1, \ldots, e_{i - 1}}$. We remark here that $G_1 = G$ and $G_{m + 1} = (V, \emptyset)$. Then we define the Holant instance $\Phi_i = (G_i, \vecf)$ for every $i \in [m]$. Obviously $(\Phi_i, \Delta, r)$ satisfies~\Cref{cond:Holant-condition} for every $i \in [m]$. Then,
    \begin{align*}
        Z_{\Phi} &= Z_{\Phi}^{(e_1 \gets 0)} + Z_{\Phi}^{(e_1 \gets 1)} \\
        &= \left(1 + R_{\Phi}(e_1)\right) Z_{\Phi}^{(e_1 \gets 0)} \\
        &= \left(1 + R_{\Phi_1}(e_1)\right) Z_{\Phi_2} \\
        &= \left(1 + R_{\Phi_1}(e_1)\right) \left(Z_{\Phi_2}^{(e_2 \gets 0)} + Z_{\Phi_2}^{(e_2 \gets 1)}\right) \\
        &= \left(1 + R_{\Phi_1}(e_1)\right) \left(1 + R_{\Phi_2}(e_2)\right) Z_{\Phi_3} \\
        &= \cdots \\
        &= Z_{\Phi_{m + 1}}\prod_{i = 1}^{m} \left(1 + R_{\Phi_i}(e_i)\right) \\
        &= \prod_{v \in V} f_v(0) \prod_{i = 1}^{m} \left(1 + R_{\Phi_i}(e_i)\right)
    \end{align*}
    where the last identity follows the fact that $E_{m + 1} = \emptyset$. For every $1 \le i \le m$, we run the marginal ratio estimator $\+A$ in~\Cref{thm:Holant-marginal-ratio-estimator} with tolerance error $\varepsilon/(2m)$ on input $\Phi_i, e_i$ to obtain an $\varepsilon/(2m)$-approximation $\wh{R}_i$ to $R_{\Phi_i}(e_i)$. Let $\wh{Z} = \prod_{v \in V} f_v(0) \prod_{i = 1}^{m} (1 + \wh{R}_i)$. Then it holds that
    \begin{gather*}
        \frac{\wh{Z}}{Z_\Phi} \ge \prod_{i = 1}^{m} \frac{1 + (1 - \varepsilon/(2m)) R_{\Phi_i}(e_i)}{1 + R_{\Phi_i}(e_i)} \ge (1 - \varepsilon/(2m))^m \ge 1 - \varepsilon, \\
        \frac{\wh{Z}}{Z_\Phi} \le \prod_{i = 1}^{m} \frac{1 + (1 + \varepsilon/(2m)) R_{\Phi_i}(e_i)}{1 + R_{\Phi_i}(e_i)} \le (1 + \varepsilon/(2m))^m \le 1 + \varepsilon.
    \end{gather*}
    Here we use inequalities $(1 - x)^{n} \ge 1 - nx$ and $(1 + x/(2n))^n \le e^{x/2} \le 1 + x$ for $0 \le x \le 1$ and $n \in \mathbb{N}$. This means that $\wh{Z}$ is an $\varepsilon$-approximation to $Z_{\Phi}$.

    For the running time, firstly we can obtain $\prod_{v \in V} f_v(0)$ in time $O(\abs{V})$. For every $i \in [m]$, by~\Cref{thm:Holant-marginal-ratio-estimator} on input $\Phi_i, e_i$ with tolerance error $\varepsilon/(2m)$, we can obtain $\wh{R}_i$ in time $\poly\left((m/\varepsilon)^{C(\Delta, r)}\right)$ where the coefficients of the polynomial and $C(\Delta, r)$ depend only on $\Delta$ and $r$. Hence we can calculate $\wh{Z}$ in time $\poly\left((m/\varepsilon)^{C(\Delta, r)}\right)$. Then we finish the proof.
\end{proof}

The algorithm for counting $b$-matchings comes directly from~\Cref{thm:counting-Holant}.
\begin{proof}[Proof of~\Cref{thm:counting-b-matchings}]
    For a graph $G = (V, E)$ with maximum degree $\le \Delta$ and every positive integer $b \le \Delta$, we view $b$-matchings as a Holant instance $\Phi_{G, b} = (G, \vecf)$. Formally speaking, we associate every $v \in V$ with $f_v$ defined as $f_v(i) = \id{i \le b}$ for every $0 \le i \le \Delta$. By definition, the instance $\left(\Phi_{G, b}, \Delta, 1\right)$ satisfies~\Cref{cond:Holant-condition}. Applying~\Cref{thm:counting-Holant} on input $\Phi_{G, b}$ with tolerance error $\varepsilon$, we obtain an $\varepsilon$-approximation to $\abs{\Omega_{G, b}}$ in time $\poly\left((\abs{E} \cdot \varepsilon^{-1})^{C}\right)$ where $C$ depends only on $\Delta$.
\end{proof}

\section{Sampling}

\zdtodo{Sampling results: design the algorithm}

\section{Counterexample}
\zdtodo{move this part to the end of the construction of the coupling tree.}

In this section, we give a counterexample to show that the updated edge cannot be arbitrarily chosen in the coupling process.

Let $G = (V, E)$ be the graph shown in~\Cref{fig:edge-selection-counterexample} with $V = \set{v_\bot, v_1, v_2, v_3, v_4, v_5}$ and $E = \set{e_1, e_2, e_3, e_4, e_5, e_6, e_\bot}$ consisting of normal edges $e_1 = \{v_\bot, v_1\}, e_2 = \{v_\bot, v_2\}, e_3 = \{v_\bot, v_3\}, e_4 = \{v_1, v_4\}, e_5 = \{v_3, v_5\}, e_6 = \{v_4, v_5\}$ and a half-edge $e_\bot = \set{v_\bot}$.
Let $t$ be a large positive integer which will be picked later. We assign signatures $\vecf = \set{f_v}_{v \in V}$ as $f_{v_\bot} = [1, 1, 1]$, $f_{v_2} = [1, 1]$ and $f_{v_1} = f_{v_3} = f_{v_4} = f_{v_5} = [1, t]$.

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}
        % Vertices
        \node (v0) at (0, 3) [circle, fill, inner sep=1pt, minimum size=6pt, label=above:$v_\bot$] {};
        \node (v1) at (-2, 1) [circle, fill, inner sep=1pt, minimum size=6pt, label=left:$v_1$] {};
        \node (v2) at (0, 1) [circle, fill, inner sep=1pt, minimum size=6pt, label=right:$v_2$] {};
        \node (v3) at (2, 1) [circle, fill, inner sep=1pt, minimum size=6pt, label=right:$v_3$] {};
        \node (v4) at (-2, -1) [circle, fill, inner sep=1pt, minimum size=6pt, label=below:$v_4$] {};
        \node (v5) at (2, -1) [circle, fill, inner sep=1pt, minimum size=6pt, label=below:$v_5$] {};
        % Edges
        \draw (v0) -- (2, 3) node[midway, above] {$e_{\bot}$};
        \draw (v0) -- (v1) node[midway, left] {$e_1$};
        \draw (v0) -- (v2) node[midway, right] {$e_2$};
        \draw (v0) -- (v3) node[midway, right] {$e_3$};
        \draw (v1) -- (v4) node[midway, left] {$e_4$};
        \draw (v3) -- (v5) node[midway, right] {$e_5$};
        \draw (v4) -- (v5) node[midway, below] {$e_6$};
    \end{tikzpicture}
    \caption{The counterexample of the edge selection.}
    \label{fig:edge-selection-counterexample}
\end{figure}

Let $\sigma = (e_\bot \gets 0)$ and $\tau = (e_\bot \gets 1)$. It is easy to verify that the instance $(\Phi = (G, \vecf), \sigma, \tau, v_\bot)$ satisfies~\Cref{cond-instancepair}. Consider the marginals of $e_2 = \set{v_\bot, v_2}$. By direct calculation, we have $\Ham(\sigma, E_{v_\bot}) < \Ham(\tau, E_{v_\bot})$ and
$$
    \mu_{e_2}^{\sigma}(1) = \frac{t^4 + 4t^3 + 3t^2 + 2t + 1}{3t^4 + 8t^3 + 7t^2 + 4t + 2}, \quad \mu_{e_2}^{\tau}(1) = \frac{t^4 + 3t^2 + 1}{2t^4 + 4t^3 + 6t^2 + 2t + 2}.
$$
Pick $t = 10$, and it holds that
$$
    \mu_{e_2}^{\sigma}(1) = \frac{14321}{38742} < \frac{10301}{24622} = \mu_{e_2}^{\tau}(1),
$$
meaning that there exists an instance $(\Phi, \sigma, \tau, v)$ and an edge $e \in E_v^\sigma$ such that $\Ham(\sigma, E_v) < \Ham(\tau, E_v)$ and $\mu_{e}^{\sigma}(1) < \mu_{e}^{\tau}(1)$.

% Let $G = (V,E)$ with $V  = \{v_0,v_1,v_2,v_3,v_4,v_5\}$
% and $E=\{e_1,e_2,e_3,e_4,e_5,e_6\}$ where 
% % $e_1 = \{v_0,v_1\},e_2 = \{v_0,v_2\},e_3 = \{v_0,v_3\},e_4 = \{v_1,v_4\},e_5 = \{v_3,v_5\},e_6=\{v_4,v_5\}$.
% Moreover, let $t$ be a large integer.
% $f_{v_0} = [1,1,1]$, $f_{v_2} = [1,1]$,
% $f_{v_1} = f_{v_3} = f_{v_4} = f_{v_5} = [1,t]$.

% One can verify that only three subgraphs of $G$ with a weight larger than $t^4$.

% The first one is the graph 
% $G_1 = (V,E_1)$ where $E_1 = (e_1,e_3,e_6)$. And the weight is $t^4$.

% The second one is the graph 
% $G_2 = (V,E_2)$ where $E_2 = (e_4,e_5)$. And the weight is $t^4$.

% The second one is the graph 
% $G_3 = (V,E_3)$ where $E_2 = (e_2,e_4,e_5)$. And the weight is $t^4$.

% When $t$ goes into infinity, only these three subgraphs are valid.
% Thus, the probability that the edge $e_2$ occurs is 1/3.

% Consider another $f'_{v_0} = [1,1]$.
% One can verify that when $t$ goes into infinity, only $G_2$ and $G_3$ are valid.
% Thus, the probability that the edge $e_2$ occurs is 1/2,
% larger than 1/3.

% In the first instance:
% $$
%     \mu^{e_\bot = 0}_{e_2}(1) = \frac{t^4 + 4t^3 + 3t^2 + 2t + 1}{3t^4 + 8t^3 + 7t^2 + 4t + 2}
% $$
% while in the second instance:
% $$
%     \mu^{e_\bot = 1}_{e_2}(1) = \frac{t^4 + 3t^2 + 1}{2t^4 + 4t^3 + 6t^2 + 2t + 2}.
% $$


\appendix

\section{Omitted proofs}\label{sec:omitted Proofs}

\OneroundDecay*
\begin{proof}
Let $\ell$ denote $\abs{E_v^\sigma}$. 
% Let $\ell$ denote the number of times the while loop is executed in $\!{Couple}(\Phi, \sigma, \tau, v)$.
% We remark that $\ell$ is a random variable.
For each $i>0$,
let $e_i, \sigma_i,\tau_i,\sigma_{e_i},\tau_{e_i}$ be the random variables $e,\sigma,\tau,\sigma_{e},\tau_{e}$ in Line \ref{line-edge-sample} of the $i$-th iteration of the while loop, respectively.
Let $\+E$ denote the event that 
$\land_{i\in [\ell]}(\sigma_{e_i} = \tau_{e_i} = ( e_i\leftarrow 0))$.
We remark that this event is well-defined.
Because for each $1< i \leq \ell$,
under the condition that $\sigma_{e_{i-1}} = \tau_{e_{i-1}} = (e_{i-1}\leftarrow 0)$, Line \ref{line-recursive-call} of Algorithm \ref{algo:Couple} is skiped.
In addition, we have $E^{\sigma_{i}}_v\neq \emptyset$, because only $i-1<\ell$ edges in $E_v^\sigma$ are assigned in $\sigma_{i}$.
Thus, the $i$-th iteration of the while loop must be performed
and $\sigma_{e_i}$ and $\tau_{e_i}$ are well-defined.
Therefore, $\+E$ is also well-defined.


For any $1<i\leq  \ell$, under the condition $\left(\sigma_{e_{1}}=\tau_{e_{1}} = (e_1\leftarrow 0)\right)\land \cdots\land \left(\sigma_{e_{i-1}} = \tau_{e_{i-1}}=(e_{i-1}\leftarrow 0)\right)$,
we have $\!{Ham}\left(\sigma_i, {E_v}\right) = \!{Ham}\left(\sigma, {E_v}\right) > \!{Ham}\left(\tau, {E_v}\right) = \!{Ham}\left(\tau_i, {E_v}\right)$.
Thus, we have $e_i$ satisfies 
                    $\mu^{\sigma_i}_e(1) \leq \mu^{\tau_i}_e(1)$
by Line \ref{line:pick-dominating-edge-2} of Algorithm \ref{algo:Couple}.
Therefore, $\mu^{\sigma_i}_e(0) = 1 - \mu^{\sigma_i}_e(1) \geq 1 - \mu^{\tau_i}_e(1)  = \mu^{\tau_i}_e(0)$.
Moreover, according to Line \ref{line-edge-sample},
we have 
$(\sigma_{e_i}, \tau_{e_i})$ is sampled from an optimal coupling of $(\mu^{\sigma_i}_{e_i}, \mu^{\tau_i}_{e_i})$.
Thus, we have 
\begin{align*}
&\Pr{\sigma_{e_{i}} = \tau_{e_{i}} = (e_i\leftarrow 0)\mid \left(\sigma_{e_{1}}=\tau_{e_{1}} = (e_1\leftarrow 0)\right)\land \cdots\land \left(\sigma_{e_{i-1}} = \tau_{e_{i-1}}=(e_{i-1}\leftarrow 0)\right)}
\\=& \min\left\{\mu^{\sigma_i}_e(0), \mu^{\tau_i}_e(0)\right\} = \mu^{\tau_i}_e(0),
\end{align*}
where the last inequality is by $\mu^{\sigma_i}_e(0) \geq \mu^{\tau_i}_e(0)$.
Similarly, one can also prove that 
\begin{align*}
\Pr{\sigma_{e_{1}} = \tau_{e_{1}} = (e_1\leftarrow 0)} = \mu^{\tau_1}_e(0) = \mu^{\tau}_e(0).
\end{align*}
Therefore, we have 
\begin{equation*}
\begin{aligned}
&\Pr{\+E} = \prod_{i\in [\ell]}\Pr{\sigma_{e_{i}} = \tau_{e_{i}} = (e_i\leftarrow 0)\mid \left(\sigma_{e_{1}}=\tau_{e_{1}} = (e_1\leftarrow 0)\right)\land \cdots\land \left(\sigma_{e_{i-1}} = \tau_{e_{i-1}}=(e_{i-1}\leftarrow 0)\right)}
\\= &\prod_{i\in [\ell]}\mu^{\tau_i}_{e_i}(0)
=\mu^{\tau}_{e_1}(0)\cdot \mu^{\tau\land (e_1\leftarrow 0)}_{e_2}(0)\cdots \mu^{\tau\land (e_1\leftarrow 0)\cdots\land (e_{\ell-1}\leftarrow 0)}_{e_{\ell}}(0)
= \mu_{E_v^\tau}^{\tau}(\zero)
\geq  B_{\min}(\Phi),
\end{aligned}
\end{equation*}
where the last inequality is by \Cref{lem:marginal-bound}.
In addition, we claim that if $\+E$ happens, then $\!{Couple}(\cdot)$ is not called in $\!{Couple}(\Phi, \sigma, \tau, v)$.
Thus, we have 
\begin{equation*}
\begin{aligned}
&\Pr{\!{Couple}(\cdot) \text{ is recursively called in Line \ref{line-recursive-call} of $\!{Couple}(\Phi, \sigma, \tau, v)$}}
\\=&1-\Pr{\!{Couple}(\cdot) \text{ is never called in Line \ref{line-recursive-call} of $\!{Couple}(\Phi, \sigma, \tau, v)$}}
\\\leq & 1- \Pr{\+E}
\\\leq &1- B_{\min}(\Phi).
\end{aligned}
\end{equation*}

At last, we prove the claim that $\!{Couple}(\cdot)$ is not called in $\!{Couple}(\Phi, \sigma, \tau, v)$ if $\+E$ happens.
Then the lemma is immediate.
Assume $\+E$ happens.
We have $\sigma_{e_i} = \tau_{e_i}$ for each $i\in [\ell]$,
Then Line \ref{line-recursive-call} of Algorithm \ref{algo:Couple} is skiped in the $i$-th iteration of the while loop.
{Moreover, we have $E^{\sigma_{\ell}\land \sigma_{e_\ell}}_v=\emptyset$, because all the edges 
$e_1,\cdots,e_{\ell}$ in $E_v^\sigma$ are assigned in $\sigma_{\ell}\land \sigma_{e_\ell}$.}
Thus, the $(\ell+1)$-th iteration of the while loop will not be performed.
Thus, $\!{Couple}(\cdot)$ in Line \ref{line-recursive-call} is never called in $\!{Couple}(\Phi, \sigma, \tau, v)$.
The claim is proved, which finishes the proof of the lemma.


\end{proof}


\PropertyDefrpc*
\begin{proof}
To prove this lemma, it is sufficient to prove 
\eqref{eq-def-trp-perperty-sigma}.
Then \eqref{eq-def-trp-perperty-sigma-e} can be proved similarly and the lemma is immediate.
In the following, we prove \eqref{eq-def-trp-perperty-sigma} by induction on the length of $\seqS$.
The induction basis is when $\seqS = \varnothing$.
In this case, by \eqref{eq-def-pro-stsvl} we have 
\begin{align*}
 \Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)} = \Pr{\left(T\geq 0\right)\land \left((\sigma, \tau, \seqS, v, L) = (\sigma_\bot, \tau_\bot, \varnothing, v_\bot, 0)\right)}\leq 1 = \mu^{\sigma_{\bot}}_{\seqS}(\sigma).
\end{align*}
% If $\sigma = \sigma_\bot$, we have 
% \begin{align*}
%  \Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)} \leq 1 = \mu^{\sigma_{\bot}}_{\seqS}(\sigma).
% \end{align*}
% Otherwise, $\sigma \neq \sigma_\bot$. We have 
% \begin{align*}
%  \Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)} = 0 \leq \Pr[X \sim \mu]{X \in \sigma \mid X(e_\bot) = 1}.
% \end{align*}
The base case is proved. For the induction step,  assume $\abs{\seqS} = t>0$, $\seqS = \seqS'\circ e$, $\sigma = \sigma'\land(e\leftarrow a)$, $\tau = \tau'\land(e\leftarrow b)$ for some $e\in E(G),a,b\in \{0,1\}$.
By \eqref{eq-def-pro-stsvl} we have 
\begin{align*}
 \Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)} 
 = \Pr{\left(T\geq t\right)\land \left((\sigma, \tau, \seqS, v, L) = (\sigma_t, \tau_t, \seqS_t, v_t, L_t)\right)}.
\end{align*}
In addition, by \Cref{def:truncated-random-process},
we have the event $\left(T\geq t\right)\land \left((\sigma, \tau, \seqS, v, L) = (\sigma_t, \tau_t, \seqS_t, v_t, L_t)\right)$ happens only if 
\begin{itemize}
\item the event $\+E\triangleq \left(T\geq t-1\right)\land \left((\sigma', \tau', \seqS', v(\sigma',\tau'), L(\sigma',\tau')) = (\sigma_{t-1}, \tau_{t-1}, \seqS_{t-1}, v_{t-1}, L_{t-1})\right)$ happens;
\item the chosen edge in the state $(\sigma_{t-1}, \tau_{t-1}, \seqS_{t-1}, v_{t-1}, L_{t-1})$ is $e$ and the sample $(\sigma_e,\tau_e)$ from an optimal coupling of $(\mu_e^{\sigma_{t-1}},\mu_e^{\tau_{t-1}})$ is $(a,b)$.
\end{itemize}
Thus, we have
\begin{align*}
 &\quad\Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)}\\ 
 &= \Pr{\left(T\geq t\right)\land \left((\sigma, \tau, \seqS, v, L) = (\sigma_t, \tau_t, \seqS_t, v_t, L_t)\right)}\\
&\leq 
\Pr{\+E}\cdot\Pr{(\sigma_e,\tau_e) =(a,b)\mid \+E}
\\&\leq \Pr{\left(T\geq t-1\right)\land \left((\sigma', \tau', \seqS', v(\sigma',\tau'), L(\sigma',\tau')) = (\sigma_{t-1}, \tau_{t-1}, \seqS_{t-1}, v_{t-1}, L_{t-1})\right)}\cdot\Pr{(\sigma_e,\tau_e) =(a,b)\mid \+E}
\\&\leq \mu^{\sigma_{\bot}}_{\seqS'}(\sigma')\cdot\Pr{(\sigma_e,\tau_e) =(a,b)\mid \+E},
\end{align*}
where the last inequality is by the induction hypothesis.
Moreover, we have 
\begin{equation*}
\begin{aligned}
\Pr{(\sigma_e,\tau_e) =(a,b)\mid \+E}
\leq \Pr{\sigma_e =a\mid \+E}
= \mu_e^{\sigma_{t-1}}\left(a \mid \+E\right)
= \mu_e^{\sigma'}(a),
\end{aligned}
\end{equation*}
where the first equality is by $(\sigma_e,\tau_e)$ is a coupling of $(\mu_e^{\sigma_{t-1}},\mu_e^{\tau_{t-1}})$ and the second equality is by $\sigma_{t-1} = \sigma'$ if $\+E$ happens.
Therefore, we have 
\begin{align*}
\Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)}\leq \mu^{\sigma_{\bot}}_{\seqS'}(\sigma')\cdot \mu_e^{\sigma'}(a) = \mu^{\sigma_{\bot}}_{\seqS}(\sigma).
\end{align*}
Similarly, one can also prove that 
$\Pr[\!{cp}]{(\sigma, \tau, \seqS, v, L)}\leq \mu^{\tau_{\bot}}_{\seqS}(\tau)$.
Then \eqref{eq-def-trp-perperty-sigma} is proved and the lemma is immediate.
\end{proof}


\section{Proofs for Properties of Holant Instances} \label{sec:proof-Holant-properties}

Now we complete the deferred proofs for properties of Holant instances. Recall that $\Phi = (G = (V, E), \vecf = \set{f_v})$ is a binary symmetric Holant instance with positive log-concave signatures and $\mu = \mu_\Phi$ is its Gibbs distribution.

\subsection{Feasibility of partial assignments}

To show the feasibility of a partial assignment $\sigma$, we first make the following claim.
\begin{claim} \label{claim:partial-assignment-feasibility}
    A partial assignment $\sigma$ is feasible if and only if the assignment $\sigma'$ on $E$ is feasible where $\sigma'$ is defined as
    \begin{align} \label{eq:0-assignment}
        \sigma'(e) = \begin{cases}
            \sigma(e) & \sigma \in \Lambda(\sigma) \\
            0 & \mbox{otherwise}
        \end{cases}\;.
    \end{align}
\end{claim}
\begin{proof}
    When $\sigma'$ is feasible, obviously $\sigma$ is feasible since $\sigma' \in \sigma$. To see the other side, when $\sigma$ is feasible, there exists $\tau : E \to \set{0, 1}$, $\tau \in \sigma$ such that $\mu(\tau) > 0$. Hence we have $\prod_{v \in V} f_v\left(\abs{\tau \vert_{E_v}}\right) > 0$ meaning that $f_v\left(\abs{\tau \vert_{E_v}}\right) > 0$ for every $v \in V$. Since $f_v$ is positive log-concave and $\abs{\sigma' \vert_{E_v}} \le \abs{\tau \vert_{E_v}}$, it holds that $f_v\left(\abs{\sigma' \vert_{E_v}}\right) > 0$. Therefore we obtain that $\mu(\sigma') > 0$ by the definition of $\mu(\sigma')$.
\end{proof}

\begin{proof}[Proof of~\Cref{lem:partial-assignment-feasibility}]
    By~\Cref{claim:partial-assignment-feasibility}, we check the feasibility of $\sigma'$ defined as~\eqref{eq:0-assignment}. In other words, we check whether $f\left(\abs{\sigma'\vert_{E_v}}\right) > 0$ for every $v \in V$. By definition, for every $v \in V$, $\abs{\sigma' \vert_{E_v}} = \abs{\sigma \vert_{E_v}}$ and thus $f\left(\abs{\sigma' \vert_{E_v}}\right) = f\left(\abs{\sigma \vert_{E_v}}\right)$. When $\sigma \vert_{E_v} = \emptyset$, nothing needs to do from the assumption $f_v(0) > 0$. Let $V(\sigma) \defeq \set{v \in V \cmid \sigma_{E_v} \neq \emptyset}$. It is not hard to see that we can find $V(\sigma)$ and compute $\sigma \vert_{E_v}$ for every $v \in V(\sigma)$ in time $O(\abs{\Lambda(\sigma)})$ by enumerating all edges in $\Lambda(\sigma)$. Hence we can check the feasibility of a partial assignment $\sigma$ in time $O(\abs{\Lambda(\sigma)})$. 
\end{proof}

\subsection{Marginal and ratio bounds}

\zdtodo{prove the marginal and ratio bounds here.}

\end{document}